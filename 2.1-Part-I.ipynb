{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f057eaae",
   "metadata": {},
   "source": [
    "# **Quantum Physics of the Discrete World**\n",
    "**Subtitle:** *From Lattice Mechanics to Quantum Engineering*\n",
    "**Series:** Springer Graduate Texts in Physics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb55da0",
   "metadata": {},
   "source": [
    "\n",
    "## **Part I: Foundations of Discrete Quantum Mechanics**\n",
    "**Subtitle:** *The Single-Particle Toolkit*\n",
    "\n",
    "**Pedagogical Goal:** By the end of this part, the reader should be able to define a Hilbert space, construct a Hamiltonian matrix, and solve for the spectrum of **any** discrete system.\n",
    "\n",
    "### **Section 1: The 1D Lattice (The Fabric of Space)**\n",
    "*We start with the simplest possible universe: a line of points. We establish the dictionary between Calculus and Linear Algebra.*\n",
    "\n",
    "#### **Chapter 1: The Discretized World**\n",
    "* **1.1 The State Vector:** Abandoning $\\psi(x)$ for the column vector $\\vec{\\psi}$. The universe as a list of $N$ complex amplitudes.\n",
    "* **1.2 The Inner Product:** Replacing integrals $\\int \\psi^* \\phi dx$ with dot products $\\vec{\\psi}^\\dagger \\vec{\\phi}$.\n",
    "* **1.3 Operators as Matrices:** Why observables (Position, Momentum) must be $N \\times N$ matrices. The Commutator $[A, B]$ as a check for matrix order.\n",
    "\n",
    "#### **Chapter 2: The Kinetic Matrix**\n",
    "* **2.1 The Finite Difference:** Deriving the discrete derivative. The central difference stencil.\n",
    "* **2.2 The Laplacian Matrix:** Deriving the \"1 -2 1\" Tridiagonal Matrix. This is the master key to all kinetic energy.\n",
    "* **2.3 The Hopping Parameter ($t$):** Physical interpretation of off-diagonal elements as tunneling amplitudes.\n",
    "* **2.4 Dispersion Relations:** Solving the matrix to find $E = 2t(1 - \\cos k)$. Recovering the parabolic continuum limit ($E=p^2/2m$) from the cosine band.\n",
    "\n",
    "#### **Chapter 3: Sculpting Potentials (1D Bound States)**\n",
    "* **3.1 The Particle in a Box:** Modeling hard walls by simply truncating the matrix (Dirichlet boundary conditions). \n",
    "* **3.2 The Harmonic Oscillator:** Adding a parabolic diagonal matrix $V = \\text{diag}(kx^2)$. Seeing the Gaussian ground state emerge from the eigenvector computation.\n",
    "* **3.3 The Double Well:** Modeling a qubit. How a barrier in the diagonal potential creates symmetric (bonding) and anti-symmetric (anti-bonding) states.\n",
    "\n",
    "#### **Chapter 4: Dynamics & Time Evolution**\n",
    "* **4.1 The Hamiltonian as a Clock:** The Schrödinger equation $\\frac{d\\vec{\\psi}}{dt} = -i \\mathbf{H} \\vec{\\psi}$ as a matrix differential equation.\n",
    "* **4.2 The Propagator:** Computing the Matrix Exponential $U(t) = e^{-i\\mathbf{H}t}$.\n",
    "* **4.3 Wave Packet Dispersion:** Simulating a Gaussian packet spreading over time on a lattice. The concept of Group Velocity on a grid.\n",
    "\n",
    "### **Section 2: The 2D Lattice (Geometry & Tensor Products)**\n",
    "*We step up a dimension, introducing the Kronecker Product and Complex Geometry.*\n",
    "\n",
    "#### **Chapter 5: Building Dimensions**\n",
    "* **5.1 The Tensor Product ($\\otimes$):** Constructing the 2D basis $|x, y\\rangle = |x\\rangle \\otimes |y\\rangle$.\n",
    "* **5.2 Separable Hamiltonians:** How to build the 2D Kinetic Matrix using Kronecker sums: $H_{2D} = H_{1D} \\otimes I + I \\otimes H_{1D}$.\n",
    "* **5.3 The Curse of Dimensionality (Intro):** How a $10 \\times 10$ grid becomes a $100 \\times 100$ matrix.\n",
    "\n",
    "#### **Chapter 6: The Square Lattice**\n",
    "* **6.1 The 5-Point Stencil:** The 2D Discrete Laplacian. A site connected to its North, South, East, and West neighbors.\n",
    "* **6.2 The Brillouin Zone:** 2D Momentum space $(k_x, k_y)$. Visualizing energy bands as surfaces. \n",
    "* **6.3 The Van Hove Singularity:** Topological changes in the Fermi surface when bands saddle.\n",
    "\n",
    "#### **Chapter 7: Complex Geometries (Graphene)**\n",
    "* **7.1 Non-Bravais Lattices:** Lattices with multi-atom unit cells.\n",
    "* **7.2 The Honeycomb Matrix:** Constructing the Bipartite Adjacency Matrix (Sublattices A and B).\n",
    "* **7.3 Dirac Cones:** Diagonalizing the $2 \\times 2$ momentum matrix to reveal linear dispersion ($E \\propto k$). Simulating massless particles on a grid. \n",
    "\n",
    "### **Section 3: The Generalized Lattice (Quantum Graphs)**\n",
    "*We remove the geometry entirely. Space is no longer a grid; it is a network.*\n",
    "\n",
    "#### **Chapter 8: The Adjacency Hamiltonian**\n",
    "* **8.1 From Lattice to Graph:** Defining the universe via the Adjacency Matrix $\\mathbf{A}$ ($A_{ij}=1$ if connected).\n",
    "* **8.2 The Graph Laplacian:** $\\mathbf{L} = \\mathbf{D} - \\mathbf{A}$. The generalized kinetic energy operator on a complex network.\n",
    "* **8.3 Eigenvector Centrality:** Using the ground state wavefunction to find the \"most important\" nodes in a network.\n",
    "\n",
    "#### **Chapter 9: Spectral Graph Theory**\n",
    "* **9.1 Graph Spectra:** Reading the topology of a network from its list of energy eigenvalues.\n",
    "* **9.2 The Spectral Gap:** How the first non-zero eigenvalue ($\\lambda_2$) determines how \"connected\" the universe is (The Fiedler Value).\n",
    "* **9.3 Isospectral Graphs:** Different shapes that sound the same. Why you can't always \"hear the shape of a drum.\"\n",
    "\n",
    "#### **Chapter 10: Quantum Walks**\n",
    "* **10.1 Classical vs. Quantum Diffusion:** Probability vectors vs. Amplitude vectors.\n",
    "* **10.2 Coherent Interference:** How a quantum particle finds paths faster than a random walker by cancelling out dead ends.\n",
    "* **10.3 Search as a Physical Process:** Framing Grover's Algorithm as a particle finding a \"sink\" in a fully connected graph.\n",
    "\n",
    "### **Section 4: Numerical Solvers (The Engine Room)**\n",
    "*Now that we can define any Hamiltonian, how do we solve it?*\n",
    "\n",
    "#### **Chapter 11: Exact Diagonalization (ED)**\n",
    "* **11.1 Dense Solvers:** Using standard libraries (LAPACK/NumPy) for small systems ($N < 5000$).\n",
    "* **11.2 Sparse Matrices:** Storing only non-zero elements. The CSR (Compressed Sparse Row) format.\n",
    "* **11.3 The Power Method:** Finding the dominant eigenvalue by repeated matrix multiplication.\n",
    "\n",
    "#### **Chapter 12: The Lanczos Algorithm**\n",
    "* **12.1 Krylov Subspaces:** Projecting the giant Hamiltonian into a tiny effective space.\n",
    "* **12.2 Convergence:** Why we find the ground state (lowest energy) first.\n",
    "* **12.3 Ghost Eigenvalues:** Numerical instability and re-orthogonalization.\n",
    "\n",
    "#### **Chapter 13: Time Stepping Methods**\n",
    "* **13.1 Finite Difference in Time:** Why Euler's method fails for Schrödinger (it violates unitarity).\n",
    "* **13.2 Crank-Nicolson:** Preserving probability with implicit methods.\n",
    "* **13.3 Trotter-Suzuki Decomposition:** Splitting $e^{-i(T+V)t} \\approx e^{-iTt}e^{-iVt}$ to simulate dynamics efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df9978d",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 1** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "# **Part I: Foundations of Discrete Quantum Mechanics**\n",
    "\n",
    "## **Subtitle:** *The Single-Particle Toolkit*\n",
    "\n",
    "**Pedagogical Goal:** By the end of this part, the reader should be able to define a Hilbert space, construct a Hamiltonian matrix, and solve for the spectrum of **any** discrete system.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 1: The 1D Lattice (The Fabric of Space)**\n",
    "\n",
    "#### **Chapter 1: The Discretized World**\n",
    "\n",
    "Standard quantum mechanics textbooks often begin with a daunting premise: the universe is a smooth, continuous fabric where a particle’s position $x$ can be any real number. This leads to wavefunctions $\\psi(x)$ governed by complex differential calculus. While mathematically rigorous, this continuous approach often obscures the algebraic simplicity of quantum theory.\n",
    "\n",
    "In this book, we take a different approach. We imagine the universe not as a smooth canvas, but as a grid of discrete points—a **lattice**. By discretizing space, we transform the complex calculus of functions into the tangible linear algebra of **vectors and matrices**.\n",
    "\n",
    "### **1.1 The State Vector**\n",
    "\n",
    "In the discrete world, the fundamental question we ask a particle is not \"What is your function?\" but \"Which site are you on?\"\n",
    "\n",
    "Let us define a 1-dimensional universe consisting of $N$ discrete sites. We introduce the **Lattice Constant**, denoted by $a$, which represents the smallest unit of distance—the spacing between two adjacent points. The position of the $n$-th site is:\n",
    "\n",
    "$$x_n = n \\cdot a$$\n",
    "\n",
    "where $n$ is an integer index ranging from $0$ to $N-1$.\n",
    "\n",
    "#### **From Function to Column**\n",
    "\n",
    "In continuum mechanics, the state of a system is described by a complex-valued function $\\psi(x)$. In our discrete formulation, we sample this function at our lattice points. The state becomes a list of $N$ complex numbers, which we organize into a column vector, denoted by the \"ket\" $| \\psi \\rangle$:\n",
    "\n",
    "$$\n",
    "| \\psi \\rangle \\doteq \\begin{pmatrix} c_0 \\\\ c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_{N-1} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Here, the coefficient $c_n$ represents the **probability amplitude** of finding the particle at site $n$. This simple column vector contains all the physical information about the system. Unlike $\\psi(x)$, which lives in an infinite-dimensional Hilbert space, our vector lives in a finite, $N$-dimensional complex vector space $\\mathbb{C}^N$ [1].\n",
    "\n",
    "### **1.2 The Inner Product**\n",
    "\n",
    "To make physical predictions, we must extract scalars (probabilities, energies) from these vectors. This requires the **Inner Product**.\n",
    "\n",
    "In continuous quantum mechanics, checking the overlap between two states $\\psi$ and $\\phi$ involves an integral:\n",
    "$$\\langle \\phi | \\psi \\rangle_{cont} = \\int_{-\\infty}^{\\infty} \\phi^*(x) \\psi(x) \\, dx$$\n",
    "\n",
    "In the discrete world, integrals become sums. The inner product is simply the dot product of the two vectors. We first define the \"bra\" $\\langle \\phi |$ as the Hermitian conjugate (conjugate transpose) of the ket:\n",
    "\n",
    "$$\n",
    "\\langle \\phi | = (| \\phi \\rangle)^\\dagger = \\begin{pmatrix} d_0^* & d_1^* & \\dots & d_{N-1}^* \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The inner product is the row multiplied by the column:\n",
    "\n",
    "$$\n",
    "\\langle \\phi | \\psi \\rangle = \\sum_{n=0}^{N-1} d_n^* c_n\n",
    "$$\n",
    "\n",
    "#### **Normalization and Probability**\n",
    "\n",
    "The Born Rule states that the probability $P_n$ of finding the particle at site $n$ is the square of the magnitude of the amplitude ($|c_n|^2$). Since the particle must exist *somewhere* in the lattice, the sum of all probabilities must equal 1. This leads to the normalization condition:\n",
    "\n",
    "$$\n",
    "\\langle \\psi | \\psi \\rangle = \\sum_{n=0}^{N-1} |c_n|^2 = 1\n",
    "$$\n",
    "\n",
    "This is geometrically equivalent to saying the state vector must have a length (norm) of 1 [2].\n",
    "\n",
    "### **1.3 Operators as Matrices**\n",
    "\n",
    "If states are vectors, then physical observables—quantities we can measure, like Position, Momentum, or Energy—must be objects that act on vectors. In linear algebra, linear maps are **Matrices**.\n",
    "\n",
    "An operator $\\hat{O}$ is an $N \\times N$ matrix. When it acts on a state $|\\psi\\rangle$, it transforms it into a new vector:\n",
    "$$\\hat{O} | \\psi \\rangle = | \\psi' \\rangle$$\n",
    "\n",
    "#### **The Position Matrix ($\\mathbf{X}$)**\n",
    "\n",
    "The simplest operator is Position. If a particle is perfectly localized at site $n$ (represented by the basis vector $|n\\rangle$), a measurement of position must yield the value $x_n = n \\cdot a$.\n",
    "This implies that the basis vectors are eigenvectors of the position operator. Therefore, the Position Matrix $\\mathbf{X}$ is **diagonal**:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{pmatrix} \n",
    "x_0 & 0 & \\cdots & 0 \\\\ \n",
    "0 & x_1 & \\cdots & 0 \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "0 & 0 & \\cdots & x_{N-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### **The Commutator**\n",
    "\n",
    "A defining feature of quantum mechanics is that the order of operations matters. In classical arithmetic, $A \\times B = B \\times A$. In matrix algebra, this is generally false. We quantify this non-commuting property using the **Commutator**:\n",
    "\n",
    "$$[\\mathbf{A}, \\mathbf{B}] = \\mathbf{A}\\mathbf{B} - \\mathbf{B}\\mathbf{A}$$\n",
    "\n",
    "If $[\\mathbf{A}, \\mathbf{B}] \\neq 0$, the two observables cannot be measured simultaneously with arbitrary precision (the generalized uncertainty principle) [3]. In the discrete world, checking for quantum behavior is as simple as checking if `np.dot(A, B) - np.dot(B, A)` is zero.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will now build this universe in Python. We will construct a discrete Gaussian wavepacket, normalize it, and define the Position operator matrix.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def discrete_quantum_world():\n",
    "    # 1. Define the Universe (The Lattice)\n",
    "    N = 50              # Number of sites\n",
    "    a = 1.0             # Lattice spacing (arbitrary units)\n",
    "    x = np.linspace(-N/2, N/2, N) * a  # Position array\n",
    "\n",
    "    # 2. Define a State Vector (The Wavefunction)\n",
    "    # We create a Gaussian wavepacket centered at x=0\n",
    "    sigma = 5.0\n",
    "    psi = np.exp(-(x**2) / (2 * sigma**2))\n",
    "    \n",
    "    # This vector is currently unnormalized.\n",
    "    # Current norm:\n",
    "    norm_squared = np.sum(np.abs(psi)**2)\n",
    "    print(f\"Unnormalized Probability Sum: {norm_squared:.4f}\")\n",
    "\n",
    "    # 3. Normalize the Vector\n",
    "    psi = psi / np.sqrt(norm_squared)\n",
    "    \n",
    "    # Verify Normalization (The Inner Product <psi|psi>)\n",
    "    # Note: np.vdot handles the complex conjugation automatically for the first argument\n",
    "    prob_sum = np.vdot(psi, psi).real\n",
    "    print(f\"Normalized Probability Sum: {prob_sum:.4f}\")\n",
    "\n",
    "    # 4. Construct the Position Operator (Matrix)\n",
    "    # A diagonal matrix with x values on the diagonal\n",
    "    X_operator = np.diag(x)\n",
    "    \n",
    "    print(\"\\nPosition Operator (First 5x5 block):\")\n",
    "    print(X_operator[:5, :5])\n",
    "\n",
    "    # 5. Calculate Expectation Value <X> = <psi| X |psi>\n",
    "    # Physically: The average position of the particle\n",
    "    # Algebraically: Vector-Matrix-Vector multiplication\n",
    "    X_psi = np.dot(X_operator, psi)      # Apply operator to ket\n",
    "    exp_X = np.vdot(psi, X_psi).real     # Inner product with bra\n",
    "    \n",
    "    print(f\"\\nExpectation Value <X>: {exp_X:.4f}\")\n",
    "    \n",
    "    # Visualizing the Probability Density\n",
    "    plt.bar(x, np.abs(psi)**2, width=a, color='skyblue', edgecolor='black')\n",
    "    plt.title(\"Discrete Probability Density |c_n|^2\")\n",
    "    plt.xlabel(\"Position (Lattice Sites)\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    discrete_quantum_world()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] R. Shankar, *Principles of Quantum Mechanics*, 2nd ed. (Plenum Press, New York, 1994).  \n",
    "[2] D. J. Griffiths and D. F. Schroeter, *Introduction to Quantum Mechanics*, 3rd ed. (Cambridge University Press, 2018).  \n",
    "[3] M. A. Nielsen and I. L. Chuang, *Quantum Computation and Quantum Information* (Cambridge University Press, 2010)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a4b6b3",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 2** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 1: The 1D Lattice (The Fabric of Space)**\n",
    "\n",
    "#### **Chapter 2: The Kinetic Matrix**\n",
    "\n",
    "In Chapter 1, we defined the \"state\" of the universe as a static vector. But physics is the study of change. A particle does not just sit at a site; it moves. In continuous quantum mechanics, motion is governed by the Kinetic Energy operator, which is proportional to the second derivative (curvature) of the wavefunction:\n",
    "\n",
    "$$\\hat{T} = -\\frac{\\hbar^2}{2m} \\frac{\\partial^2}{\\partial x^2}$$\n",
    "\n",
    "How do we perform a derivative on a grid where $x$ is not continuous? We must translate calculus into the language of matrices. This chapter derives the **Kinetic Matrix**, the engine that drives all quantum motion on a lattice.\n",
    "\n",
    "### **2.1 The Finite Difference**\n",
    "\n",
    "To find the slope (derivative) of a function sampled at discrete points $x_n$, we cannot take the limit $\\Delta x \\to 0$. We must settle for a finite $\\Delta x = a$ (the lattice constant).\n",
    "\n",
    "Recall the definition of the derivative:\n",
    "$$\\frac{d\\psi}{dx} \\approx \\frac{\\psi(x+a) - \\psi(x)}{a}$$\n",
    "\n",
    "This \"forward difference\" is biased to the right. To maintain the symmetry essential for Hermitian quantum operators (where moving left should be as likely as moving right), we use the **Central Difference** approximation [1]:\n",
    "\n",
    "$$\\frac{d\\psi}{dx} \\Big|_n \\approx \\frac{\\psi_{n+1} - \\psi_{n-1}}{2a}$$\n",
    "\n",
    "This formula tells us that the slope at site $n$ depends on the values of its neighbors.\n",
    "\n",
    "### **2.2 The Laplacian Matrix**\n",
    "\n",
    "The Kinetic Energy operator requires the **Second Derivative** (the Laplacian, $\\nabla^2$). Mathematically, this is the \"difference of the differences.\" It measures the local curvature of the wavefunction.\n",
    "\n",
    "$$\\frac{d^2\\psi}{dx^2} \\Big|_n \\approx \\frac{\\psi'_{n+1/2} - \\psi'_{n-1/2}}{a}$$\n",
    "\n",
    "Expanding this yields the famous **Three-Point Stencil** or the \"1 -2 1\" rule:\n",
    "\n",
    "$$\\frac{d^2\\psi}{dx^2} \\Big|_n \\approx \\frac{\\psi_{n+1} - 2\\psi_n + \\psi_{n-1}}{a^2}$$\n",
    "\n",
    "#### **Constructing the Matrix**\n",
    "\n",
    "If we apply this operation to the vector $|\\psi\\rangle$, we can represent the second derivative operator $\\mathbf{D}^2$ as a matrix. For a lattice of size $N=5$, the matrix looks like this:\n",
    "\n",
    "$$\n",
    "\\mathbf{D}^2 = \\frac{1}{a^2} \\begin{pmatrix}\n",
    "-2 & 1 & 0 & 0 & 0 \\\\\n",
    "1 & -2 & 1 & 0 & 0 \\\\\n",
    "0 & 1 & -2 & 1 & 0 \\\\\n",
    "0 & 0 & 1 & -2 & 1 \\\\\n",
    "0 & 0 & 0 & 1 & -2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This is a **Tridiagonal Matrix**. The diagonal elements (-2) represent the value at the site itself, while the off-diagonal elements (1) represent connections to immediate neighbors.\n",
    "\n",
    "### **2.3 The Hopping Parameter ($t$)**\n",
    "\n",
    "We can now write the full Hamiltonian for a free particle:\n",
    "$$\\mathbf{H} = -\\frac{\\hbar^2}{2m} \\mathbf{D}^2$$\n",
    "\n",
    "Substituting our matrix $\\mathbf{D}^2$, we get:\n",
    "$$\\mathbf{H} |\\psi\\rangle_n = -\\frac{\\hbar^2}{2ma^2} (\\psi_{n+1} - 2\\psi_n + \\psi_{n-1})$$\n",
    "\n",
    "We define a new fundamental constant of the lattice, the **Hopping Parameter** $t$:\n",
    "\n",
    "$$t \\equiv \\frac{\\hbar^2}{2ma^2}$$\n",
    "\n",
    "Rewriting the Hamiltonian in terms of $t$:\n",
    "$$\\mathbf{H} |\\psi\\rangle_n = -t \\psi_{n+1} + 2t \\psi_n - t \\psi_{n-1}$$\n",
    "\n",
    "#### **Physical Interpretation**\n",
    "\n",
    "The matrix $\\mathbf{H}$ has a beautiful structure:\n",
    "\n",
    "  * **Diagonal ($2t$):** The \"on-site\" energy cost. A particle pays an energy penalty just for existing on a confined lattice site.\n",
    "  * **Off-Diagonal ($-t$):** These terms connect site $n$ to sites $n \\pm 1$. In quantum mechanics, an off-diagonal term in the Hamiltonian causes transitions. This term allows the particle to **tunnel** or \"hop\" from one site to the next [2].\n",
    "\n",
    "$$\n",
    "\\mathbf{H} = \\begin{pmatrix}\n",
    "2t & -t & 0 & \\cdots \\\\\n",
    "-t & 2t & -t & \\cdots \\\\\n",
    "0 & -t & 2t & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### **2.4 Dispersion Relations**\n",
    "\n",
    "What are the energy levels of this matrix? To find them, we solve the eigenvalue equation $\\mathbf{H} \\vec{\\psi} = E \\vec{\\psi}$.\n",
    "For an infinite or periodic lattice, the eigenvectors are **Plane Waves**:\n",
    "$$\\psi_n = e^{ik (na)}$$\n",
    "where $k$ is the wave number (momentum).\n",
    "\n",
    "Substituting this ansatz into the difference equation:\n",
    "$$E e^{ikna} = -t e^{ik(n+1)a} + 2t e^{ikna} - t e^{ik(n-1)a}$$\n",
    "\n",
    "Dividing by $e^{ikna}$:\n",
    "$$E = -t e^{ika} + 2t - t e^{-ika}$$\n",
    "$$E = 2t - t(e^{ika} + e^{-ika})$$\n",
    "\n",
    "Using Euler's identity ($e^{ix} + e^{-ix} = 2\\cos x$), we arrive at the discrete **Dispersion Relation**:\n",
    "\n",
    "$$E(k) = 2t(1 - \\cos(ka))$$\n",
    "\n",
    "#### **Recovering the Continuum**\n",
    "\n",
    "Does this match reality? In the limit where the lattice spacing $a$ is very small (long wavelengths), $ka \\ll 1$. We can Taylor expand the cosine: $\\cos(x) \\approx 1 - x^2/2$.\n",
    "\n",
    "$$E(k) \\approx 2t \\left( 1 - (1 - \\frac{k^2 a^2}{2}) \\right) = t k^2 a^2$$\n",
    "\n",
    "Substituting $t = \\frac{\\hbar^2}{2ma^2}$:\n",
    "\n",
    "$$E(k) \\approx \\frac{\\hbar^2}{2ma^2} k^2 a^2 = \\frac{\\hbar^2 k^2}{2m} = \\frac{p^2}{2m}$$\n",
    "\n",
    "We have successfully recovered the classical kinetic energy parabola [3]. The lattice model is not just an approximation; it is a generalization that naturally includes band structure effects (the cosine curve) appearing in solid-state physics.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will now build the Kinetic Matrix in Python and visualize the \"Cosine Band\" dispersion relation.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def kinetic_matrix_simulation():\n",
    "    # 1. System Parameters\n",
    "    N = 50              # Lattice size\n",
    "    t = 1.0             # Hopping parameter (Energy unit)\n",
    "    \n",
    "    # 2. Construct the Kinetic Hamiltonian Matrix\n",
    "    # We use np.diag to build a tridiagonal matrix\n",
    "    # Main diagonal: 2t\n",
    "    main_diag = 2 * t * np.ones(N)\n",
    "    # Off-diagonals: -t\n",
    "    off_diag = -t * np.ones(N - 1)\n",
    "    \n",
    "    H = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n",
    "    \n",
    "    print(\"Hamiltonian Matrix (First 5x5 block):\")\n",
    "    print(H[:5, :5])\n",
    "    \n",
    "    # 3. Exact Diagonalization\n",
    "    # We find the eigenvalues (Energy spectrum)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(H)\n",
    "    \n",
    "    # 4. Analytical Comparison\n",
    "    # Allowed momentum k for a 1D chain of length L = N*a\n",
    "    # k ranges from -pi to +pi\n",
    "    k_analytical = np.linspace(-np.pi, np.pi, 100)\n",
    "    E_analytical = 2 * t * (1 - np.cos(k_analytical))\n",
    "    \n",
    "    # 5. Visualization\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Plot 1: The Analytical Cosine Band\n",
    "    plt.plot(k_analytical, E_analytical, label='Analytical Band: $2t(1-\\cos k)$', color='blue')\n",
    "    \n",
    "    # Plot 2: The Discrete Eigenvalues\n",
    "    # We map the sorted eigenvalues to the effective k-space\n",
    "    k_discrete = np.linspace(0, np.pi, N) # Positive k branch for comparison\n",
    "    plt.scatter(k_discrete, eigenvalues, color='red', label='Matrix Eigenvalues', zorder=5)\n",
    "    \n",
    "    plt.title(\"Dispersion Relation: Continuous vs. Discrete\")\n",
    "    plt.xlabel(\"Momentum ($ka$)\")\n",
    "    plt.ylabel(\"Energy ($E/t$)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kinetic_matrix_simulation()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery, *Numerical Recipes: The Art of Scientific Computing*, 3rd ed. (Cambridge University Press, 2007).  \n",
    "[2] R. P. Feynman, R. B. Leighton, and M. Sands, *The Feynman Lectures on Physics, Vol. III*, New Millennium Edition (Basic Books, 2011).  \n",
    "[3] N. W. Ashcroft and N. D. Mermin, *Solid State Physics* (Saunders College, 1976)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b3696",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 3** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 1: The 1D Lattice (The Fabric of Space)**\n",
    "\n",
    "#### **Chapter 3: Sculpting Potentials (1D Bound States)**\n",
    "\n",
    "In Chapter 2, we built the engine of motion: the Kinetic Matrix $\\mathbf{T}$. A particle governed by $\\mathbf{T}$ alone is a free particle—it spreads out infinitely. To model real physical systems like atoms, molecules, or qubits, we must trap the particle. We must apply forces.\n",
    "\n",
    "In the matrix formalism, a potential energy field $V(x)$ is represented by a diagonal matrix $\\mathbf{V}$. The total Hamiltonian becomes the sum of the kinetic and potential matrices:\n",
    "\n",
    "$$\\mathbf{H} = \\mathbf{T} + \\mathbf{V}$$\n",
    "\n",
    "This chapter explores how simply changing the numbers on the diagonal of $\\mathbf{V}$ allows us to sculpt the universe, creating bound states, harmonic oscillators, and quantum bits.\n",
    "\n",
    "### **3.1 The Particle in a Box**\n",
    "\n",
    "The simplest way to trap a particle is to place it in a container with infinitely hard walls. Physically, this means the potential energy is zero inside the box ($0 < x < L$) and infinite outside.\n",
    "\n",
    "#### **Truncating the Matrix**\n",
    "\n",
    "In continuous quantum mechanics, we solve differential equations with boundary conditions $\\psi(0)=0$ and $\\psi(L)=0$. On the lattice, we model this by **truncating the matrix**.\n",
    "\n",
    "If we define our lattice to have $N$ sites, the \"walls\" exist at the hypothetical sites $0$ and $N+1$. By simply not including these sites in our matrix, we enforce the condition that the particle cannot exist there (Dirichlet boundary conditions). The Hamiltonian is just the kinetic matrix $\\mathbf{T}$ with no \"wrap-around\" corners [1]:\n",
    "\n",
    "$$\n",
    "\\mathbf{H}_{box} = \\begin{pmatrix}\n",
    "2t & -t & 0 & \\cdots \\\\\n",
    "-t & 2t & -t & \\cdots \\\\\n",
    "0 & -t & 2t & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "[Image of particle in a box wavefunctions]\n",
    "\n",
    "#### **The Spectrum of Confinement**\n",
    "\n",
    "When we diagonalize this matrix, the eigenvectors are not traveling plane waves ($e^{ikn}$) but **standing waves** ($\\sin(k_m n)$). The energy eigenvalues are quantized:\n",
    "\n",
    "$$E_m = 2t \\left[ 1 - \\cos\\left( \\frac{m \\pi}{N+1} \\right) \\right]$$\n",
    "\n",
    "This discrete spectrum reveals a fundamental truth: **Confinement creates Energy.** The ground state ($m=1$) has non-zero energy ($E_1 > 0$), a direct manifestation of the uncertainty principle arising from the matrix's positive definiteness.\n",
    "\n",
    "### **3.2 The Harmonic Oscillator**\n",
    "\n",
    "Nature rarely uses hard walls. Real potentials, like the bond between two atoms, are \"soft.\" The most important model in physics is the **Harmonic Oscillator**, where the restoring force is proportional to the displacement ($F=-kx$), leading to a parabolic potential $V(x) = \\frac{1}{2} m \\omega^2 x^2$.\n",
    "\n",
    "#### **The Parabolic Diagonal**\n",
    "\n",
    "To simulate this, we define the center of our lattice as $x=0$ (site index $n$ ranges from $-N/2$ to $N/2$). The potential matrix $\\mathbf{V}$ is diagonal, with entries that grow quadratically with distance from the center:\n",
    "\n",
    "$$V_n = \\frac{1}{2} m \\omega^2 (na)^2$$\n",
    "\n",
    "$$\n",
    "\\mathbf{V} = \\begin{pmatrix}\n",
    "V_{-N/2} & 0 & \\cdots & 0 \\\\\n",
    "0 & \\ddots & 0 & 0 \\\\\n",
    "\\vdots & 0 & V_0 & \\vdots \\\\\n",
    "0 & 0 & \\cdots & V_{N/2}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### **Emergence of the Gaussian**\n",
    "\n",
    "The total Hamiltonian $\\mathbf{H} = \\mathbf{T} + \\mathbf{V}$ represents a tug-of-war.\n",
    "\n",
    "  * **Off-Diagonal ($-t$):** Tries to delocalize the particle (spread it out to lower kinetic curvature).\n",
    "  * **Diagonal ($V_n$):** Tries to localize the particle (push it to the center where $V=0$).\n",
    "\n",
    "The mathematical compromise between these forces is the **Gaussian** function ($\\psi_0 \\propto e^{-x^2}$). When we diagonalize this tridiagonal matrix, the ground state eigenvector naturally emerges as a bell curve, and the energy levels become equally spaced ($E_n \\approx \\hbar \\omega (n + 1/2)$) [2].\n",
    "\n",
    "### **3.3 The Double Well**\n",
    "\n",
    "What happens if we have two stable equilibrium points? Consider a potential with two minima separated by a barrier—a **Double Well**. This is the fundamental model for a chemical bond (an electron shared by two protons) and a quantum bit (qubit).\n",
    "\n",
    "#### **Constructing the Potential**\n",
    "\n",
    "We can model this by placing two harmonic wells at positions $x_L$ and $x_R$:\n",
    "$$V(x) = \\min\\left( \\frac{1}{2}k(x-x_L)^2, \\frac{1}{2}k(x-x_R)^2 \\right)$$\n",
    "Or more smoothly, using a quartic polynomial $V(x) = a x^4 - b x^2$.\n",
    "\n",
    "#### **Tunneling and Splitting**\n",
    "\n",
    "Classically, a particle with low energy would be trapped in either the left well *or* the right well.\n",
    "Quantum mechanically, the kinetic term $-t$ in the Hamiltonian allows the particle to **tunnel** through the barrier connecting the wells.\n",
    "\n",
    "This coupling causes the energy levels to split:\n",
    "\n",
    "1.  **Symmetric State (Ground State):** $|\\psi_+\\rangle \\approx \\frac{1}{\\sqrt{2}}(|L\\rangle + |R\\rangle)$. The wavefunction has no nodes and lower energy. In chemistry, this is the **Bonding Orbital** that holds molecules together.\n",
    "2.  **Anti-Symmetric State (Excited State):** $|\\psi_-\\rangle \\approx \\frac{1}{\\sqrt{2}}(|L\\rangle - |R\\rangle)$. The wavefunction crosses zero in the middle (a node) and has higher energy. This is the **Anti-Bonding Orbital** [3].\n",
    "\n",
    "The energy difference $\\Delta E = E_- - E_+$ is the **Tunneling Splitting**, which determines the frequency at which the particle oscillates between the wells (Rabi oscillation).\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will write a general solver that can handle any 1D potential. We will demonstrate it on the Harmonic Oscillator and the Double Well.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import eigh_tridiagonal\n",
    "\n",
    "def solve_1d_potential(N=100, potential_type='harmonic'):\n",
    "    # 1. Define the Lattice\n",
    "    a = 1.0\n",
    "    x = np.linspace(-N/2, N/2, N) * a\n",
    "    \n",
    "    # 2. Define the Kinetic Parameters\n",
    "    t = 1.0  # Hopping parameter\n",
    "    \n",
    "    # 3. Define the Potential Matrix (Diagonal)\n",
    "    if potential_type == 'harmonic':\n",
    "        k = 0.01  # Spring constant\n",
    "        V = 0.5 * k * x**2\n",
    "        title = \"Harmonic Oscillator\"\n",
    "    elif potential_type == 'double_well':\n",
    "        # V = a*x^4 - b*x^2\n",
    "        V = 0.0002 * x**4 - 0.05 * x**2\n",
    "        # Shift potential up so minimum is around 0 for plotting\n",
    "        V = V - np.min(V)\n",
    "        title = \"Double Well (Qubit)\"\n",
    "    elif potential_type == 'box':\n",
    "        V = np.zeros(N) # Zero potential inside the defined grid\n",
    "        title = \"Particle in a Box\"\n",
    "    \n",
    "    # 4. Construct Hamiltonian\n",
    "    # We use a specialized tridiagonal solver for efficiency\n",
    "    # Diagonal elements: 2t + V_n\n",
    "    d = 2 * t + V\n",
    "    # Off-diagonal elements: -t\n",
    "    e = -t * np.ones(N - 1)\n",
    "    \n",
    "    # 5. Diagonalize\n",
    "    # select='i' allows us to find just the first few eigenvalues (indices 0 to 2)\n",
    "    energies, eigenvectors = eigh_tridiagonal(d, e, select='i', select_range=(0, 2))\n",
    "    \n",
    "    # 6. Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot Potential (scaled down for visibility if needed, or on twin axis)\n",
    "    plt.plot(x, V, 'k--', label='Potential V(x)', alpha=0.5)\n",
    "    \n",
    "    # Plot Eigenstates (shifted by their eigenenergy)\n",
    "    scale_factor = 5.0 # Scale wavefunction for visibility\n",
    "    \n",
    "    # Ground State\n",
    "    psi_0 = eigenvectors[:, 0]\n",
    "    # Normalize for plotting consistency\n",
    "    psi_0 = psi_0 / np.max(np.abs(psi_0))\n",
    "    plt.plot(x, psi_0 * scale_factor + energies[0], label=f'Ground State (E={energies[0]:.2f})', color='blue')\n",
    "    plt.fill_between(x, energies[0], psi_0 * scale_factor + energies[0], color='blue', alpha=0.1)\n",
    "    \n",
    "    # First Excited State\n",
    "    psi_1 = eigenvectors[:, 1]\n",
    "    psi_1 = psi_1 / np.max(np.abs(psi_1))\n",
    "    plt.plot(x, psi_1 * scale_factor + energies[1], label=f'1st Excited (E={energies[1]:.2f})', color='red')\n",
    "    \n",
    "    plt.title(f\"{title}: Eigenstates and Eigenenergies\")\n",
    "    plt.xlabel(\"Position\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.ylim(np.min(V) - 1, np.max(V[int(N/3):int(2*N/3)]) + 5) # Zoom in on the center\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    solve_1d_potential(N=100, potential_type='harmonic')\n",
    "    solve_1d_potential(N=100, potential_type='double_well')\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] D. J. Griffiths, *Introduction to Quantum Mechanics*, 2nd ed. (Pearson Prentice Hall, 2005).  \n",
    "[2] R. Shankar, *Principles of Quantum Mechanics*, 2nd ed. (Plenum Press, New York, 1994).  \n",
    "[3] C. Cohen-Tannoudji, B. Diu, and F. Laloë, *Quantum Mechanics, Vol. 1* (Wiley-Interscience, 1977)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b0a8c",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 4** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 1: The 1D Lattice (The Fabric of Space)**\n",
    "\n",
    "#### **Chapter 4: Dynamics & Time Evolution**\n",
    "\n",
    "In the previous chapters, we solved the **Time-Independent** Schrödinger Equation ($H\\psi = E\\psi$) to find stationary states. These are the \"standing waves\" of the universe—they sit frozen in time, buzzing with a constant phase.\n",
    "\n",
    "But the universe moves. To describe collisions, transport, and change, we must return to the full **Time-Dependent** Schrödinger Equation:\n",
    "\n",
    "$$i\\hbar \\frac{d}{dt} |\\psi(t)\\rangle = \\mathbf{H} |\\psi(t)\\rangle$$\n",
    "\n",
    "This equation reveals the true nature of the Hamiltonian $\\mathbf{H}$. It is not just a measure of energy; it is the **Generator of Time Translation**. It is the clock that drives the universe forward.\n",
    "\n",
    "### **4.1 The Hamiltonian as a Clock**\n",
    "\n",
    "If we set $\\hbar = 1$ for simplicity, the equation becomes:\n",
    "$$\\frac{d}{dt} \\vec{\\psi} = -i \\mathbf{H} \\vec{\\psi}$$\n",
    "\n",
    "This is a system of coupled linear ordinary differential equations (ODEs). In our discrete lattice of size $N$, this looks like:\n",
    "$$\\frac{d}{dt} \\begin{pmatrix} c_0 \\\\ c_1 \\\\ \\vdots \\end{pmatrix} = -i \\begin{pmatrix} H_{00} & H_{01} & \\dots \\\\ H_{10} & H_{11} & \\dots \\\\ \\vdots & \\vdots & \\ddots \\end{pmatrix} \\begin{pmatrix} c_0 \\\\ c_1 \\\\ \\vdots \\end{pmatrix}$$\n",
    "\n",
    "The rate of change of the amplitude at site $n$ depends on the current amplitudes at all connected sites. The kinetic hopping terms (off-diagonal elements $H_{nm}$) literally pump probability amplitude from site $m$ to site $n$ [1].\n",
    "\n",
    "### **4.2 The Propagator**\n",
    "\n",
    "If the Hamiltonian $\\mathbf{H}$ is constant in time (no time-varying external fields), we can integrate the differential equation directly. The solution is:\n",
    "\n",
    "$$|\\psi(t)\\rangle = e^{-i\\mathbf{H}t} |\\psi(0)\\rangle$$\n",
    "\n",
    "We define the **Time Evolution Operator** (or Propagator) as the matrix exponential:\n",
    "$$\\mathbf{U}(t) = \\exp(-i\\mathbf{H}t)$$\n",
    "\n",
    "#### **Computing the Matrix Exponential**\n",
    "\n",
    "It is a common mistake to think we simply exponentiate each element of the matrix. **This is incorrect.**\n",
    "$$\\left(e^{\\mathbf{H}}\\right)_{ij} \\neq e^{(\\mathbf{H}_{ij})}$$\n",
    "\n",
    "Instead, the matrix exponential is defined by its Taylor Series:\n",
    "$$e^{\\mathbf{A}} = \\mathbf{I} + \\mathbf{A} + \\frac{1}{2!} \\mathbf{A}^2 + \\frac{1}{3!} \\mathbf{A}^3 + \\dots$$\n",
    "\n",
    "Calculating this infinite sum is computationally expensive. A more efficient method relies on **Spectral Decomposition**. If we can diagonalize $\\mathbf{H}$ such that $\\mathbf{H} = \\mathbf{V} \\mathbf{D} \\mathbf{V}^\\dagger$, where $\\mathbf{D}$ is the diagonal matrix of eigenvalues $E_n$, then:\n",
    "\n",
    "$$\\mathbf{U}(t) = \\mathbf{V} e^{-i\\mathbf{D}t} \\mathbf{V}^\\dagger$$\n",
    "\n",
    "Since $\\mathbf{D}$ is diagonal, $e^{-i\\mathbf{D}t}$ is simply a diagonal matrix of phase factors $e^{-iE_n t}$.\n",
    "This gives us a three-step algorithm for time travel:\n",
    "\n",
    "1.  **Decompose:** Switch to the energy basis ($\\mathbf{V}^\\dagger$).\n",
    "2.  **Rotate:** Advance the phase of each energy component ($e^{-iE_n t}$).\n",
    "3.  **Reconstruct:** Switch back to the position basis ($\\mathbf{V}$).\n",
    "\n",
    "### **4.3 Wave Packet Dispersion**\n",
    "\n",
    "The most striking difference between classical and quantum motion is **Dispersion**.\n",
    "If you kick a classical ball, it moves.\n",
    "If you kick a quantum wave packet, it moves, but it also **melts**.\n",
    "\n",
    "#### **Group Velocity on the Grid**\n",
    "\n",
    "A localized wave packet is a superposition of many different momentum states ($k$).\n",
    "$$|\\psi\\rangle = \\sum_k A_k |k\\rangle$$\n",
    "According to the propagator, each component evolves as $e^{-iE(k)t}$. The speed at which the \"envelope\" of the packet moves is the **Group Velocity**:\n",
    "\n",
    "$$v_g = \\frac{dE}{dk}$$\n",
    "\n",
    "On our lattice, the dispersion relation is $E(k) = 2t(1 - \\cos(ka))$.\n",
    "Therefore, the velocity depends on momentum:\n",
    "$$v_g(k) = 2ta \\sin(ka)$$\n",
    "\n",
    "#### **The Spreading Mechanism**\n",
    "\n",
    "Since $v_g$ depends on $k$, different parts of the wave packet travel at different speeds.\n",
    "\n",
    "  * The \"fast\" momentum components race ahead.\n",
    "  * The \"slow\" momentum components lag behind.\n",
    "  * The result is that the packet widens ($\\Delta x$ increases) over time.\n",
    "\n",
    "On a lattice, this has an extra twist. The velocity $v_g \\propto \\sin(ka)$ is not monotonic. It reaches a maximum at $k = \\pi/2a$ and drops to zero at the zone boundary $k = \\pi/a$. High-momentum components can actually stop moving or move backward (negative mass), leading to complex interference patterns known as \"Quantum Carpets\" [2].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will simulate the time evolution of a Gaussian wave packet on a free lattice. We will visualize the packet spreading and moving.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import expm\n",
    "\n",
    "def simulate_time_evolution():\n",
    "    # 1. System Parameters\n",
    "    N = 100             # Lattice size\n",
    "    t_hop = 1.0         # Hopping parameter\n",
    "    dt = 2.0            # Time step for evolution\n",
    "    \n",
    "    # 2. Build Hamiltonian (Free Particle)\n",
    "    main_diag = 2 * t_hop * np.ones(N)\n",
    "    off_diag = -t_hop * np.ones(N - 1)\n",
    "    H = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n",
    "    \n",
    "    # 3. Initial State: Gaussian Wave Packet\n",
    "    x = np.arange(N)\n",
    "    x0 = N // 4         # Start at 1/4 of the grid\n",
    "    sigma = 5.0         # Width\n",
    "    k0 = np.pi / 2      # Initial momentum (Right moving)\n",
    "    \n",
    "    # Psi = Envelope * Plane Wave\n",
    "    psi_0 = np.exp(-(x - x0)**2 / (2 * sigma**2)) * np.exp(1j * k0 * x)\n",
    "    psi_0 = psi_0 / np.linalg.norm(psi_0) # Normalize\n",
    "    \n",
    "    # 4. Compute Propagator U(t) = exp(-iHt)\n",
    "    # For small matrices, we can use scipy.linalg.expm\n",
    "    U = expm(-1j * H * dt)\n",
    "    \n",
    "    # 5. Time Evolution Loop\n",
    "    time_steps = 5\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    psi = psi_0\n",
    "    for step in range(time_steps):\n",
    "        # Calculate Probability Density\n",
    "        prob_density = np.abs(psi)**2\n",
    "        \n",
    "        # Plot\n",
    "        plt.plot(x, prob_density, label=f't = {step * dt:.1f}')\n",
    "        \n",
    "        # Evolve state for next step\n",
    "        psi = U @ psi \n",
    "        \n",
    "    plt.title(\"Time Evolution of a Wave Packet on a Lattice\")\n",
    "    plt.xlabel(\"Position (Site Index)\")\n",
    "    plt.ylabel(\"Probability Density\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    simulate_time_evolution()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] J. J. Sakurai and J. Napolitano, *Modern Quantum Mechanics*, 2nd ed. (Cambridge University Press, 2017).  \n",
    "[2] M. Berry, \"Quantum fractals in boxes,\" *Journal of Physics A: Mathematical and General* 29, 6617 (1996)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded15e8",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 5** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 2: The 2D Lattice (Geometry & Tensor Products)**\n",
    "\n",
    "#### **Chapter 5: Building Dimensions**\n",
    "\n",
    "In Section 1, our universe was a line. We described the state of a system using a vector of size $N$. But the real world is multidimensional. To describe a particle moving on a 2D plane (or a 3D volume), we need a mathematical machine that glues simple spaces together to form larger, more complex spaces.\n",
    "\n",
    "This machine is the **Tensor Product** (symbolized by $\\otimes$). It is the fundamental operation for scaling quantum mechanics. It allows us to take a description of \"Left-Right\" and a description of \"Up-Down\" and combine them into a description of \"Space.\"\n",
    "\n",
    "### **5.1 The Tensor Product ($\\otimes$)**\n",
    "\n",
    "#### **5.1.1 The Product Basis**\n",
    "\n",
    "Imagine a particle on a 2D grid of size $N \\times N$.\n",
    "To specify its location, we need two coordinates: an x-index ($i$) and a y-index ($j$).\n",
    "The basis state is $|i, j\\rangle$.\n",
    "\n",
    "In linear algebra, this state is constructed from the individual 1D basis vectors via the tensor product:\n",
    "$$|i, j\\rangle = |i\\rangle_x \\otimes |j\\rangle_y$$\n",
    "\n",
    "If the x-space has dimension $N$ and the y-space has dimension $N$, the combined **Hilbert Space** has dimension $N \\times N = N^2$.\n",
    "\n",
    "#### **5.1.2 The Kronecker Product**\n",
    "\n",
    "How do we calculate this numerically? For vectors, the tensor product is implemented as the **Kronecker Product**.\n",
    "Let vector $\\mathbf{u}$ represent the state in x, and vector $\\mathbf{v}$ represent the state in y.\n",
    "\n",
    "$$\n",
    "\\mathbf{u} = \\begin{pmatrix} u_0 \\\\ u_1 \\end{pmatrix}, \\quad \\mathbf{v} = \\begin{pmatrix} v_0 \\\\ v_1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The combined state $\\mathbf{\\psi} = \\mathbf{u} \\otimes \\mathbf{v}$ is formed by multiplying every element of $\\mathbf{u}$ by the entire vector $\\mathbf{v}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} \\otimes \\mathbf{v} = \\begin{pmatrix} u_0 \\mathbf{v} \\\\ u_1 \\mathbf{v} \\end{pmatrix} = \\begin{pmatrix} u_0 v_0 \\\\ u_0 v_1 \\\\ u_1 v_0 \\\\ u_1 v_1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This operation explodes the vector size. If $\\mathbf{u}$ and $\\mathbf{v}$ are length $N$, the result is length $N^2$. This single long vector contains all possible correlations between the x and y coordinates [1].\n",
    "\n",
    "### **5.2 Separable Hamiltonians**\n",
    "\n",
    "Now that we have a 2D state vector, we need a 2D Hamiltonian to move it.\n",
    "The Kinetic Energy operator in 2D is the Laplacian $\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$.\n",
    "In the discrete world, this corresponds to a Hamiltonian that allows hopping in the x-direction **plus** hopping in the y-direction.\n",
    "\n",
    "$$H_{2D} = T_x + T_y$$\n",
    "\n",
    "But we have a dimension mismatch. $T_x$ is an $N \\times N$ matrix acting on the x-coordinate. It doesn't know how to act on the $N^2$-sized 2D vector.\n",
    "We must \"inflate\" the 1D matrices to fit the 2D space.\n",
    "\n",
    "#### **5.2.1 Expanding Operators**\n",
    "\n",
    "1.  **Action on X ($T_x \\otimes I$):**\n",
    "    We want to move the particle in the x-direction while leaving the y-coordinate unchanged. We calculate the tensor product of $T_x$ with the Identity matrix $I_y$:\n",
    "    $$H_x = T_x \\otimes I_N$$\n",
    "    This matrix hops the \"block indices\" of the vector while keeping the internal structure of the blocks frozen.\n",
    "\n",
    "2.  **Action on Y ($I \\otimes T_y$):**\n",
    "    We want to move the particle in the y-direction while leaving the x-coordinate unchanged.\n",
    "    $$H_y = I_N \\otimes T_y$$\n",
    "    This matrix acts identically inside every block, hopping the internal indices.\n",
    "\n",
    "#### **5.2.2 The Kronecker Sum**\n",
    "\n",
    "The total 2D Hamiltonian is the **Kronecker Sum** of the 1D Hamiltonians:\n",
    "\n",
    "$$H_{2D} = (T_{1D} \\otimes I_N) + (I_N \\otimes T_{1D})$$\n",
    "\n",
    "This recipe is universal. It works for 3D ($T \\otimes I \\otimes I + \\dots$), and it works for many-body physics (Particle 1 $\\otimes$ Particle 2). It allows us to build complex high-dimensional operators solely from simple 1D building blocks [2].\n",
    "\n",
    "### **5.3 The Curse of Dimensionality (Intro)**\n",
    "\n",
    "The tensor product reveals the fundamental bottleneck of quantum simulation.\n",
    "Let us count the cost of adding dimensions.\n",
    "\n",
    "  * **1D Chain ($N=100$):**\n",
    "\n",
    "      * Vector Size: $100$.\n",
    "      * Matrix Size: $100 \\times 100 = 10,000$ elements.\n",
    "      * *Result:* Trivial for a calculator.\n",
    "\n",
    "  * **2D Grid ($100 \\times 100$):**\n",
    "\n",
    "      * Vector Size: $100^2 = 10,000$.\n",
    "      * Matrix Size: $10,000 \\times 10,000 = 10^8$ elements.\n",
    "      * *Result:* Requires $\\approx 800$ MB of RAM. Doable on a laptop.\n",
    "\n",
    "  * **3D Cube ($100 \\times 100 \\times 100$):**\n",
    "\n",
    "      * Vector Size: $100^3 = 1,000,000$.\n",
    "      * Matrix Size: $10^6 \\times 10^6 = 10^{12}$ elements.\n",
    "      * *Result:* Requires **8 Terabytes** of RAM to store the dense matrix.\n",
    "\n",
    "This exponential scaling ($N^d$) is the **Curse of Dimensionality**. It is why we cannot simply simulate a large protein or a material by brute force. However, note that while the matrix *size* is huge, it is mostly zeros (Sparse). The structure $I \\otimes T + T \\otimes I$ ensures that each row has only 5 non-zero elements (the 5-point stencil), regardless of the dimension size. This sparsity is the only reason 2D and 3D simulations are possible [3].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will demonstrate how to build a 2D Laplacian matrix from 1D parts using `numpy.kron`. We will verify the size explosion.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def build_2d_hamiltonian():\n",
    "    # 1. Define 1D Parameters\n",
    "    N = 10              # Linear size (small for visualization)\n",
    "    t = 1.0             # Hopping parameter\n",
    "    \n",
    "    # 2. Build 1D Kinetic Matrix (Tridiagonal)\n",
    "    # Using sparse matrices to save memory\n",
    "    main_diag = 2 * t * np.ones(N)\n",
    "    off_diag = -t * np.ones(N - 1)\n",
    "    \n",
    "    # Diags takes [diagonals], [offsets]\n",
    "    H_1D = sp.diags([off_diag, main_diag, off_diag], [-1, 0, 1], shape=(N, N))\n",
    "    \n",
    "    print(f\"1D Matrix Shape: {H_1D.shape}\")\n",
    "    \n",
    "    # 3. Build 2D Operators via Kronecker Product\n",
    "    I = sp.eye(N)\n",
    "    \n",
    "    # H_x = T (x) I\n",
    "    H_x = sp.kron(H_1D, I)\n",
    "    \n",
    "    # H_y = I (x) T\n",
    "    H_y = sp.kron(I, H_1D)\n",
    "    \n",
    "    # 4. The Total 2D Hamiltonian\n",
    "    H_2D = H_x + H_y\n",
    "    \n",
    "    print(f\"2D Matrix Shape: {H_2D.shape}\")\n",
    "    print(f\"Total Elements: {H_2D.shape[0]**2}\")\n",
    "    print(f\"Non-zero Elements: {H_2D.nnz}\")\n",
    "    \n",
    "    # 5. Visualization of the Matrix Structure (Sparsity Pattern)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.spy(H_1D, markersize=5)\n",
    "    plt.title(\"1D Hamiltonian\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.spy(H_x, markersize=1)\n",
    "    plt.title(\"H_x = T $\\otimes$ I\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.spy(H_2D, markersize=1)\n",
    "    plt.title(\"2D Hamiltonian (H_x + H_y)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_2d_hamiltonian()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] G. H. Golub and C. F. Van Loan, *Matrix Computations*, 4th ed. (Johns Hopkins University Press, 2013).  \n",
    "[2] M. A. Nielsen and I. L. Chuang, *Quantum Computation and Quantum Information* (Cambridge University Press, 2010).  \n",
    "[3] Y. Saad, *Iterative Methods for Sparse Linear Systems*, 2nd ed. (SIAM, 2003)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1475f2a",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 6** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 2: The 2D Lattice (Geometry & Tensor Products)**\n",
    "\n",
    "#### **Chapter 6: The Square Lattice**\n",
    "\n",
    "In Chapter 5, we introduced the Tensor Product as a mathematical machine to build higher dimensions. Now, we apply this machine to the most common structure in computational physics and materials science: the **Square Lattice**.\n",
    "\n",
    "This geometry is the \"Hello World\" of 2D physics. It represents pixels on a screen, atoms in a copper-oxide plane, or a discretized field in a simulation. By solving the matrix mechanics of this grid, we discover phenomena that do not exist in 1D, such as saddle points in the energy landscape and topological changes in the Fermi surface.\n",
    "\n",
    "### **6.1 The 5-Point Stencil**\n",
    "\n",
    "The Kinetic Energy operator in 2D is proportional to the Laplacian $\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$.\n",
    "To discretize this, we apply the finite difference method to both the $x$ and $y$ directions simultaneously.\n",
    "\n",
    "#### **6.1.1 Neighbors on a Grid**\n",
    "\n",
    "Consider a site at coordinates $(i, j)$.\n",
    "\n",
    "  * Its $x$-neighbors are $(i+1, j)$ and $(i-1, j)$.\n",
    "  * Its $y$-neighbors are $(i, j+1)$ and $(i, j-1)$.\n",
    "\n",
    "The discrete Laplacian sums the curvature in both directions:\n",
    "\n",
    "$$\n",
    "\\nabla^2 \\psi_{i,j} \\approx \\frac{\\psi_{i+1,j} - 2\\psi_{i,j} + \\psi_{i-1,j}}{a^2} + \\frac{\\psi_{i,j+1} - 2\\psi_{i,j} + \\psi_{i,j-1}}{a^2}\n",
    "$$\n",
    "\n",
    "Grouping the terms, we find the famous **5-Point Stencil**:\n",
    "\n",
    "$$\n",
    "\\nabla^2 \\psi_{i,j} \\approx \\frac{1}{a^2} \\left( \\psi_{\\text{North}} + \\psi_{\\text{South}} + \\psi_{\\text{East}} + \\psi_{\\text{West}} - 4\\psi_{\\text{Center}} \\right)\n",
    "$$\n",
    "\n",
    "#### **6.1.2 The Hamiltonian**\n",
    "\n",
    "Multiplying by $-\\frac{\\hbar^2}{2m}$ defines the hopping parameter $t$. The Hamiltonian acts on a site by summing the amplitudes of its four nearest neighbors and subtracting a central cost:\n",
    "\n",
    "$$\n",
    "\\mathbf{H} \\psi_{i,j} = -t \\left( \\psi_{i+1,j} + \\psi_{i-1,j} + \\psi_{i,j+1} + \\psi_{i,j-1} \\right) + 4t \\psi_{i,j}\n",
    "$$\n",
    "\n",
    "  * **Diagonal ($4t$):** In 1D, the cost to exist was $2t$ (2 neighbors). In 2D, it is $4t$ (4 neighbors). The more connected the lattice, the higher the kinetic \"pressure\" to delocalize [1].\n",
    "  * **Off-Diagonal ($-t$):** The particle can hop in four cardinal directions.\n",
    "\n",
    "### **6.2 The Brillouin Zone**\n",
    "\n",
    "We solve this system using the 2D version of Bloch's Theorem. We propose a plane wave solution that travels diagonally across the grid:\n",
    "\n",
    "$$\n",
    "\\psi_{n,m} = e^{i(k_x n a + k_y m a)}\n",
    "$$\n",
    "\n",
    "Here, $\\vec{k} = (k_x, k_y)$ is the 2D momentum vector.\n",
    "\n",
    "#### **6.2.1 The 2D Dispersion Relation**\n",
    "\n",
    "Substituting this ansatz into the Hamiltonian equation yields the energy eigenvalues:\n",
    "\n",
    "$$\n",
    "E(k_x, k_y) = -t (e^{ik_x a} + e^{-ik_x a} + e^{ik_y a} + e^{-ik_y a}) + 4t\n",
    "$$\n",
    "\n",
    "Using Euler's identity ($e^{ix} + e^{-ix} = 2\\cos x$), we arrive at the dispersion surface:\n",
    "\n",
    "$$\n",
    "E(\\vec{k}) = 2t \\left( 2 - \\cos(k_x a) - \\cos(k_y a) \\right)\n",
    "$$\n",
    "\n",
    "This function defines a surface shaped like an egg crate or a trampoline.\n",
    "\n",
    "  * **Minimum (Ground State):** At $\\vec{k}=(0,0)$, $\\cos(0)=1$. $E = 2t(2 - 1 - 1) = 0$.\n",
    "      * *Physics:* Long wavelength, low energy.\n",
    "  * **Maximum:** At $\\vec{k}=(\\pi/a, \\pi/a)$, $\\cos(\\pi)=-1$. $E = 2t(2 - (-1) - (-1)) = 8t$.\n",
    "      * *Physics:* Checkerboard oscillation (highest curvature).\n",
    "\n",
    "#### **6.2.2 Visualizing Momentum Space**\n",
    "\n",
    "The allowed values of momentum form a square region called the **First Brillouin Zone**, defined by $-\\pi/a < k_x, k_y \\le \\pi/a$.\n",
    "This square represents the \"momentum universe\" of the lattice. No particle can have a momentum outside this box; due to aliasing, a wave with $k_x = 1.1 \\pi$ is indistinguishable from $k_x = -0.9 \\pi$ [2].\n",
    "\n",
    "### **6.3 The Van Hove Singularity**\n",
    "\n",
    "The most striking feature of the 2D lattice appears when we look at the topology of the energy contours.\n",
    "\n",
    "Imagine filling the band with electrons, starting from the bottom ($E=0$).\n",
    "\n",
    "1.  **Low Energy:** The contours $E(\\vec{k}) = C$ are circles centered at $(0,0)$. The electrons form a \"Fermi Sea\" that looks like a circular lake.\n",
    "2.  **High Energy:** Near the maximum ($E=8t$), the contours are circles centered at the corners $(\\pi, \\pi)$. This is a lake of \"holes.\"\n",
    "\n",
    "#### **6.3.1 The Saddle Point**\n",
    "\n",
    "Something drastic happens exactly in the middle of the band ($E = 4t$).\n",
    "This energy corresponds to momenta like $(\\pi/a, 0)$ and $(0, \\pi/a)$.\n",
    "At these points, the band curves **up** in one direction and **down** in the other.\n",
    "\n",
    "  * In $x$: It is a maximum ($\\cos(\\pi) = -1$).\n",
    "  * In $y$: It is a minimum ($\\cos(0) = 1$).\n",
    "\n",
    "This is a **Saddle Point**. At a saddle point, the group velocity vanishes ($v_g = \\nabla_k E = 0$), even though the momentum is non-zero. The particle momentarily stops moving.\n",
    "\n",
    "#### **6.3.2 Divergence in Density of States (DOS)**\n",
    "\n",
    "Because the particles slow down near the saddle point, they \"pile up\" at this specific energy.\n",
    "The **Density of States** $g(E)$, which counts how many quantum states exist at energy $E$, diverges logarithmically:\n",
    "\n",
    "$$\n",
    "g(E) \\propto -\\ln \\left| \\frac{E - 4t}{t} \\right|\n",
    "$$\n",
    "\n",
    "This peak is called a **Van Hove Singularity**.\n",
    "In materials physics, this is a \"sweet spot.\" If the Fermi level of a material aligns with a Van Hove singularity, interactions are massively enhanced. This mechanism is believed to boost the critical temperature of High-$T_c$ Superconductors (cuprates), which are effectively 2D square lattices of Copper and Oxygen [3].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will visualize the 2D dispersion surface and calculate the Density of States histogram to reveal the logarithmic singularity.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def square_lattice_physics():\n",
    "    # 1. Define Momentum Space (The Brillouin Zone)\n",
    "    N = 200  # Resolution\n",
    "    k_range = np.linspace(-np.pi, np.pi, N)\n",
    "    KX, KY = np.meshgrid(k_range, k_range)\n",
    "    \n",
    "    t = 1.0  # Hopping parameter\n",
    "    \n",
    "    # 2. Calculate Dispersion Relation E(kx, ky)\n",
    "    # E = 2t * (2 - cos(kx) - cos(ky))\n",
    "    E = 2 * t * (2 - np.cos(KX) - np.cos(KY))\n",
    "    \n",
    "    # 3. Visualization: The Energy Surface\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Subplot 1: 3D Surface\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    surf = ax1.plot_surface(KX, KY, E, cmap='viridis', edgecolor='none')\n",
    "    ax1.set_title(\"2D Dispersion Surface $E(k)$\")\n",
    "    ax1.set_xlabel(\"$k_x a$\")\n",
    "    ax1.set_ylabel(\"$k_y a$\")\n",
    "    ax1.set_zlabel(\"Energy ($t$)\")\n",
    "    \n",
    "    # Subplot 2: Density of States (Histogram)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    # Flatten the energy array to treat it as a list of states\n",
    "    energy_list = E.flatten()\n",
    "    \n",
    "    # Plot histogram\n",
    "    count, bins, ignored = ax2.hist(energy_list, bins=100, density=True, color='orange', alpha=0.7)\n",
    "    \n",
    "    # Highlight the Van Hove Singularity\n",
    "    ax2.axvline(x=4*t, color='red', linestyle='--', label='Van Hove Singularity (E=4t)')\n",
    "    \n",
    "    ax2.set_title(\"Density of States (DOS)\")\n",
    "    ax2.set_xlabel(\"Energy ($t$)\")\n",
    "    ax2.set_ylabel(\"Density $g(E)$\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    square_lattice_physics()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] N. W. Ashcroft and N. D. Mermin, *Solid State Physics* (Saunders College, 1976).  \n",
    "[2] C. Kittel, *Introduction to Solid State Physics*, 8th ed. (Wiley, 2004).  \n",
    "[3] J. Singleton, *Band Theory and Electronic Properties of Solids* (Oxford University Press, 2001)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc01218",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 7** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 2: The 2D Lattice (Geometry & Tensor Products)**\n",
    "\n",
    "#### **Chapter 7: Complex Geometries (Graphene)**\n",
    "\n",
    "In previous chapters, we dealt with simple grids where every point was identical to every other point under translation (Bravais lattices). But nature often prefers complexity. The most famous 2D material, **Graphene** (a single layer of graphite), is arranged in a **Honeycomb Lattice**.\n",
    "\n",
    "At first glance, this looks like a chicken wire mesh. Computationally, this structure poses a new challenge: it is not a simple grid. If you stand on an atom and look North, you might see a bond. If you move to the immediate neighbor and look North, you might see empty space. To model this using matrices, we must introduce the concept of the **Multi-Atom Unit Cell**.\n",
    "\n",
    "### **7.1 Non-Bravais Lattices**\n",
    "\n",
    "A Honeycomb lattice is not a Bravais lattice because not all sites are equivalent. However, it can be described as a triangular lattice with a **Basis** of two atoms. We identify two distinct types of sites:\n",
    "\n",
    "  * **Sublattice A:** Atoms with bonds pointing (for example) North, South-East, South-West.\n",
    "  * **Sublattice B:** Atoms with bonds pointing South, North-East, North-West.\n",
    "\n",
    "We define a **Unit Cell** at position $\\vec{R}$ containing one A-atom and one B-atom. The wavefunction at cell $\\vec{R}$ is no longer a scalar, but a vector of length 2:\n",
    "\n",
    "$$\n",
    "|\\psi_{\\vec{R}}\\rangle = \\begin{pmatrix} \\psi_{\\vec{R},A} \\\\ \\psi_{\\vec{R},B} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The total Hilbert space size is $2 \\times N_{cells}$.\n",
    "\n",
    "### **7.2 The Honeycomb Matrix**\n",
    "\n",
    "The Hamiltonian measures the connectivity between these atoms. Since an A-atom *only* connects to three B-atoms (and vice-versa), the lattice is **Bipartite**. There are no A-A or B-B hops in the nearest-neighbor approximation.\n",
    "\n",
    "This geometry dictates the structure of the Hamiltonian matrix $\\mathbf{H}$. If we set the on-site energy to zero ($E=0$), the diagonal blocks of the matrix must be zero.\n",
    "\n",
    "$$\n",
    "\\mathbf{H} = \\begin{pmatrix}\n",
    "\\mathbf{0} & \\mathbf{H}_{AB} \\\\\n",
    "\\mathbf{H}_{BA} & \\mathbf{0}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "  * **$\\mathbf{0}$:** Represents the lack of hopping within the same sublattice.\n",
    "  * **$\\mathbf{H}_{AB}$:** A sparse matrix encoding the connections from A atoms to B atoms.\n",
    "  * **$\\mathbf{H}_{BA}$:** The Hermitian conjugate ($\\mathbf{H}_{AB}^\\dagger$), representing B to A hopping.\n",
    "\n",
    "This **Chiral Symmetry** (zeros on the diagonal) is the algebraic root of graphene's stability and unique electronic properties [1].\n",
    "\n",
    "### **7.3 Dirac Cones**\n",
    "\n",
    "To solve for the energy spectrum, we use Bloch's Theorem. We transform the infinite spatial matrix into a momentum-dependent matrix $\\mathbf{H}(\\vec{k})$ for a single unit cell.\n",
    "\n",
    "#### **7.3.1 The Geometric Phase Sum**\n",
    "\n",
    "Consider an A-atom at the origin. It hops to three B-neighbors located at vectors $\\vec{\\delta}_1, \\vec{\\delta}_2, \\vec{\\delta}_3$. The hopping amplitude is $-t$.\n",
    "In the Bloch picture, hopping by a vector $\\vec{\\delta}$ picks up a phase factor $e^{i \\vec{k} \\cdot \\vec{\\delta}}$.\n",
    "\n",
    "We define the geometric structure function $f(\\vec{k})$ as the sum of these three paths:\n",
    "$$f(\\vec{k}) = -t \\left( e^{i \\vec{k} \\cdot \\vec{\\delta}_1} + e^{i \\vec{k} \\cdot \\vec{\\delta}_2} + e^{i \\vec{k} \\cdot \\vec{\\delta}_3} \\right)$$\n",
    "\n",
    "The Bloch Hamiltonian becomes a simple $2 \\times 2$ matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{H}(\\vec{k}) = \\begin{pmatrix}\n",
    "0 & f(\\vec{k}) \\\\\n",
    "f^*(\\vec{k}) & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### **7.3.2 Diagonalization**\n",
    "\n",
    "The eigenvalues of this matrix are found by solving the characteristic equation $\\det(\\mathbf{H}(\\vec{k}) - E\\mathbf{I}) = 0$:\n",
    "$$E^2 - |f(\\vec{k})|^2 = 0 \\implies E(\\vec{k}) = \\pm |f(\\vec{k})|$$\n",
    "\n",
    "This generates two energy bands: the Conduction Band ($+E$) and the Valence Band ($-E$), which are perfectly symmetric around zero energy.\n",
    "\n",
    "#### **7.3.3 The Emergence of Massless Particles**\n",
    "\n",
    "In a standard square lattice, the ground state is a parabola ($E \\propto k^2$). In graphene, the bands touch at specific corners of the Brillouin Zone, labeled **K** and **K'**. At these points, $f(\\vec{k}) = 0$.\n",
    "\n",
    "If we Taylor expand the Hamiltonian around these K-points (letting $\\vec{q} = \\vec{k} - \\vec{K}$), the function $f(\\vec{k})$ becomes linear in $\\vec{q}$. The energy dispersion becomes a cone:\n",
    "\n",
    "$$E(\\vec{q}) \\approx \\pm \\hbar v_F |\\vec{q}|$$\n",
    "\n",
    "This equation ($E=cp$) describes ultra-relativistic, massless particles. By simply arranging carbon atoms in a hexagon, we have created a tabletop simulator for **Quantum Electrodynamics (QED)**. The electrons in graphene behave as if they have lost their mass, traveling at a constant \"Fermi Velocity\" $v_F \\approx c/300$ [2].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will build the Graphene Hamiltonian and visualize the famous Dirac Cones in 3D.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def graphene_band_structure():\n",
    "    # 1. Define Lattice Geometry\n",
    "    a = 1.0 # Carbon-Carbon distance\n",
    "    # Nearest neighbor vectors\n",
    "    d1 = np.array([0, a])\n",
    "    d2 = np.array([a * np.sqrt(3)/2, -a/2])\n",
    "    d3 = np.array([-a * np.sqrt(3)/2, -a/2])\n",
    "    \n",
    "    t = 2.7  # Hopping parameter in eV\n",
    "    \n",
    "    # 2. Define Momentum Grid (Brillouin Zone)\n",
    "    # We scan a range large enough to see the K points\n",
    "    kx = np.linspace(-4*np.pi/(3*np.sqrt(3)*a), 4*np.pi/(3*np.sqrt(3)*a), 100)\n",
    "    ky = np.linspace(-4*np.pi/(3*a), 4*np.pi/(3*a), 100)\n",
    "    KX, KY = np.meshgrid(kx, ky)\n",
    "    \n",
    "    # 3. Calculate Structure Function f(k)\n",
    "    # f(k) = -t * sum(exp(i * k . delta))\n",
    "    f_k = -t * (np.exp(1j * (KX*d1[0] + KY*d1[1])) + \n",
    "                np.exp(1j * (KX*d2[0] + KY*d2[1])) + \n",
    "                np.exp(1j * (KX*d3[0] + KY*d3[1])))\n",
    "    \n",
    "    # 4. Calculate Energy Eigenvalues E = +/- |f(k)|\n",
    "    E_conduction = np.abs(f_k)\n",
    "    E_valence = -np.abs(f_k)\n",
    "    \n",
    "    # 5. Visualization\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot Conduction Band\n",
    "    ax.plot_surface(KX, KY, E_conduction, cmap='autumn', alpha=0.8, edgecolor='none')\n",
    "    # Plot Valence Band\n",
    "    ax.plot_surface(KX, KY, E_valence, cmap='winter', alpha=0.8, edgecolor='none')\n",
    "    \n",
    "    ax.set_title(\"Graphene Band Structure: Dirac Cones\")\n",
    "    ax.set_xlabel(\"$k_x$\")\n",
    "    ax.set_ylabel(\"$k_y$\")\n",
    "    ax.set_zlabel(\"Energy (eV)\")\n",
    "    ax.view_init(elev=15, azim=45)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    graphene_band_structure()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] A. H. Castro Neto, F. Guinea, N. M. R. Peres, K. S. Novoselov, and A. K. Geim, *The electronic properties of graphene*, Rev. Mod. Phys. **81**, 109 (2009).  \n",
    "[2] M. I. Katsnelson, *Graphene: Carbon in Two Dimensions* (Cambridge University Press, 2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd11d5e",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 8** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 3: The Generalized Lattice (Quantum Graphs)**\n",
    "\n",
    "#### **Chapter 8: The Adjacency Hamiltonian**\n",
    "\n",
    "In the previous sections, we modeled the universe as a crystal—a highly ordered structure where every site has the same number of neighbors and the same local geometry. This assumption of **Translational Symmetry** allowed us to define momentum $k$ and energy bands.\n",
    "\n",
    "However, many real-world systems do not look like crystals.\n",
    "\n",
    "  * **The Internet:** A tangled web of servers and fiber optics.\n",
    "  * **Social Networks:** A complex graph of friends and followers.\n",
    "  * **The Brain:** A hierarchy of neurons connected by synapses.\n",
    "\n",
    "To apply quantum mechanics to these systems, we must break the crystal. We replace the rigid lattice coordinates $(x, y, z)$ with a generalized connectivity map called the **Graph**. In this chapter, we learn how to write the Schrödinger equation for a universe with no geometric symmetry.\n",
    "\n",
    "### **8.1 From Lattice to Graph**\n",
    "\n",
    "We define our universe as a **Graph** $G(V, E)$, consisting of a set of Vertices (sites) $V$ and a set of Edges (connections) $E$.\n",
    "The fundamental mathematical object describing this universe is no longer a metric tensor, but the **Adjacency Matrix** $\\mathbf{A}$.\n",
    "\n",
    "#### **8.1.1 The Adjacency Matrix**\n",
    "\n",
    "For a system with $N$ nodes, $\\mathbf{A}$ is an $N \\times N$ matrix. Its entries define the topology:\n",
    "\n",
    "$$\n",
    "A_{ij} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if nodes } i \\text{ and } j \\text{ are connected} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "If the edges are weighted (e.g., bandwidth capacity or synaptic strength), $A_{ij}$ can be any real number representing the coupling strength.\n",
    "\n",
    "#### **8.1.2 The Hamiltonian**\n",
    "\n",
    "We postulate a \"tight-binding\" Hamiltonian proportional to this connectivity. If a particle is at node $j$, it can hop to node $i$ if and only if an edge exists.\n",
    "\n",
    "$$H_{adj} = -\\gamma \\mathbf{A}$$\n",
    "\n",
    "  * **$\\gamma$ (Hopping Amplitude):** The energy scale of the transition.\n",
    "  * **The Physics:** A quantum particle on this graph can \"teleport\" between any connected nodes. Unlike a crystal, where hopping is restricted to immediate neighbors $n \\pm 1$, here a particle might hop from node 1 to node 1,000 instantly if a direct link exists. The concept of \"distance\" is replaced by \"connectivity.\"\n",
    "\n",
    "### **8.2 The Graph Laplacian**\n",
    "\n",
    "In Volume I, we defined the Kinetic Energy operator (the discretized second derivative $\\nabla^2$) using the \"1 -2 1\" rule. This formula relied on every site having exactly two neighbors. How do we calculate \"curvature\" or \"diffusion\" on a graph where a node might have 3 neighbors, or 100, or 1?\n",
    "\n",
    "To generalize kinetic energy, we introduce the **Graph Laplacian**.\n",
    "\n",
    "#### **8.2.1 The Degree Matrix ($\\mathbf{D}$)**\n",
    "\n",
    "First, we count the neighbors. We define the **Degree Matrix** as a diagonal matrix where the entry $D_{ii}$ is the number of connections (degree) of node $i$.\n",
    "\n",
    "$$D_{ii} = \\sum_j A_{ij} = k_i$$\n",
    "\n",
    "#### **8.2.2 The Laplacian Operator ($\\mathbf{L}$)**\n",
    "\n",
    "We define the Graph Laplacian as:\n",
    "\n",
    "$$\\mathbf{L} = \\mathbf{D} - \\mathbf{A}$$\n",
    "\n",
    "Let us apply this matrix to a quantum state vector $\\vec{\\psi}$ at node $i$:\n",
    "$$(\\mathbf{L} \\vec{\\psi})_i = D_{ii} \\psi_i - \\sum_{j} A_{ij} \\psi_j$$\n",
    "$$(\\mathbf{L} \\vec{\\psi})_i = \\sum_{j \\in \\text{neighbors}} (\\psi_i - \\psi_j)$$\n",
    "\n",
    "This formula is the exact generalization of the diffusion operator.\n",
    "\n",
    "  * It sums the **differences** between the amplitude at the node and its neighbors.\n",
    "  * If $\\psi_i$ is the average of its neighbors, $(\\mathbf{L}\\psi)_i = 0$ (Equilibrium).\n",
    "  * If $\\psi_i$ is a sharp peak compared to its surroundings, $(\\mathbf{L}\\psi)_i$ is large.\n",
    "\n",
    "**Physical Interpretation:**\n",
    "The expectation value of the Laplacian measures the **smoothness** of the wavefunction across the network [1].\n",
    "$$E = \\langle \\psi | \\mathbf{L} | \\psi \\rangle = \\sum_{(i,j) \\in E} |\\psi_i - \\psi_j|^2$$\n",
    "A state with low energy varies slowly across connected nodes; a state with high energy oscillates rapidly between neighbors.\n",
    "\n",
    "### **8.3 Eigenvector Centrality**\n",
    "\n",
    "Since we cannot plot energy bands ($E$ vs $k$) because momentum $k$ does not exist, we analyze the graph by looking at its eigenstates directly.\n",
    "The eigenvectors of the Adjacency Matrix (or Laplacian) reveal the hidden hierarchy of the network.\n",
    "\n",
    "#### **8.3.1 The Principal Eigenvector**\n",
    "\n",
    "Consider the eigenvector $\\vec{v}_{max}$ corresponding to the largest eigenvalue $\\lambda_{max}$ of the Adjacency Matrix $\\mathbf{A}$.\n",
    "$$\\mathbf{A} \\vec{v}_{max} = \\lambda_{max} \\vec{v}_{max}$$\n",
    "\n",
    "The component $v_i$ of this vector is proportional to the sum of the components of its neighbors.\n",
    "$$v_i \\propto \\sum_{j \\in \\text{neighbors}} v_j$$\n",
    "\n",
    "This implies a recursive definition of importance: **A node is important if it is connected to other important nodes.**\n",
    "This metric is called **Eigenvector Centrality**. It is the algorithm behind Google's original PageRank.\n",
    "\n",
    "#### **8.3.2 Quantum Localization on Hubs**\n",
    "\n",
    "If we place a quantum particle on a graph and let it evolve, where does it go?\n",
    "The particle naturally flows toward nodes with high centrality.\n",
    "In the ground state of the Hamiltonian $H = -\\mathbf{A}$, the probability density $|\\psi_i|^2$ is concentrated on the **Hubs** (high-degree nodes) [2]. The \"mass\" of the wavefunction gravitates toward the connectivity centers of the universe.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will construct a random graph (Erdős-Rényi) and a structured graph (Star Graph), compute their Laplacians, and visualize the \"Ground State\" centrality.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def graph_quantum_mechanics():\n",
    "    # 1. Create a Graph (The Universe)\n",
    "    # Let's create a \"Barbell\" graph: Two clusters connected by a bridge\n",
    "    G = nx.barbell_graph(m1=5, m2=1) \n",
    "    \n",
    "    # 2. Get Matrices\n",
    "    # Adjacency Matrix A\n",
    "    A = nx.to_numpy_array(G)\n",
    "    \n",
    "    # Degree Matrix D\n",
    "    degrees = np.sum(A, axis=1)\n",
    "    D = np.diag(degrees)\n",
    "    \n",
    "    # Laplacian L = D - A\n",
    "    L = D - A\n",
    "    \n",
    "    print(\"Laplacian Matrix Shape:\", L.shape)\n",
    "    \n",
    "    # 3. Solve the Schrödinger Equation (Diagonalize L)\n",
    "    # We find eigenvalues and eigenvectors\n",
    "    # eigenvalues correspond to \"Energy Levels\" of the network\n",
    "    evals, evecs = np.linalg.eigh(L)\n",
    "    \n",
    "    # 4. Analyze the Fiedler Vector (First non-zero excitation)\n",
    "    # The ground state of L is always 0 (Uniform vector).\n",
    "    # The first excited state (index 1) tells us about the graph cuts.\n",
    "    fiedler_val = evals[1]\n",
    "    fiedler_vec = evecs[:, 1]\n",
    "    \n",
    "    print(f\"Spectral Gap (Fiedler Value): {fiedler_val:.4f}\")\n",
    "    \n",
    "    # 5. Visualization\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Draw Graph\n",
    "    plt.subplot(1, 2, 1)\n",
    "    pos = nx.spring_layout(G)\n",
    "    # Color nodes by the Fiedler vector value\n",
    "    nx.draw(G, pos, node_color=fiedler_vec, cmap='coolwarm', \n",
    "            with_labels=True, node_size=500)\n",
    "    plt.title(\"Graph Topology colored by Wavefunction\")\n",
    "    \n",
    "    # Plot Spectrum\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(evals, 'o-')\n",
    "    plt.title(\" Laplacian Spectrum (Energy Levels)\")\n",
    "    plt.xlabel(\"Mode Index\")\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    graph_quantum_mechanics()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] M. Newman, *Networks: An Introduction* (Oxford University Press, 2010).  \n",
    "[2] F. R. K. Chung, *Spectral Graph Theory* (American Mathematical Society, 1997)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2818e0",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 9** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 3: The Generalized Lattice (Quantum Graphs)**\n",
    "\n",
    "#### **Chapter 9: Spectral Graph Theory**\n",
    "\n",
    "In Chapter 8, we defined the Hamiltonian for a network using the Adjacency Matrix $\\mathbf{A}$ and the Graph Laplacian $\\mathbf{L}$. We saw that a quantum particle on a graph moves according to the \"connections\" rather than physical distance.\n",
    "\n",
    "Now, we ask the inverse question: **Can we hear the shape of a network?**\n",
    "By analyzing the list of energy levels (eigenvalues) of the Laplacian matrix—the **Spectrum**—we can deduce the topological properties of the universe without ever looking at a map. This field is known as **Spectral Graph Theory**.\n",
    "\n",
    "### **9.1 Graph Spectra**\n",
    "\n",
    "The spectrum of a graph $G$ is the set of eigenvalues of its Laplacian matrix $\\mathbf{L}$:\n",
    "$$0 = \\lambda_0 \\le \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_{N-1}$$\n",
    "\n",
    "This list of numbers is the \"bar code\" of the network. Just as the optical spectrum of a star reveals its chemical composition, the graph spectrum reveals the network's connectivity, clustering, and robustness.\n",
    "\n",
    "#### **9.1.1 The Zero Mode ($\\lambda_0$)**\n",
    "\n",
    "The Laplacian $\\mathbf{L} = \\mathbf{D} - \\mathbf{A}$ always has a trivial eigenvector: the uniform vector $\\vec{v}_0 = (1, 1, \\dots, 1)^T$.\n",
    "$$(\\mathbf{L} \\vec{v}_0)_i = \\sum_j L_{ij} \\cdot 1 = D_{ii} - \\sum_{j \\sim i} 1 = k_i - k_i = 0$$\n",
    "Thus, $\\lambda_0 = 0$.\n",
    "**Physics:** This represents the steady state of diffusion. If you pour energy or probability into the network and wait $t \\to \\infty$, it spreads out evenly (assuming the graph is connected). This equilibrium costs zero energy.\n",
    "\n",
    "#### **9.1.2 Multiplicity and Components**\n",
    "\n",
    "If the graph is disconnected (e.g., two separate islands), then diffusion on Island A cannot reach Island B. The probability mass is conserved separately on each island.\n",
    "This means there are *two* independent steady states.\n",
    "**Theorem:** The multiplicity of the eigenvalue $\\lambda=0$ equals the number of connected components in the graph [1].\n",
    "\n",
    "### **9.2 The Spectral Gap ($\\lambda_1$)**\n",
    "\n",
    "The most important number in the entire spectrum is the first non-zero eigenvalue, $\\lambda_1$. This is often called the **Algebraic Connectivity** or the **Fiedler Value**.\n",
    "\n",
    "It governs the **dynamics** of the network.\n",
    "The time evolution of a diffusion process is dominated by the smallest non-zero exponent:\n",
    "$$P(t) \\sim e^{-\\lambda_1 t}$$\n",
    "Therefore, the **Relaxation Time** of the network is $\\tau \\approx 1/\\lambda_1$.\n",
    "\n",
    "#### **9.2.1 The Dumbbell vs. The Sphere**\n",
    "\n",
    "  * **Small Gap ($\\lambda_1 \\approx 0$):**\n",
    "    Imagine two large clusters of nodes connected by a single, weak bridge (a \"Dumbbell\" shape). To diffuse from Left to Right, the particle must tunnel through this bottleneck. The process is slow. The relaxation time $\\tau$ is huge.\n",
    "    *Physics:* The system is fragile. Cutting one link destroys global connectivity.\n",
    "\n",
    "  * **Large Gap ($\\lambda_1 \\gg 0$):**\n",
    "    Imagine a highly connected graph (like a \"Small World\" network or a Ramanujan graph). Every node is just a few hops from every other node. Diffusion is explosive.\n",
    "    *Physics:* The system is robust. It is an **Expander Graph**.\n",
    "\n",
    "#### **9.2.2 The Cheeger Inequality**\n",
    "\n",
    "There is a rigorous bound relating this spectral quantity $\\lambda_1$ to the geometric \"choke point\" of the graph, known as the **Cheeger Constant** $h(G)$ (the minimum cut ratio):\n",
    "$$\\frac{\\lambda_1}{2} \\le h(G) \\le \\sqrt{2\\lambda_1}$$\n",
    "This inequality proves that \"spectral physics\" and \"geometric cuts\" are equivalent. We can diagnose a supply chain bottleneck simply by diagonalizing a matrix [2].\n",
    "\n",
    "### **9.3 Isospectral Graphs**\n",
    "\n",
    "If we know the entire list of eigenvalues, do we know the graph perfectly?\n",
    "**No.**\n",
    "Just as you cannot reconstruct a drum's exact shape purely from its sound frequencies, there exist **Isospectral Graphs**—different networks that have the exact same eigenvalues.\n",
    "\n",
    "#### **9.3.1 The Counter-Example**\n",
    "\n",
    "Consider two graphs with $N$ vertices.\n",
    "\n",
    "1.  **Graph A:** A specific arrangement of hexagons and triangles.\n",
    "2.  **Graph B:** A different arrangement.\n",
    "    They can be constructed such that their scattering matrices are identical. A particle walking on Graph A \"hears\" the same echoes as a particle on Graph B.\n",
    "\n",
    "This limit tells us that the Spectrum captures **global** topology (connectivity, diameter, loops), but loses some **local** structural information. To distinguish isospectral graphs, we need to look at the **Eigenvectors** (the wavefunctions themselves), not just the Energies [3].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will generate two different graphs, compute their spectra, and analyze the Fiedler vector to perform \"Spectral Clustering\"—using quantum mechanics to cut a graph in half.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def spectral_graph_theory():\n",
    "    # 1. Create a \"Dumbbell\" Graph (Two clusters + Bridge)\n",
    "    # Cluster 1: Nodes 0-4 (Complete)\n",
    "    # Cluster 2: Nodes 5-9 (Complete)\n",
    "    # Bridge: Node 4 connected to Node 5\n",
    "    G = nx.disjoint_union(nx.complete_graph(5), nx.complete_graph(5))\n",
    "    G.add_edge(4, 5)\n",
    "    \n",
    "    # 2. Compute Laplacian Matrix\n",
    "    L = nx.laplacian_matrix(G).toarray()\n",
    "    \n",
    "    # 3. Diagonalize\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(L)\n",
    "    \n",
    "    # 4. Analyze the Spectrum\n",
    "    print(\"First 5 Eigenvalues:\", np.round(eigenvalues[:5], 3))\n",
    "    \n",
    "    lambda_1 = eigenvalues[1]\n",
    "    print(f\"Spectral Gap (Fiedler Value): {lambda_1:.4f}\")\n",
    "    # A small value indicates a bottleneck\n",
    "    \n",
    "    # 5. Analyze the Fiedler Vector (Eigenvector for lambda_1)\n",
    "    fiedler_vector = eigenvectors[:, 1]\n",
    "    print(\"\\nFiedler Vector components:\")\n",
    "    print(np.round(fiedler_vector, 2))\n",
    "    \n",
    "    # 6. Visualization\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Subplot 1: The Graph colored by Fiedler Vector\n",
    "    plt.subplot(1, 2, 1)\n",
    "    pos = nx.spring_layout(G)\n",
    "    # Nodes with negative values vs positive values define the two clusters\n",
    "    nx.draw(G, pos, node_color=fiedler_vector, cmap='coolwarm', \n",
    "            with_labels=True, node_size=600, font_color='white')\n",
    "    plt.title(\"Spectral Partitioning (Color = Fiedler Amplitude)\")\n",
    "    \n",
    "    # Subplot 2: The Eigenspectrum\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(eigenvalues, 'o-', markerfacecolor='white')\n",
    "    plt.title(\"Laplacian Spectrum\")\n",
    "    plt.xlabel(\"Index $k$\")\n",
    "    plt.ylabel(\"Eigenvalue $\\lambda_k$\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spectral_graph_theory()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] F. R. K. Chung, *Spectral Graph Theory* (American Mathematical Society, 1997).  \n",
    "[2] D. A. Spielman, *Spectral and Algebraic Graph Theory* (Yale University, 2019).  \n",
    "[3] M. Kac, \"Can One Hear the Shape of a Drum?\" *American Mathematical Monthly* **73**, 1-23 (1966)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63067c3",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 10** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 3: The Generalized Lattice (Quantum Graphs)**\n",
    "\n",
    "#### **Chapter 10: Quantum Walks**\n",
    "\n",
    "In Chapter 9, we studied static properties of graphs (spectra). Now we turn to dynamics. How does a particle move through a complex network?\n",
    "\n",
    "The study of **Random Walks** is the foundation of classical computer science (Monte Carlo methods, PageRank). A walker flips a coin and moves to a neighbor. Over time, it forgets where it started and settles into a steady state.\n",
    "\n",
    "A **Quantum Walk** is fundamentally different. It does not flip coins; it rotates unitary matrices. It does not diffuse; it propagates. It does not forget; it interferes. This chapter explores how replacing probability with amplitude transforms a slow, stumbling drunkard into a ballistic explorer.\n",
    "\n",
    "### **10.1 Classical vs. Quantum Diffusion**\n",
    "\n",
    "#### **10.1.1 The Drunkard's Walk (Classical)**\n",
    "\n",
    "Consider a walker on a 1D line. At every step, they flip a coin: Heads (Right), Tails (Left).\n",
    "We describe the system by a **Probability Vector** $\\vec{p}(t)$, where $p_n$ is the probability of being at site $n$.\n",
    "The evolution is governed by a Stochastic Matrix $\\mathbf{M}$:\n",
    "$$\\vec{p}(t+1) = \\mathbf{M} \\vec{p}(t)$$\n",
    "Because probabilities are real non-negative numbers, they always add up.\n",
    "$$p_n(t+1) = \\frac{1}{2}p_{n-1}(t) + \\frac{1}{2}p_{n+1}(t)$$\n",
    "The result is a Gaussian distribution that spreads as $\\sigma \\sim \\sqrt{t}$. This is **Diffusion**. It is slow [1].\n",
    "\n",
    "#### **10.1.2 The Quantum Walk**\n",
    "\n",
    "Now, replace the probability vector with a complex **Amplitude Vector** $|\\psi(t)\\rangle$.\n",
    "The evolution is governed by a Unitary Matrix $\\mathbf{U}$:\n",
    "$$|\\psi(t+1)\\rangle = \\mathbf{U} |\\psi(t)\\rangle$$\n",
    "Amplitudes can be negative.\n",
    "$$\\psi_n(t+1) = \\frac{1}{\\sqrt{2}}\\psi_{n-1}(t) - \\frac{1}{\\sqrt{2}}\\psi_{n+1}(t)$$\n",
    "Crucially, terms can **cancel out**. This destructive interference eliminates paths that \"turn back\" on themselves, suppressing the probability of staying near the origin.\n",
    "\n",
    "### **10.2 Coherent Interference**\n",
    "\n",
    "To make the walk unitary (reversible), the quantum walker requires an internal degree of freedom—a \"quantum coin\" (e.g., Spin Up/Down).\n",
    "The Hilbert space is $\\mathcal{H} = \\mathcal{H}_{position} \\otimes \\mathcal{H}_{coin}$.\n",
    "\n",
    "#### **10.2.1 The Coin and Shift Operators**\n",
    "\n",
    "A single step consists of two operations:\n",
    "\n",
    "1.  **Coin Flip ($\\mathbf{C}$):** A Hadamard matrix mixes the spin states.\n",
    "    $$|n, \\uparrow\\rangle \\to \\frac{1}{\\sqrt{2}}(|n, \\uparrow\\rangle + |n, \\downarrow\\rangle)$$\n",
    "2.  **Conditional Shift ($\\mathbf{S}$):** Move according to the spin.\n",
    "      * $\\uparrow$ moves to $n+1$.\n",
    "      * $\\downarrow$ moves to $n-1$.\n",
    "\n",
    "The total evolution operator is $\\mathbf{W} = \\mathbf{S} \\cdot (\\mathbf{I} \\otimes \\mathbf{C})$.\n",
    "\n",
    "#### **10.2.2 Ballistic Transport**\n",
    "\n",
    "When we simulate this, the probability distribution does *not* look like a Bell curve. It looks like a \"dual-peak\" wave that rushes away from the origin.\n",
    "The width of the distribution grows linearly with time:\n",
    "$$\\sigma \\sim t$$\n",
    "This is **Ballistic Transport**.\n",
    "\n",
    "  * Classical: To explore $N$ nodes, time $T \\sim N^2$.\n",
    "  * Quantum: To explore $N$ nodes, time $T \\sim N$.\n",
    "    This quadratic speedup is the origin of the power of quantum search algorithms [2].\n",
    "\n",
    "### **10.3 Search as a Physical Process**\n",
    "\n",
    "The most famous application of quantum walking is **Grover's Search Algorithm**. While usually taught as \"amplitude amplification,\" physically it is a quantum walk on a **Complete Graph** (where every node is connected to every other node).\n",
    "\n",
    "#### **10.3.1 The Oracle Hamiltonian**\n",
    "\n",
    "Imagine a graph where one node $w$ (the winner) is a \"sink\" with a different potential energy.\n",
    "We construct a Hamiltonian $H = H_{kinetic} + H_{potential}$.\n",
    "\n",
    "1.  **Kinetic ($H_K$):** The Adjacency matrix of the complete graph (connects everything). This drives diffusion.\n",
    "2.  **Potential ($H_V$):** An Oracle term that applies a phase shift only at node $w$.\n",
    "    $$H_V = -|w\\rangle \\langle w|$$\n",
    "\n",
    "#### **10.3.2 Finding the Sink**\n",
    "\n",
    "Classically, on a complete graph, knowing you are at node $A$ tells you nothing about node $w$. You must hop randomly.\n",
    "Quantum mechanically, the \"Potential Well\" at $w$ modifies the spectrum of the Hamiltonian. The ground state becomes localized at $w$.\n",
    "If we initialize the system in a uniform superposition (the \"flat\" state), it is no longer an eigenstate. It effectively starts \"on the rim\" of the potential well.\n",
    "The system evolves (rotates) towards the bottom of the well ($|w\\rangle$). The time it takes to fall in is determined by the energy gap:\n",
    "$$T_{fall} \\sim \\frac{1}{\\Delta E} \\sim \\sqrt{N}$$\n",
    "Thus, Grover's algorithm is simply a particle sliding into a hole on a high-dimensional graph [3].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will simulate a 1D Quantum Walk and compare it directly to a Classical Random Walk to visualize the \"Ballistic vs. Diffusive\" behavior.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_walks(steps=100):\n",
    "    # Lattice size needs to be large enough to hold the walk\n",
    "    N = 2 * steps + 1\n",
    "    center = steps\n",
    "    \n",
    "    # --- Classical Walk ---\n",
    "    # p[n] = probability at site n\n",
    "    p_classical = np.zeros(N)\n",
    "    p_classical[center] = 1.0\n",
    "    \n",
    "    for t in range(steps):\n",
    "        new_p = np.zeros(N)\n",
    "        # p(n) comes from 0.5*p(n-1) + 0.5*p(n+1)\n",
    "        new_p[1:-1] = 0.5 * p_classical[:-2] + 0.5 * p_classical[2:]\n",
    "        p_classical = new_p\n",
    "\n",
    "    # --- Quantum Walk ---\n",
    "    # State is tensor of size (N, 2) for Spin Up/Down\n",
    "    # psi[n, 0] = Up, psi[n, 1] = Down\n",
    "    psi = np.zeros((N, 2), dtype=complex)\n",
    "    \n",
    "    # Initialize: Localized at center, Symmetric coin state |S> = (|0> + i|1>)\n",
    "    psi[center, 0] = 1.0 / np.sqrt(2)\n",
    "    psi[center, 1] = 1.0j / np.sqrt(2)\n",
    "    \n",
    "    # Hadamard Coin\n",
    "    H_coin = np.array([[1, 1], [1, -1]]) / np.sqrt(2)\n",
    "    \n",
    "    for t in range(steps):\n",
    "        # 1. Coin Step (Apply H to every site)\n",
    "        # We use Einstein summation for broadcasting matrix mult\n",
    "        psi = np.einsum('ij,nj->ni', H_coin, psi)\n",
    "        \n",
    "        # 2. Shift Step\n",
    "        new_psi = np.zeros_like(psi)\n",
    "        # Up (0) moves Right (index + 1)\n",
    "        new_psi[1:, 0] = psi[:-1, 0]\n",
    "        # Down (1) moves Left (index - 1)\n",
    "        new_psi[:-1, 1] = psi[1:, 1]\n",
    "        \n",
    "        psi = new_psi\n",
    "\n",
    "    # Calculate Probability P(n) = |Up|^2 + |Down|^2\n",
    "    p_quantum = np.abs(psi[:, 0])**2 + np.abs(psi[:, 1])**2\n",
    "    \n",
    "    # --- Visualization ---\n",
    "    x = np.arange(N) - center\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, p_classical, 'r--', label='Classical (Gaussian)', linewidth=2)\n",
    "    plt.plot(x, p_quantum, 'b-', label='Quantum (Ballistic)', linewidth=2)\n",
    "    \n",
    "    plt.title(f\"Random Walk vs. Quantum Walk (t={steps})\")\n",
    "    plt.xlabel(\"Position\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_walks(steps=100)\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] J. Kempe, \"Quantum random walks: an introductory overview,\" *Contemporary Physics* **44**, 307 (2003).  \n",
    "[2] A. M. Childs, \"Universal computation by quantum walk,\" *Physical Review Letters* **102**, 180501 (2009).  \n",
    "[3] E. Farhi and S. Gutmann, \"Analog analogue of a digital quantum computation,\" *Physical Review A* **57**, 2403 (1998)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2e4240",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 11** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 4: Numerical Solvers (The Engine Room)**\n",
    "\n",
    "#### **Chapter 11: Exact Diagonalization (ED)**\n",
    "\n",
    "In the first three sections of this book, we constructed Hamiltonians for everything from single particles to complex networks. We wrote down matrices $\\mathbf{H}$ and asked for their eigenvalues $E$ and eigenvectors $\\vec{\\psi}$.\n",
    "\n",
    "But how do we actually find them?\n",
    "For a $2 \\times 2$ matrix, we solve the quadratic characteristic equation on paper.\n",
    "For a $100,000 \\times 100,000$ matrix, paper is useless. We need algorithms.\n",
    "\n",
    "This section opens the hood of the computational engine. We explore **Exact Diagonalization (ED)**, the brute-force approach to solving the Schrödinger equation. While it is limited by memory (the \"Exponential Wall\" discussed in Volume II), it is the gold standard for accuracy.\n",
    "\n",
    "### **11.1 Dense Solvers**\n",
    "\n",
    "The most straightforward way to solve $\\mathbf{H}\\vec{\\psi} = E\\vec{\\psi}$ is to treat $\\mathbf{H}$ as a dense table of numbers and use standard linear algebra libraries.\n",
    "\n",
    "#### **11.1.1 The Black Box (LAPACK)**\n",
    "\n",
    "Behind almost every scientific software (NumPy, MATLAB, Mathematica) sits a library called **LAPACK** (Linear Algebra PACKage). It uses algorithms like the QR algorithm or Divide-and-Conquer to diagonalize symmetric matrices.\n",
    "\n",
    "If your system size $N$ is small ($N < 5000$), you should simply use these dense solvers.\n",
    "\n",
    "  * **Storage:** Requires storing $N^2$ floating-point numbers.\n",
    "      * $N=5000 \\implies 25 \\times 10^6$ doubles $\\approx 200$ MB. (Trivial).\n",
    "      * $N=50,000 \\implies 25 \\times 10^8$ doubles $\\approx 20$ GB. (Possible on a workstation).\n",
    "  * **Complexity:** The time to diagonalize scales as $O(N^3)$. Doubling the system size makes the calculation $8\\times$ slower.\n",
    "\n",
    "#### **11.1.2 Full Spectrum vs. Ground State**\n",
    "\n",
    "Dense solvers typically return **all** $N$ eigenvalues and eigenvectors.\n",
    "\n",
    "  * *Pros:* You get the complete picture: thermodynamics, excited states, and time evolution.\n",
    "  * *Cons:* It is wasteful if you only care about the Ground State [1].\n",
    "\n",
    "### **11.2 Sparse Matrices**\n",
    "\n",
    "Most physical Hamiltonians are **Sparse**.\n",
    "Consider the 1D Kinetic Matrix (Chapter 2). A site $i$ connects only to $i-1$ and $i+1$.\n",
    "In a matrix of size $1000 \\times 1000$ (1 million entries), only roughly 3000 entries are non-zero. The matrix is 99.7% empty space.\n",
    "\n",
    "Storing the zeros is a waste of memory. We need a format that stores only the \"muscle\" of the matrix.\n",
    "\n",
    "#### **11.2.1 Compressed Sparse Row (CSR)**\n",
    "\n",
    "The industry standard for static sparse matrices is the **Compressed Sparse Row (CSR)** format. It represents the matrix using three 1D arrays:\n",
    "\n",
    "1.  **Values:** A list of all non-zero numbers (length $NNZ$).\n",
    "2.  **Indices:** The column index for each value (length $NNZ$).\n",
    "3.  **Ptr (Pointers):** Where each row starts in the Values list (length $N+1$).\n",
    "\n",
    "This compression is dramatic. A Hamiltonian for a spin chain with $N=20$ sites has dimension $2^{20} \\approx 10^6$.\n",
    "\n",
    "  * **Dense:** $10^{12}$ entries (8 Terabytes). Impossible.\n",
    "  * **Sparse:** $\\approx 10 \\times 10^6$ entries (100 Megabytes). Trivial.\n",
    "\n",
    "However, standard algorithms (like QR) destroy sparsity. They \"fill in\" the zeros during Gaussian elimination. To diagonalize sparse matrices, we need **Iterative Methods** that only require multiplying a vector by a matrix ($\\mathbf{H}\\vec{v}$) without ever modifying $\\mathbf{H}$ itself [2].\n",
    "\n",
    "### **11.3 The Power Method**\n",
    "\n",
    "The simplest iterative method is the **Power Method**. It finds the dominant eigenvalue (the one with the largest absolute magnitude).\n",
    "\n",
    "#### **11.3.1 The Algorithm**\n",
    "\n",
    "Let the eigenvalues of $\\mathbf{H}$ be sorted $|\\lambda_0| > |\\lambda_1| > \\dots$.\n",
    "We start with a random vector $\\vec{v}^{(0)}$. We can write this vector as a sum of the eigenvectors $\\vec{u}_i$:\n",
    "$$\\vec{v}^{(0)} = c_0 \\vec{u}_0 + c_1 \\vec{u}_1 + \\dots$$\n",
    "\n",
    "Now, apply the Hamiltonian $\\mathbf{H}$ repeatedly $k$ times:\n",
    "$$\\mathbf{H}^k \\vec{v}^{(0)} = c_0 \\lambda_0^k \\vec{u}_0 + c_1 \\lambda_1^k \\vec{u}_1 + \\dots$$\n",
    "\n",
    "Factor out the dominant term $\\lambda_0^k$:\n",
    "$$\\mathbf{H}^k \\vec{v}^{(0)} = \\lambda_0^k \\left[ c_0 \\vec{u}_0 + c_1 \\left(\\frac{\\lambda_1}{\\lambda_0}\\right)^k \\vec{u}_1 + \\dots \\right]$$\n",
    "\n",
    "Since $|\\lambda_1 / \\lambda_0| < 1$, the ratio raised to the power $k$ decays to zero.\n",
    "As $k \\to \\infty$, the vector converges perfectly to the dominant eigenvector $\\vec{u}_0$.\n",
    "\n",
    "#### **11.3.2 Finding the Ground State (Inverse Iteration)**\n",
    "\n",
    "In physics, the \"dominant\" eigenvalue is usually the one with the largest magnitude (highest energy). We usually want the **Ground State** (lowest energy, often near 0 or negative).\n",
    "To find the lowest energy, we can apply the Power Method to the operator $(\\mathbf{H} - \\sigma \\mathbf{I})^{-1}$. This converges to the eigenvalue closest to the shift $\\sigma$.\n",
    "Alternatively, for bounded Hamiltonians, we can shift the spectrum: $\\mathbf{H}' = E_{max}\\mathbf{I} - \\mathbf{H}$. The lowest energy of $\\mathbf{H}$ becomes the largest magnitude eigenvalue of $\\mathbf{H}'$, accessible via the Power Method [3].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will compare the memory usage of Dense vs. Sparse matrices and implement the Power Method from scratch to find the ground state of a random Hamiltonian.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def numerical_solvers_demo():\n",
    "    # 1. Define System Size\n",
    "    N = 2000  # Dimension of the Hilbert Space\n",
    "    \n",
    "    # 2. Create a Sparse Hamiltonian (Random Hermitian Matrix)\n",
    "    # We create a random sparse matrix with ~5 non-zeros per row\n",
    "    density = 5.0 / N\n",
    "    H_sparse = sp.random(N, N, density=density, format='csr')\n",
    "    # Make it Hermitian\n",
    "    H_sparse = H_sparse + H_sparse.H\n",
    "    \n",
    "    print(f\"Matrix Dimension: {N}x{N}\")\n",
    "    print(f\"Non-zero elements: {H_sparse.nnz}\")\n",
    "    print(f\"Sparsity: {H_sparse.nnz / (N**2) * 100:.4f}%\")\n",
    "    \n",
    "    # 3. Dense Solver (NumPy) - The \"Black Box\"\n",
    "    print(\"\\n--- Dense Solver (eigh) ---\")\n",
    "    start_time = time.time()\n",
    "    H_dense = H_sparse.toarray() # Convert to dense (Consumes memory!)\n",
    "    evals_dense, _ = np.linalg.eigh(H_dense)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Time: {end_time - start_time:.4f} s\")\n",
    "    print(f\"Ground State Energy: {evals_dense[0]:.6f}\")\n",
    "    \n",
    "    # 4. The Power Method (Custom Implementation)\n",
    "    # To find the largest magnitude eigenvalue (Dominant)\n",
    "    print(\"\\n--- Power Method ---\")\n",
    "    \n",
    "    # Start with random vector\n",
    "    v = np.random.rand(N)\n",
    "    v = v / np.linalg.norm(v)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    iterations = 50\n",
    "    convergence = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Matrix-Vector Multiplication (Very fast for sparse!)\n",
    "        v_next = H_sparse.dot(v)\n",
    "        \n",
    "        # Rayleight Quotient (Estimate Eigenvalue)\n",
    "        eigenvalue_est = np.dot(v.conj(), v_next)\n",
    "        convergence.append(eigenvalue_est)\n",
    "        \n",
    "        # Normalize\n",
    "        v = v_next / np.linalg.norm(v_next)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Time: {end_time - start_time:.4f} s\")\n",
    "    print(f\"Dominant Eigenvalue found: {convergence[-1]:.6f}\")\n",
    "    print(f\"True Dominant (Max Abs): {np.max(np.abs(evals_dense)):.6f}\")\n",
    "    \n",
    "    # Visualization of Convergence\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(convergence, 'o-', label='Power Method Estimate')\n",
    "    plt.axhline(y=evals_dense[-1], color='r', linestyle='--', label='True Max Eigenvalue')\n",
    "    plt.axhline(y=evals_dense[0], color='g', linestyle='--', label='True Min Eigenvalue')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Eigenvalue Estimate\")\n",
    "    plt.title(\"Convergence of the Power Method\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    numerical_solvers_demo()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] G. H. Golub and C. F. Van Loan, *Matrix Computations*, 4th ed. (Johns Hopkins University Press, 2013).  \n",
    "[2] Y. Saad, *Iterative Methods for Sparse Linear Systems*, 2nd ed. (SIAM, 2003).  \n",
    "[3] L. N. Trefethen and D. Bau III, *Numerical Linear Algebra* (SIAM, 1997)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19d697",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 12** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 4: Numerical Solvers (The Engine Room)**\n",
    "\n",
    "#### **Chapter 12: The Lanczos Algorithm**\n",
    "\n",
    "In Chapter 11, we saw that the Power Method can find the single largest eigenvalue of a sparse matrix. But what if we want the first 10 excited states? Or what if we want faster convergence?\n",
    "\n",
    "The Power Method discards information. In every step $k$, we compute a vector $\\mathbf{H}^k \\vec{v}_0$ and throw away the previous vector $\\mathbf{H}^{k-1} \\vec{v}_0$.\n",
    "The **Lanczos Algorithm** keeps this history. By analyzing the entire sequence of vectors generated during the iteration, it constructs a \"miniature model\" of the Hamiltonian that captures the essential physics of the low-energy spectrum with incredible efficiency. It is the workhorse of computational quantum mechanics.\n",
    "\n",
    "### **12.1 Krylov Subspaces**\n",
    "\n",
    "#### **12.1.1 The Information in the Powers**\n",
    "\n",
    "We start with a random vector $|\\phi_0\\rangle$. We apply the Hamiltonian repeatedly to generate a sequence:\n",
    "$$|\\phi_0\\rangle, \\mathbf{H}|\\phi_0\\rangle, \\mathbf{H}^2|\\phi_0\\rangle, \\dots, \\mathbf{H}^{m-1}|\\phi_0\\rangle$$\n",
    "\n",
    "The span of these vectors is called the **Krylov Subspace** of dimension $m$:\n",
    "$$\\mathcal{K}_m(\\mathbf{H}, \\phi_0) = \\text{span}\\{ |\\phi_0\\rangle, \\mathbf{H}|\\phi_0\\rangle, \\dots, \\mathbf{H}^{m-1}|\\phi_0\\rangle \\}$$\n",
    "\n",
    "Intuitively, $\\mathbf{H}$ acts as a \"filter.\" High powers $\\mathbf{H}^k$ emphasize the eigenstates with large eigenvalues. By keeping the lower powers, we retain information about the rest of the spectrum [1].\n",
    "\n",
    "#### **12.1.2 The Lanczos Projection**\n",
    "\n",
    "The raw powers $\\mathbf{H}^k|\\phi_0\\rangle$ all tend to point in the same direction (the dominant eigenvector). They are highly linearly dependent.\n",
    "To make a useful basis, we must **orthogonalize** them. We use the Gram-Schmidt process.\n",
    "Let $|v_0\\rangle = |\\phi_0\\rangle$.\n",
    "\n",
    "1.  Apply $\\mathbf{H}$ to $|v_0\\rangle$.\n",
    "2.  Subtract the component parallel to $|v_0\\rangle$ to get a new orthogonal direction $|v_1\\rangle$.\n",
    "3.  Apply $\\mathbf{H}$ to $|v_1\\rangle$.\n",
    "4.  Subtract components parallel to $|v_1\\rangle$ and $|v_0\\rangle$ to get $|v_2\\rangle$.\n",
    "\n",
    "Remarkably, for a Hermitian matrix, you only need to subtract the last two vectors. The recursion relation is simple (The Three-Term Recurrence):\n",
    "$$\\beta_{n+1} |v_{n+1}\\rangle = \\mathbf{H} |v_n\\rangle - \\alpha_n |v_n\\rangle - \\beta_n |v_{n-1}\\rangle$$\n",
    "\n",
    "#### **12.1.3 The Tridiagonal Matrix**\n",
    "\n",
    "In this new basis $\\{ |v_n\\rangle \\}$, the Hamiltonian $\\mathbf{H}$ (which might be $10^9 \\times 10^9$) is projected down to a tiny $m \\times m$ matrix $\\mathbf{T}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{T}_m = \\begin{pmatrix}\n",
    "\\alpha_0 & \\beta_1 & 0 & \\dots \\\\\n",
    "\\beta_1 & \\alpha_1 & \\beta_2 & \\dots \\\\\n",
    "0 & \\beta_2 & \\alpha_2 & \\dots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We can easily diagonalize this small matrix on a laptop. Its eigenvalues (called **Ritz Values**) are excellent approximations of the eigenvalues of the giant Hamiltonian.\n",
    "\n",
    "### **12.2 Convergence**\n",
    "\n",
    "Why does this work?\n",
    "The Lanczos algorithm is essentially an optimization engine.\n",
    "The ground state energy $E_0$ is the minimum value of the Rayleigh Quotient:\n",
    "$$E_0 = \\min_{\\psi} \\frac{\\langle \\psi | \\mathbf{H} | \\psi \\rangle}{\\langle \\psi | \\psi \\rangle}$$\n",
    "\n",
    "When we solve for the eigenvalues of $\\mathbf{T}_m$, we are effectively finding the minimum of this quotient *restricted to the Krylov subspace*.\n",
    "Since the subspace grows with every iteration, the minimum can only get deeper.\n",
    "$$E_{Ritz}^{(m)} \\ge E_{Ritz}^{(m+1)} \\ge E_{exact}$$\n",
    "\n",
    "#### **Extreme Eigenvalues First**\n",
    "\n",
    "The convergence is not uniform. The **extreme eigenvalues** (lowest and highest energy) converge exponentially fast.\n",
    "\n",
    "  * With just $m=50$ iterations, we might find the ground state of a million-dimensional system to machine precision.\n",
    "  * The interior eigenvalues (in the middle of the spectrum) converge much slower.\n",
    "    This is perfect for physics, where we almost always care about the Ground State and the first few Low-Lying Excitations [2].\n",
    "\n",
    "### **12.3 Ghost Eigenvalues**\n",
    "\n",
    "In exact arithmetic, the Lanczos vectors $|v_n\\rangle$ are perfectly orthogonal.\n",
    "In floating-point arithmetic (computer reality), they are not.\n",
    "As the iterations proceed, round-off errors accumulate. The new vector $|v_{new}\\rangle$ effectively \"forgets\" that it should be orthogonal to the very first vector $|v_0\\rangle$. It starts to tilt back towards the dominant eigenvector.\n",
    "\n",
    "#### **12.3.1 The Appearance of Ghosts**\n",
    "\n",
    "When orthogonality is lost, the algorithm starts rediscovering the same eigenstate over and over again.\n",
    "If we plot the Ritz values vs. iteration number, we see \"Ghost\" eigenvalues appearing as duplicates of the true eigenvalues. A single physical ground state might appear as 5 distinct eigenvalues in the list.\n",
    "\n",
    "#### **12.3.2 The Fix: Re-orthogonalization**\n",
    "\n",
    "To cure this, we must actively police the orthogonality.\n",
    "\n",
    "  * **Full Re-orthogonalization:** At every step, explicitly orthogonalize $|v_{new}\\rangle$ against *all* previous vectors $|v_0\\rangle \\dots |v_{n-1}\\rangle$. This is expensive ($O(m^2)$ memory).\n",
    "  * **Restarting:** Run Lanczos for $m$ steps, find the best approximate ground state vector, and then **restart** the whole algorithm using that vector as the new $|\\phi_0\\rangle$ [3].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will implement the Lanczos algorithm to find the ground state energy of a large sparse matrix and visualize the convergence of the Ritz values.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def lanczos_algorithm():\n",
    "    # 1. Create a Large Sparse Matrix (Symmetric/Hermitian)\n",
    "    N = 1000\n",
    "    # A random sparse matrix representing a disordered Hamiltonian\n",
    "    # We add a shift to spread eigenvalues\n",
    "    H_sparse = sp.random(N, N, density=0.01, format='csr')\n",
    "    H_sparse = H_sparse + H_sparse.T # Make Hermitian\n",
    "    # Add diagonal to make it distinct\n",
    "    H_sparse.setdiag(np.random.rand(N) * 10)\n",
    "\n",
    "    # 2. Lanczos Parameters\n",
    "    m_steps = 50  # Size of Krylov Subspace (much smaller than N)\n",
    "    \n",
    "    # 3. Lanczos Iteration\n",
    "    # Storage for tridiagonal elements\n",
    "    alphas = np.zeros(m_steps)\n",
    "    betas = np.zeros(m_steps - 1)\n",
    "    \n",
    "    # Initial vector (normalized)\n",
    "    v_prev = np.zeros(N)\n",
    "    v_curr = np.random.rand(N)\n",
    "    v_curr /= np.linalg.norm(v_curr)\n",
    "    \n",
    "    ritz_history = []\n",
    "\n",
    "    print(f\"Starting Lanczos on {N}x{N} matrix for {m_steps} steps...\")\n",
    "\n",
    "    for j in range(m_steps):\n",
    "        # Apply H to v_j\n",
    "        w = H_sparse.dot(v_curr)\n",
    "        \n",
    "        # Alpha (Diagonal) = <v_j | H | v_j>\n",
    "        alpha = np.dot(v_curr, w)\n",
    "        alphas[j] = alpha\n",
    "        \n",
    "        # Orthogonalize: w' = w - alpha*v_j - beta*v_{j-1}\n",
    "        w = w - alpha * v_curr - (betas[j-1] * v_prev if j > 0 else 0)\n",
    "        \n",
    "        # Beta (Off-diagonal) = norm(w')\n",
    "        beta = np.linalg.norm(w)\n",
    "        \n",
    "        if j < m_steps - 1:\n",
    "            betas[j] = beta\n",
    "            # Update vectors for next step\n",
    "            if beta < 1e-10: # Check for convergence/invariant subspace\n",
    "                break\n",
    "            v_prev = v_curr\n",
    "            v_curr = w / beta\n",
    "            \n",
    "        # --- Monitoring Convergence (Optional) ---\n",
    "        # Build current T matrix and diagonalize to see eigenvalue evolution\n",
    "        if j > 0:\n",
    "            T_curr = np.diag(alphas[:j+1]) + np.diag(betas[:j], k=1) + np.diag(betas[:j], k=-1)\n",
    "            evals_T, _ = np.linalg.eigh(T_curr)\n",
    "            ritz_history.append(evals_T[0]) # Track ground state estimate\n",
    "\n",
    "    # 4. Final Diagonalization of T\n",
    "    T_final = np.diag(alphas) + np.diag(betas, k=1) + np.diag(betas, k=-1)\n",
    "    ritz_values, _ = np.linalg.eigh(T_final)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Estimated Ground State Energy: {ritz_values[0]:.6f}\")\n",
    "    \n",
    "    # Compare with \"Exact\" dense diagonalization (for validation)\n",
    "    # Note: Only possible because N=1000 is small enough to cheat check.\n",
    "    print(\"Computing exact eigenvalues for verification...\")\n",
    "    true_evals, _ = np.linalg.eigh(H_sparse.toarray())\n",
    "    print(f\"True Ground State Energy:      {true_evals[0]:.6f}\")\n",
    "    \n",
    "    # 5. Visualization\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(ritz_history, 'b.-', label='Lanczos Estimate')\n",
    "    plt.axhline(y=true_evals[0], color='r', linestyle='--', label='True Ground State')\n",
    "    plt.title(\"Lanczos Convergence: Ground State Energy\")\n",
    "    plt.xlabel(\"Iteration Step\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lanczos_algorithm()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] Y. Saad, *Iterative Methods for Sparse Linear Systems*, 2nd ed. (SIAM, 2003).  \n",
    "[2] B. N. Parlett, *The Symmetric Eigenvalue Problem* (SIAM, 1998).  \n",
    "[3] G. H. Golub and C. F. Van Loan, *Matrix Computations*, 4th ed. (Johns Hopkins University Press, 2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a1c77",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 13** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 4: Numerical Solvers (The Engine Room)**\n",
    "\n",
    "#### **Chapter 13: Time Stepping Methods**\n",
    "\n",
    "In Chapter 4, we learned that the time evolution of a quantum state is given by the matrix exponential $\\vec{\\psi}(t) = e^{-i\\mathbf{H}t} \\vec{\\psi}(0)$.\n",
    "In Chapter 11, we learned that for small systems, we can compute this exponential by full diagonalization.\n",
    "\n",
    "But what if the system is huge ($N=10^6$)? We cannot diagonalize the matrix. We cannot even store the dense matrix $e^{-i\\mathbf{H}t}$.\n",
    "We must simulate time evolution step-by-step, just as we simulate weather or planets. We divide time into tiny chunks $\\Delta t$.\n",
    "However, the standard tools of classical simulation (like Runge-Kutta) are dangerous in quantum mechanics. They tend to destroy the most important property of the wavefunction: **Probability Conservation**.\n",
    "\n",
    "### **13.1 Finite Difference in Time**\n",
    "\n",
    "#### **13.1.1 The Forward Euler Method**\n",
    "\n",
    "The simplest way to solve $\\frac{d\\vec{\\psi}}{dt} = -i\\mathbf{H}\\vec{\\psi}$ is the Forward Euler method:\n",
    "$$\\frac{\\vec{\\psi}(t+\\Delta t) - \\vec{\\psi}(t)}{\\Delta t} \\approx -i\\mathbf{H}\\vec{\\psi}(t)$$\n",
    "$$\\vec{\\psi}(t+\\Delta t) \\approx (\\mathbf{I} - i\\mathbf{H}\\Delta t) \\vec{\\psi}(t)$$\n",
    "\n",
    "This looks reasonable. It is the first-order Taylor expansion of $e^{-i\\mathbf{H}\\Delta t}$.\n",
    "\n",
    "#### **13.1.2 The Violation of Unitarity**\n",
    "\n",
    "Let's check if probability is conserved. Let the evolution operator be $\\mathbf{U}_{Euler} = \\mathbf{I} - i\\mathbf{H}\\Delta t$.\n",
    "For probability to be conserved, $\\mathbf{U}$ must be **Unitary** ($\\mathbf{U}^\\dagger \\mathbf{U} = \\mathbf{I}$).\n",
    "\n",
    "$$\\mathbf{U}^\\dagger \\mathbf{U} = (\\mathbf{I} + i\\mathbf{H}\\Delta t)(\\mathbf{I} - i\\mathbf{H}\\Delta t)$$\n",
    "$$= \\mathbf{I} + i\\mathbf{H}\\Delta t - i\\mathbf{H}\\Delta t + \\mathbf{H}^2 (\\Delta t)^2$$\n",
    "$$= \\mathbf{I} + \\mathbf{H}^2 (\\Delta t)^2$$\n",
    "\n",
    "The norm is not 1. It is $1 + O(\\Delta t^2)$.\n",
    "Since $\\mathbf{H}^2$ is positive definite, the norm is strictly **greater than 1**.\n",
    "Every time step, the probability vector grows. The energy of the system explodes. Euler's method is unconditionally **unstable** for the Schrödinger equation [1].\n",
    "\n",
    "### **13.2 Crank-Nicolson**\n",
    "\n",
    "To fix this, we need an operator that is strictly unitary, regardless of the step size $\\Delta t$.\n",
    "We use an **Implicit Method**. We evaluate the derivative at the midpoint of the interval:\n",
    "$$\\frac{\\vec{\\psi}(t+\\Delta t) - \\vec{\\psi}(t)}{\\Delta t} = -i\\mathbf{H} \\frac{\\vec{\\psi}(t+\\Delta t) + \\vec{\\psi}(t)}{2}$$\n",
    "\n",
    "Rearranging terms to put unknowns ($\\psi(t+\\Delta t)$) on the left and knowns ($\\psi(t)$) on the right:\n",
    "$$(\\mathbf{I} + i\\frac{\\Delta t}{2} \\mathbf{H}) \\vec{\\psi}(t+\\Delta t) = (\\mathbf{I} - i\\frac{\\Delta t}{2} \\mathbf{H}) \\vec{\\psi}(t)$$\n",
    "\n",
    "$$\\vec{\\psi}(t+\\Delta t) = \\frac{\\mathbf{I} - i\\frac{\\Delta t}{2} \\mathbf{H}}{\\mathbf{I} + i\\frac{\\Delta t}{2} \\mathbf{H}} \\vec{\\psi}(t)$$\n",
    "\n",
    "#### **13.2.1 Cayley's Form**\n",
    "\n",
    "The evolution operator is now a **Cayley Transform** of the Hamiltonian:\n",
    "$$\\mathbf{U}_{CN} = (\\mathbf{I} + i\\epsilon \\mathbf{H})^{-1} (\\mathbf{I} - i\\epsilon \\mathbf{H})$$\n",
    "This operator is perfectly unitary:\n",
    "$$|\\frac{1-ix}{1+ix}| = 1$$\n",
    "The Crank-Nicolson method is **unconditionally stable**. It preserves probability exactly, even for large time steps.\n",
    "\n",
    "#### **13.2.2 The Cost**\n",
    "\n",
    "The catch is the inverse $(\\mathbf{I} + i\\epsilon \\mathbf{H})^{-1}$. We must solve a linear system of equations $\\mathbf{A}\\vec{x} = \\vec{b}$ at *every single time step*.\n",
    "For 1D systems, the matrix is tridiagonal, so this is fast ($O(N)$). For 2D or 3D systems, solving the linear system is very expensive [2].\n",
    "\n",
    "### **13.3 Trotter-Suzuki Decomposition**\n",
    "\n",
    "To avoid solving linear systems, we look at the structure of the Hamiltonian:\n",
    "$$\\mathbf{H} = \\mathbf{T} + \\mathbf{V}$$\n",
    "We want to compute $e^{-i(\\mathbf{T}+\\mathbf{V})\\Delta t}$.\n",
    "Since $\\mathbf{T}$ and $\\mathbf{V}$ do not commute ($[\\mathbf{X}, \\mathbf{P}] \\neq 0$), we cannot simply write $e^{\\mathbf{T}+\\mathbf{V}} = e^{\\mathbf{T}} e^{\\mathbf{V}}$.\n",
    "\n",
    "However, for small $\\Delta t$, we can approximate it. The **First-Order Trotter Decomposition** is:\n",
    "$$e^{-i(\\mathbf{T}+\\mathbf{V})\\Delta t} \\approx e^{-i\\mathbf{T}\\Delta t} e^{-i\\mathbf{V}\\Delta t} + O(\\Delta t^2)$$\n",
    "\n",
    "#### **13.3.1 Splitting the Operator**\n",
    "\n",
    "Why is this useful?\n",
    "\n",
    "1.  **Potential Step ($e^{-i\\mathbf{V}\\Delta t}$):** Since $\\mathbf{V}$ is diagonal in the **Position Basis**, its exponential is trivial. We just multiply every site $n$ by the phase $e^{-iV_n \\Delta t}$.\n",
    "2.  **Kinetic Step ($e^{-i\\mathbf{T}\\Delta t}$):** Since $\\mathbf{T}$ is diagonal in the **Momentum Basis**, its exponential is trivial in $k$-space.\n",
    "\n",
    "#### **13.3.2 The Split-Operator Algorithm**\n",
    "\n",
    "This gives us a lightning-fast algorithm using the Fast Fourier Transform (FFT):\n",
    "\n",
    "1.  **Potential Kick:** Multiply $\\psi(x)$ by $e^{-iV(x)\\Delta t/2}$.\n",
    "2.  **FFT:** Transform to momentum space $\\psi(k)$.\n",
    "3.  **Kinetic Drift:** Multiply by $e^{-i \\frac{\\hbar^2 k^2}{2m} \\Delta t}$.\n",
    "4.  **IFFT:** Transform back to position space $\\psi(x)$.\n",
    "5.  **Potential Kick:** Multiply by $e^{-iV(x)\\Delta t/2}$.\n",
    "\n",
    "This symmetric sequence (V/2 - T - V/2) is the **Second-Order Trotter (Strang Splitting)**, with error $O(\\Delta t^3)$.\n",
    "It is Unitary (since FFT and Phase multiplication are unitary), Symplectic, and incredibly fast ($N \\log N$) [3].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will simulate a Gaussian wave packet hitting a barrier using all three methods to demonstrate the instability of Euler and the stability of Trotter.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse.linalg import splu\n",
    "\n",
    "def time_stepping_comparison():\n",
    "    # 1. System Parameters\n",
    "    N = 256\n",
    "    L = 100.0\n",
    "    dx = L / N\n",
    "    x = np.linspace(0, L, N)\n",
    "    dt = 0.1\n",
    "    steps = 100\n",
    "    \n",
    "    # 2. Initial State (Gaussian)\n",
    "    psi = np.exp(-(x - 30)**2 / (2 * 5**2)) * np.exp(1j * 1.0 * x)\n",
    "    psi /= np.linalg.norm(psi)\n",
    "    \n",
    "    # 3. Operators\n",
    "    # Potential V (Barrier)\n",
    "    V = np.zeros(N)\n",
    "    V[N//2 : N//2 + 20] = 2.0\n",
    "    \n",
    "    # Kinetic T (Finite Difference Matrix for Crank-Nicolson)\n",
    "    ones = np.ones(N)\n",
    "    T_mat = diags([-ones, 2*ones, -ones], [-1, 0, 1], shape=(N, N)) / (2 * dx**2)\n",
    "    \n",
    "    # Hamiltonian\n",
    "    H = T_mat + diags([V], [0], shape=(N, N))\n",
    "    \n",
    "    # 4. Solvers\n",
    "    \n",
    "    # A. Euler (Unstable)\n",
    "    psi_euler = psi.copy()\n",
    "    norm_euler = []\n",
    "    \n",
    "    # B. Crank-Nicolson (Implicit)\n",
    "    # (I + i dt/2 H) psi_new = (I - i dt/2 H) psi_old\n",
    "    # Ax = b\n",
    "    I = diags([ones], [0], shape=(N, N))\n",
    "    A_cn = I + 1j * (dt / 2) * H\n",
    "    B_cn = I - 1j * (dt / 2) * H\n",
    "    solve_cn = splu(A_cn).solve # Pre-factorize for speed\n",
    "    \n",
    "    psi_cn = psi.copy()\n",
    "    norm_cn = []\n",
    "    \n",
    "    # C. Trotter-Suzuki (Split Operator)\n",
    "    # Pre-compute phase factors\n",
    "    k = 2 * np.pi * np.fft.fftfreq(N, d=dx)\n",
    "    exp_T = np.exp(-1j * (k**2 / 2) * dt)\n",
    "    exp_V = np.exp(-1j * V * dt)\n",
    "    \n",
    "    psi_trot = psi.copy()\n",
    "    norm_trot = []\n",
    "    \n",
    "    # 5. Evolution Loop\n",
    "    for _ in range(steps):\n",
    "        # --- Euler ---\n",
    "        # psi += -i * H * psi * dt\n",
    "        psi_euler = psi_euler - 1j * H.dot(psi_euler) * dt\n",
    "        norm_euler.append(np.linalg.norm(psi_euler))\n",
    "        \n",
    "        # --- Crank-Nicolson ---\n",
    "        rhs = B_cn.dot(psi_cn)\n",
    "        psi_cn = solve_cn(rhs)\n",
    "        norm_cn.append(np.linalg.norm(psi_cn))\n",
    "        \n",
    "        # --- Trotter (Split Step) ---\n",
    "        # V/2 (Half step)\n",
    "        psi_trot *= np.exp(-1j * V * dt / 2)\n",
    "        # T (Full step in k-space)\n",
    "        psi_k = np.fft.fft(psi_trot)\n",
    "        psi_k *= exp_T\n",
    "        psi_trot = np.fft.ifft(psi_k)\n",
    "        # V/2 (Half step)\n",
    "        psi_trot *= np.exp(-1j * V * dt / 2)\n",
    "        \n",
    "        norm_trot.append(np.linalg.norm(psi_trot))\n",
    "\n",
    "    # 6. Visualization\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(norm_euler, label='Euler (Explodes)', linestyle='--')\n",
    "    plt.plot(norm_cn, label='Crank-Nicolson (Stable)')\n",
    "    plt.plot(norm_trot, label='Trotter (Stable & Fast)', linestyle='-.')\n",
    "    plt.axhline(1.0, color='k', linewidth=0.5)\n",
    "    plt.title(\"Conservation of Probability (Norm)\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(r\"$\\langle \\psi | \\psi \\rangle$\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    time_stepping_comparison()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery, *Numerical Recipes: The Art of Scientific Computing*, 3rd ed. (Cambridge University Press, 2007).  \n",
    "[2] J. Crank and P. Nicolson, \"A practical method for numerical evaluation of solutions of partial differential equations of the heat-conduction type,\" *Proc. Camb. Phil. Soc.* **43**, 50 (1947).  \n",
    "[3] M. Suzuki, \"Fractal decomposition of exponential operators with applications to many-body theories and Monte Carlo simulations,\" *Physics Letters A* **146**, 319 (1990)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
