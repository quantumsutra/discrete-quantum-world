{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fdd2610",
   "metadata": {},
   "source": [
    "# **Quantum Physics of the Discrete World**\n",
    "**Subtitle:** *From Lattice Mechanics to Quantum Engineering*\n",
    "**Series:** Springer Graduate Texts in Physics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774395df",
   "metadata": {},
   "source": [
    "## **Part III: Real-World Applications & Engineering**\n",
    "**Subtitle:** *From Hamiltonians to Hardware*\n",
    "\n",
    "**Pedagogical Goal:** To teach the reader how to act as a \"Quantum Software Engineer\"—mastering the modern \"Standard Model\" of algorithms (QSVT, Schrödingerization, Q-SciML) to model complex dynamical systems.\n",
    "\n",
    "### **Section 10: The Algorithmic Bridge (The Modern Standard Model)**\n",
    "*Moving beyond simple circuit construction to the unified frameworks of modern quantum linear algebra.*\n",
    "\n",
    "#### **Chapter 30: Optimization as Physics (QUBO & Ising)**\n",
    "* **30.1 The Cost Function Landscape:** Mapping TSP to an energy landscape.\n",
    "* **30.2 The Ising Embedding:** Mapping binary decisions to Spins.\n",
    "* **30.3 The Penalty Term:** Implementing constraints (Logistics/Capacity) via diagonal penalties.\n",
    "* **30.4 Real-World Case Study:** Quantum Logistics and Traffic Flow optimization.\n",
    "\n",
    "#### **Chapter 31: Matrix Arithmetics: Block Encodings & QSVT**\n",
    "* **31.1 Block Encoding:** The art of embedding non-unitary matrices ($A$) into larger unitary operators ($U$).\n",
    "* **31.2 Quantum Singular Value Transformation (QSVT):** A unified framework generalizing HHL, Grover, and Hamiltonian Simulation via polynomial transformations of singular values.\n",
    "* **31.3 The Matrix Inversion Poly:** Implementing $A^{-1}$ by designing a polynomial $P(x) \\approx 1/x$.\n",
    "\n",
    "#### **Chapter 32: Simulating Reality: Schrödingerization & Carleman Linearization**\n",
    "* **32.1 Schrödingerization:** The \"Warped Phase\" transformation to map dissipative systems (Heat Equation) to unitary quantum evolution.\n",
    "* **32.2 Boundary Conditions:** Handling Dirichlet and Neumann boundaries in the quantum register.\n",
    "* **32.3 Taming Non-Linearity (Carleman Linearization):** Mapping non-linear fluids (Navier-Stokes, Burgers' Eq) to infinite-dimensional linear systems via tensor powers.\n",
    "\n",
    "### **Section 11: Real-World Modeling**\n",
    "*Treating the Market, Life, and Data as Many-Body Systems.*\n",
    "\n",
    "#### **Chapter 33: Quantum Finance (Econophysics)**\n",
    "* **33.1 The Arbitrage Hamiltonian:** Assets as nodes, trading as hopping.\n",
    "* **33.2 Schrödingerizing Finance:** Solving the Black-Scholes-Merton equation directly via the warped phase mapping.\n",
    "* **33.3 Amplitude Estimation:** Accelerating Monte Carlo pricing for path-dependent derivatives (The Quadratic Speedup).\n",
    "\n",
    "#### **Chapter 34: Quantum Scientific Machine Learning (Q-SciML)**\n",
    "* **34.1 The Koopman Operator:** Linearizing chaotic classical dynamics by mapping observables to an infinite-dimensional Hilbert space.\n",
    "* **34.2 Geometric Deep Learning:** Constructing Group Equivariant Quantum Neural Networks (QNNs) that respect physical symmetries.\n",
    "* **34.3 Quantum Kernels:** Using Hilbert space as a feature map for Support Vector Machines.\n",
    "\n",
    "#### **Chapter 35: Quantum Biology & Neuroscience**\n",
    "* **35.1 The Photosynthetic Graph:** The FMO complex.\n",
    "* **35.2 Environment-Assisted Transport (ENAQT):** Noise-assisted efficiency.\n",
    "* **35.3 The Quantum Connectome:** Neural networks as Tensor Networks.\n",
    "\n",
    "### **Section 12: Execution (The Hardware Layer)**\n",
    "*Running on NISQ devices and the path to Fault Tolerance.*\n",
    "\n",
    "#### **Chapter 36: Variational Quantum Algorithms (VQE & QAOA)**\n",
    "* **36.1 The Hybrid Loop:** Classical optimization of Quantum parameters.\n",
    "* **36.2 Hamiltonian Averaging:** Sampling Pauli strings.\n",
    "* **36.3 QAOA:** Solving combinatorial problems via adiabatic approximation.\n",
    "\n",
    "#### **Chapter 37: Dynamic Circuits & Error Mitigation**\n",
    "* **37.1 Zero-Noise Extrapolation (ZNE):** Mitigating errors by amplifying noise and extrapolating to the zero limit.\n",
    "* **37.2 Mid-Circuit Measurement:** Implementing feed-forward logic for adaptive state preparation and teleportation.\n",
    "* **37.3 Randomized Benchmarking:** Verifying gate fidelity.\n",
    "\n",
    "#### **Chapter 38: The Road to Fault Tolerance (LDPC)**\n",
    "* **38.1 Beyond the Surface Code:** The shift toward Quantum Low-Density Parity-Check (qLDPC) codes for constant-rate encoding. \n",
    "* **38.2 Magic States:** The resource required to perform non-Clifford gates.\n",
    "* **38.3 It From Bit:** Concluding thoughts on the discrete nature of reality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e7821",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 30** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "# **Part III: Real-World Applications & Engineering**\n",
    "\n",
    "## **Subtitle:** *From Hamiltonians to Hardware*\n",
    "\n",
    "**Pedagogical Goal:** To teach the reader how to act as a \"Quantum Software Engineer\"—mastering the modern \"Standard Model\" of algorithms (QSVT, Schrödingerization, Q-SciML) to model complex dynamical systems.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 10: The Algorithmic Bridge (The Modern Standard Model)**\n",
    "\n",
    "#### **Chapter 30: Optimization as Physics (QUBO & Ising)**\n",
    "\n",
    "In Volumes I and II, we studied Hamiltonians given to us by nature (Hydrogen atoms, Crystals, Magnets). We asked: *\"What is the ground state of this system?\"*\n",
    "\n",
    "In Part III, we invert the process. We start with a human problem—a supply chain, a financial portfolio, or a traffic grid—and we ask: *\"How can I build a Hamiltonian whose ground state is the solution to my problem?\"*\n",
    "\n",
    "This approach relies on the **Principle of Minimum Energy**. Nature inherently seeks the ground state. If we can map the \"Cost\" of a business problem to the \"Energy\" of a physical system, we can let the laws of physics solve the optimization problem for us.\n",
    "\n",
    "### **30.1 The Cost Function Landscape**\n",
    "\n",
    "#### **30.1.1 The Combinatorial Explosion**\n",
    "\n",
    "Consider the **Traveling Salesman Problem (TSP)**: A truck must visit $N$ cities exactly once and return home. The goal is to minimize the total distance.\n",
    "\n",
    "  * **Classically:** We must check $N!$ possible routes. For 50 cities, this is more than the number of atoms in the universe.\n",
    "  * **Physically:** We treat the route as a \"state\" $|\\psi\\rangle$ and the total distance as the \"Energy\" $E$.\n",
    "\n",
    "#### **30.1.2 The Energy Landscape**\n",
    "\n",
    "Imagine a jagged mountain range.\n",
    "\n",
    "  * **x, y coordinates:** Represent the parameters (the chosen route).\n",
    "  * **z coordinate (Height):** Represents the Cost (Total Distance).\n",
    "\n",
    "Most random routes have high cost (High Energy peaks). The optimal route is the deepest valley (Global Minimum).\n",
    "Classical algorithms (like Gradient Descent) often get stuck in \"Local Minima\"—small valleys high up the mountain.\n",
    "Quantum algorithms (Quantum Annealing) use **Tunneling** to pass through the mountain barriers and find the true Global Minimum [1].\n",
    "\n",
    "### **30.2 The Ising Embedding**\n",
    "\n",
    "How do we mathematically translate a logical decision (\"Should the truck go to Boston?\") into a physical object?\n",
    "We map **Binary Variables** to **Quantum Spins**.\n",
    "\n",
    "#### **30.2.1 The Variable Map**\n",
    "\n",
    "  * **Classical Logic:** A variable $x_i$ can be $0$ (No) or $1$ (Yes).\n",
    "  * **Quantum Physics:** A spin $\\sigma^z_i$ can be $+1$ (Up) or $-1$ (Down).\n",
    "\n",
    "The transformation is a simple linear shift:\n",
    "$$x_i = \\frac{1 - \\sigma^z_i}{2}$$\n",
    "\n",
    "  * If Spin $\\uparrow$ ($+1$): $x = 0$.\n",
    "  * If Spin $\\downarrow$ ($-1$): $x = 1$.\n",
    "\n",
    "#### **30.2.2 The QUBO Matrix ($Q$)**\n",
    "\n",
    "Most optimization problems can be written as a **Quadratic Unconstrained Binary Optimization (QUBO)** problem. The cost function $C(\\vec{x})$ is defined as:\n",
    "\n",
    "$$C(\\vec{x}) = \\sum_i \\sum_j Q_{ij} x_i x_j$$\n",
    "\n",
    "Here, $\\mathbf{Q}$ is an upper-triangular matrix of real numbers.\n",
    "\n",
    "  * **Diagonal terms ($Q_{ii}$):** Represent the intrinsic cost of choosing $x_i$. (e.g., \"Visiting Tokyo is expensive\").\n",
    "  * **Off-Diagonal terms ($Q_{ij}$):** Represent the interaction cost. (e.g., \"If I visit Tokyo AND Osaka, I save money on fuel\").\n",
    "\n",
    "When we substitute the spin variables into the QUBO equation, we recover the familiar **Ising Model**:\n",
    "$$H_{Ising} = \\sum_i h_i \\sigma^z_i + \\sum_{i<j} J_{ij} \\sigma^z_i \\sigma^z_j + E_{offset}$$\n",
    "\n",
    "This is the \"Rosetta Stone\" of Quantum Computing: Business costs ($Q$) become magnetic fields ($h$) and couplings ($J$) [2].\n",
    "\n",
    "### **30.3 The Penalty Term**\n",
    "\n",
    "Real-world problems are rarely \"unconstrained.\" You cannot just minimize distance; you have rules.\n",
    "\n",
    "  * *Rule 1:* The truck has a max capacity.\n",
    "  * *Rule 2:* You cannot visit the same city twice.\n",
    "\n",
    "The Ising model has no \"If/Then\" logic statements. It only has energy. How do we enforce rules?\n",
    "We turn **Constraints** into **Energy Penalties**.\n",
    "\n",
    "#### **30.3.1 Soft Constraints vs. Hard Constraints**\n",
    "\n",
    "We add a \"Penalty Hamiltonian\" $H_P$ to our objective Hamiltonian $H_{obj}$.\n",
    "$$H_{total} = H_{obj} + \\lambda H_P$$\n",
    "Here, $\\lambda$ is a large number (the Lagrange Multiplier).\n",
    "\n",
    "  * **If the rule is followed:** $H_P = 0$. The system feels only the objective cost.\n",
    "  * **If the rule is broken:** $H_P > 0$. The total energy shoots up by $\\lambda$. Since the system seeks the ground state, it will naturally avoid these \"high energy\" forbidden configurations.\n",
    "\n",
    "#### **30.3.2 The \"One-Hot\" Constraint (Squared Difference)**\n",
    "\n",
    "The most common constraint is: \"Choose exactly one option from a set.\" (e.g., At time $t=1$, the truck must be in exactly one city).\n",
    "$$\\sum_{i} x_i = 1$$\n",
    "\n",
    "To turn this into a Hamiltonian, we move the 1 to the left side and **square the expression**:\n",
    "$$P(\\vec{x}) = \\left( \\sum_{i} x_i - 1 \\right)^2$$\n",
    "\n",
    "Why square it?\n",
    "\n",
    "1.  **Parabola:** A parabola has a minimum at 0. Any deviation (positive or negative) increases the value.\n",
    "2.  **Compatibility:** Squaring the sum generates terms like $x_i^2$ (Linear, since $x^2=x$ for binary) and $x_i x_j$ (Quadratic). This fits perfectly into the QUBO/Ising matrix structure.\n",
    "\n",
    "### **30.4 Real-World Case Study: Traffic Flow**\n",
    "\n",
    "Consider optimizing traffic flow in a city. We want to route $K$ cars through a network of roads to minimize congestion.\n",
    "\n",
    "1.  **Variables:** $x_{c, r} = 1$ if Car $c$ takes Route $r$.\n",
    "2.  **Constraint:** Each car must pick exactly one route.\n",
    "    $$H_{const} = \\lambda \\sum_c \\left( \\sum_r x_{c,r} - 1 \\right)^2$$\n",
    "3.  **Objective (Congestion):** The cost of a road segment $s$ depends on the number of cars using it. Cost $\\propto (\\text{Cars})^2$.\n",
    "    $$H_{obj} = \\sum_s \\left( \\sum_{c,r \\ni s} x_{c,r} \\right)^2$$\n",
    "\n",
    "Expanding this square creates terms $x_{c,r} x_{c',r'}$ whenever two cars share a road segment. This creates an antiferromagnetic coupling ($J > 0$) between those variables: the system lowers its energy if cars choose *different* routes to avoid the penalty term $J$.\n",
    "By diagonalizing this matrix (or annealing it), the system spontaneously organizes into a state of minimal traffic jams [3].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will implement a small QUBO solver for the \"Knapsack Problem\" (Maximize value of items in a bag with limited weight). We will construct the $Q$ matrix including the penalty term and solve it by brute force (exact diagonalization of the Ising Hamiltonian).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "\n",
    "def qubo_knapsack_solver():\n",
    "    # 1. Define Problem Instance\n",
    "    # Items: (Weight, Value)\n",
    "    items = [(2, 10), (3, 7), (4, 15), (1, 3)] \n",
    "    max_weight = 6\n",
    "    n_items = len(items)\n",
    "    \n",
    "    # We need auxiliary qubits (slack variables) to handle inequality constraints\n",
    "    # Constraint: Sum(w_i x_i) <= W_max\n",
    "    # Becomes: Sum(w_i x_i) + y = W_max  (y is integer slack)\n",
    "    # y = sum(2^k s_k) (Binary expansion of slack)\n",
    "    \n",
    "    # For simplicity in this demo, let's solve the EXACT WEIGHT constraint\n",
    "    # Sum(w_i x_i) = Target_Weight (e.g. 5)\n",
    "    target_weight = 5\n",
    "    \n",
    "    # 2. Hamiltonian Construction\n",
    "    # H = - (Value) + lambda * (Weight - Target)^2\n",
    "    lambda_penalty = 100.0\n",
    "    \n",
    "    # We need to build the Q matrix for H = xT Q x\n",
    "    Q = np.zeros((n_items, n_items))\n",
    "    \n",
    "    # Linear Terms (Diagonal of Q)\n",
    "    # 1. From Value: -v_i * x_i\n",
    "    # 2. From Penalty: lambda * (w_i^2 x_i^2 - 2*Target*w_i x_i)\n",
    "    # Note: x_i^2 = x_i\n",
    "    \n",
    "    for i in range(n_items):\n",
    "        w_i, v_i = items[i]\n",
    "        \n",
    "        # Value term (Negative because we minimize H)\n",
    "        Q[i, i] += -v_i\n",
    "        \n",
    "        # Penalty Linear part\n",
    "        # (Sum w x - T)^2 = (Sum w x)^2 - 2T(Sum w x) + T^2\n",
    "        # Linear part comes from -2T w_i x_i and the diagonal of (Sum w x)^2 which is w_i^2 x_i\n",
    "        Q[i, i] += lambda_penalty * (w_i**2 - 2 * target_weight * w_i)\n",
    "        \n",
    "    # Quadratic Terms (Off-diagonal of Q)\n",
    "    # From Penalty: lambda * (Sum_{i!=j} w_i w_j x_i x_j)\n",
    "    for i in range(n_items):\n",
    "        for j in range(i + 1, n_items):\n",
    "            w_i = items[i][0]\n",
    "            w_j = items[j][0]\n",
    "            \n",
    "            term = lambda_penalty * (2 * w_i * w_j)\n",
    "            Q[i, j] += term\n",
    "            # Q is usually Upper Triangular for QUBO solvers\n",
    "            \n",
    "    print(\"QUBO Matrix Q:\")\n",
    "    print(np.round(Q, 1))\n",
    "    \n",
    "    # 3. Solve (Brute Force over 2^N)\n",
    "    # We check every bitstring x\n",
    "    best_x = None\n",
    "    min_energy = float('inf')\n",
    "    \n",
    "    for i in range(2**n_items):\n",
    "        # Convert i to binary array x\n",
    "        x = np.array([int(c) for c in f\"{i:0{n_items}b}\"])\n",
    "        \n",
    "        # Energy = x.T @ Q @ x (Upper triangular multiply logic)\n",
    "        # Or simpler: sum Q_ij x_i x_j\n",
    "        energy = 0\n",
    "        for r in range(n_items):\n",
    "            for c in range(r, n_items): # Upper tri\n",
    "                if x[r] == 1 and x[c] == 1:\n",
    "                    energy += Q[r, c]\n",
    "                    \n",
    "        # Add the constant offset from penalty T^2 (optional for finding min x, but good for physics)\n",
    "        total_energy = energy + lambda_penalty * target_weight**2\n",
    "        \n",
    "        if total_energy < min_energy:\n",
    "            min_energy = total_energy\n",
    "            best_x = x\n",
    "            \n",
    "    # 4. Result\n",
    "    print(\"\\nOptimal Solution:\")\n",
    "    print(f\"Selection: {best_x}\")\n",
    "    total_w = sum(best_x[i] * items[i][0] for i in range(n_items))\n",
    "    total_v = sum(best_x[i] * items[i][1] for i in range(n_items))\n",
    "    print(f\"Total Weight: {total_w} (Target: {target_weight})\")\n",
    "    print(f\"Total Value: {total_v}\")\n",
    "    \n",
    "    if total_w == target_weight:\n",
    "        print(\"Constraint Satisfied: YES\")\n",
    "    else:\n",
    "        print(\"Constraint Satisfied: NO (Penalty too weak?)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    qubo_knapsack_solver()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] T. Kadowaki and H. Nishimori, \"Quantum annealing in the transverse Ising model,\" *Physical Review E* **58**, 5355 (1998).  \n",
    "[2] A. Lucas, \"Ising formulations of many NP problems,\" *Frontiers in Physics* **2**, 5 (2014).  \n",
    "[3] F. Neukart et al., \"Traffic flow optimization using a quantum annealer,\" *Frontiers in ICT* **4**, 29 (2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab2d06",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 31** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "# **Part III: Real-World Applications & Engineering**\n",
    "\n",
    "## **Subtitle:** *From Hamiltonians to Hardware*\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 10: The Algorithmic Bridge (The Modern Standard Model)**\n",
    "\n",
    "#### **Chapter 31: Matrix Arithmetics: Block Encodings & QSVT**\n",
    "\n",
    "In Chapter 30, we used quantum computers for optimization (finding the lowest energy). Now we turn to a more general problem: **Linear Algebra**.\n",
    "Almost every engineering calculation—from solving differential equations to filtering signals—boils down to manipulating matrices.\n",
    "\n",
    "  * $Ax = b$ (Inversion).\n",
    "  * $e^{-iAt}$ (Simulation).\n",
    "  * $A^k$ (Power iteration).\n",
    "\n",
    "However, quantum computers are restricted to **Unitary Matrices** ($U^\\dagger U = I$). This seems like a fatal flaw. Real-world matrices $A$ are rarely unitary. They shrink vectors, project them, or blow them up.\n",
    "\n",
    "This chapter introduces the \"Modern Standard Model\" of quantum algorithms: **Block Encoding** and **Quantum Singular Value Transformation (QSVT)**. These tools allow us to embed *any* matrix inside a unitary one, unlocking the full power of linear algebra on a quantum chip.\n",
    "\n",
    "### **31.1 Block Encoding**\n",
    "\n",
    "#### **31.1.1 The Embedding Problem**\n",
    "\n",
    "We want to perform an operation with a non-unitary matrix $A$ (size $N \\times N$) on a quantum state $|\\psi\\rangle$.\n",
    "We cannot implement $A$ directly. Instead, we construct a larger unitary matrix $U$ (size $M \\times M$, where $M > N$) that contains $A$ in its top-left corner.\n",
    "\n",
    "$$\n",
    "U = \\begin{pmatrix} A & \\cdot \\\\ \\cdot & \\cdot \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Mathematically, if we apply $U$ to the state $|0\\rangle_a |\\psi\\rangle_s$ (where $a$ is an ancilla register) and then project the ancilla back to $|0\\rangle$, we get $A$:\n",
    "$$(\\langle 0|_a \\otimes I_s) U (|0\\rangle_a \\otimes I_s) = A$$\n",
    "\n",
    "We say that $U$ is a **Block Encoding** of $A$. The parameter $\\alpha$ (the normalization factor) tells us how much we had to \"shrink\" $A$ to fit it inside the unitary unit circle ($||A/\\alpha|| \\le 1$).\n",
    "\n",
    "#### **31.1.2 Linear Combination of Unitaries (LCU)**\n",
    "\n",
    "How do we build such a $U$? The standard method is LCU (introduced in Chapter 4).\n",
    "Any matrix can be written as a sum of unitaries: $A = \\sum c_k U_k$.\n",
    "By using a control register to select which $U_k$ to apply, we effectively construct the block encoding.\n",
    "$$U_{LCU} = \\text{PREPARE}^\\dagger \\cdot \\text{SELECT} \\cdot \\text{PREPARE}$$\n",
    "This circuit implements $A$ probabilistically. We only succeed if we measure the ancilla as $|0\\rangle$.\n",
    "\n",
    "### **31.2 Quantum Singular Value Transformation (QSVT)**\n",
    "\n",
    "Once we have $A$ block-encoded inside $U$, what can we do with it?\n",
    "We usually don't want just $A$. We want to compute a function of $A$, such as $A^{-1}$, $e^{iA}$, or $\\text{step}(A)$.\n",
    "\n",
    "The **Grand Unification** of quantum algorithms came in 2019 with the discovery of QSVT. It showed that almost all previous algorithms (Grover, HHL, Phase Estimation) are just special cases of one geometric idea: **Polynomial Transformation of Singular Values**.\n",
    "\n",
    "#### **31.2.1 The Geometry of Rotations**\n",
    "\n",
    "Every matrix has a Singular Value Decomposition: $A = W \\Sigma V^\\dagger$.\n",
    "When we apply the block encoding $U$, we effectively rotate the state vector by an angle determined by the singular values $\\sigma_k$.\n",
    "\n",
    "  * If $\\sigma_k = 1$: The unitary does nothing to the ancilla.\n",
    "  * If $\\sigma_k < 1$: The unitary rotates the ancilla out of the $|0\\rangle$ subspace.\n",
    "\n",
    "#### **31.2.2 The Polynomial Sequence**\n",
    "\n",
    "QSVT applies an alternating sequence of two operators:\n",
    "\n",
    "1.  The Block Encoding $U$ (and its inverse $U^\\dagger$).\n",
    "2.  Phase Rotations $\\Pi_\\phi$ on the ancilla (Signal Processing).\n",
    "\n",
    "$$U_{QSVT} = \\Pi_{\\phi_d} U \\Pi_{\\phi_{d-1}} U^\\dagger \\dots \\Pi_{\\phi_1} U$$\n",
    "\n",
    "Remarkably, this sequence applies a polynomial function $P(x)$ to the singular values of $A$:\n",
    "$$U_{QSVT} \\approx \\begin{pmatrix} P(A) & \\cdot \\\\ \\cdot & \\cdot \\end{pmatrix}$$\n",
    "\n",
    "By carefully choosing the phase angles $\\vec{\\phi}$, we can implement **any** polynomial function $P(x)$ (bounded by $[-1, 1]$).\n",
    "\n",
    "### **31.3 The Matrix Inversion Poly**\n",
    "\n",
    "To solve $Ax=b$, we need to apply $A^{-1}$ to the vector $|b\\rangle$.\n",
    "In the QSVT framework, this means we must find a polynomial approximation to the function $f(x) = 1/x$.\n",
    "\n",
    "#### **31.3.1 Designing the Function**\n",
    "\n",
    "The function $1/x$ is unbounded near zero. This reflects the fact that inverting a singular matrix is impossible.\n",
    "We must define a domain $[\\kappa^{-1}, 1]$ where we approximate $1/x$.\n",
    "We design a polynomial $P(x)$ that wiggles close to $1/x$ in this region and stays bounded outside it.\n",
    "\n",
    "#### **31.3.2 The Algorithm**\n",
    "\n",
    "1.  **Block Encode:** Construct $U_A$ such that $A$ is in the corner.\n",
    "2.  **Phase Angles:** Use classical software (like the `pyqsp` package) to find the angles $\\vec{\\phi}$ that generate the polynomial $P(x) \\approx 1/x$.\n",
    "3.  **Execute:** Run the QSVT sequence on the quantum state $|b\\rangle$.\n",
    "4.  **Result:** The output is $|x\\rangle = P(A)|b\\rangle \\approx A^{-1}|b\\rangle$.\n",
    "\n",
    "This replaces the complex Phase Estimation logic of HHL with a simple, determinstic sequence of gates. It is the most efficient known algorithm for matrix inversion [1].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will use the `pyqsp` or a simplified numerical simulator to demonstrate how alternating rotations transform the singular values of a matrix. We will visualize the polynomial $P(x)$ generated by a sequence of phase angles.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def qsvt_polynomial_demo():\n",
    "    # 1. Define Target Polynomial (Approximation of 1/x)\n",
    "    # We want to invert singular values in range [0.2, 1.0]\n",
    "    # For demonstration, we manually define a simple odd polynomial\n",
    "    # P(x) = 4x - 4x^3 (Simple approximations usually require optimization)\n",
    "    # Let's verify what polynomial a random phase sequence creates.\n",
    "    \n",
    "    # 2. QSVT Sequence Simulator\n",
    "    # Inputs: Singular value x, Phase angles phi\n",
    "    # Output: The transformed singular value P(x)\n",
    "    \n",
    "    def apply_qsvt(x, phis):\n",
    "        # We model the 2x2 subspace of the ancilla for a single singular value x\n",
    "        # W(x) = [[x, sqrt(1-x^2)], [sqrt(1-x^2), -x]]  (The Signal Rotation)\n",
    "        # R(phi) = [[e^iphi, 0], [0, e^-iphi]]          (The Signal Processing)\n",
    "        \n",
    "        # Initial state: Identity (or effectively just processing the singular value)\n",
    "        # We compute the top-left element of the product sequence\n",
    "        \n",
    "        W = np.array([[x, np.sqrt(1 - x**2)], [np.sqrt(1 - x**2), -x]])\n",
    "        \n",
    "        # Start with identity\n",
    "        U_total = np.eye(2, dtype=complex)\n",
    "        \n",
    "        # Apply sequence: R(phi_d) W(x) ... R(phi_0)\n",
    "        # Note: Conventions vary. Here we use standard R-W-R-W\n",
    "        \n",
    "        for phi in phis:\n",
    "            R = np.array([[np.exp(1j * phi), 0], [0, np.exp(-1j * phi)]])\n",
    "            U_total = R @ W @ U_total\n",
    "            \n",
    "        return U_total[0, 0]\n",
    "\n",
    "    # 3. Define Phase Angles\n",
    "    # These would come from a classical optimizer (Remez algorithm)\n",
    "    # Let's use a known sequence for a Chebyshev polynomial T_3(x) = 4x^3 - 3x\n",
    "    # Angles for T_3 (approx):\n",
    "    phis = [0.0, np.pi/2, 0.0] # Simple placeholder logic for demo structure\n",
    "    # A real inversion sequence has length ~ kappa * log(1/eps)\n",
    "    \n",
    "    # Let's just scan x from -1 to 1 and see what function emerges\n",
    "    x_vals = np.linspace(-1, 1, 100)\n",
    "    poly_vals = []\n",
    "    \n",
    "    # Use random angles to show \"a polynomial\"\n",
    "    random_phis = np.random.uniform(0, 2*np.pi, 5)\n",
    "    \n",
    "    for val in x_vals:\n",
    "        # Singular values are usually positive, but QSVT processes x in [-1, 1]\n",
    "        # We need to handle the sqrt domain carefully if x > 1 (not possible for singular vals)\n",
    "        res = apply_qsvt(val, random_phis)\n",
    "        poly_vals.append(np.real(res))\n",
    "        \n",
    "    # 4. Visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x_vals, poly_vals, label='QSVT Polynomial $P(x)$', linewidth=2)\n",
    "    plt.plot(x_vals, x_vals, 'k--', alpha=0.3, label='$y=x$ (Identity)')\n",
    "    plt.title(\"QSVT: Polynomial Transformation of Singular Values\")\n",
    "    plt.xlabel(\"Input Singular Value $\\sigma$\")\n",
    "    plt.ylabel(\"Output Singular Value $P(\\sigma)$\")\n",
    "    plt.ylim(-1.1, 1.1)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"In a real Matrix Inversion algorithm, we optimize 'phis' so that P(x) looks like 1/x.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    qsvt_polynomial_demo()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] A. Gilyén, Y. Su, G. H. Low, and N. Wiebe, \"Quantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics,\" *Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing*, 193 (2019).  \n",
    "[2] J. M. Martyn, Z. M. Rossi, A. K. Tan, and I. L. Chuang, \"Grand Unification of Quantum Algorithms,\" *PRX Quantum* **2**, 040203 (2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0482366",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 32** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 10: The Algorithmic Bridge (The Modern Standard Model)**\n",
    "\n",
    "#### **Chapter 32: Simulating Reality: Schrödingerization & Carleman Linearization**\n",
    "\n",
    "In the previous chapter, we learned how to perform linear algebra on unitary quantum computers. This solves the \"Matrix Problem.\" But the world is not just linear algebra.\n",
    "\n",
    "  * **Dissipation:** The Heat Equation $\\frac{dT}{dt} = \\nabla^2 T$ is not unitary. It loses information (heat spreads out and cannot be gathered back).\n",
    "  * **Non-Linearity:** Fluid dynamics (Navier-Stokes) involves terms like $(v \\cdot \\nabla) v$. The sum of two fluid flows is not a fluid flow.\n",
    "\n",
    "To simulate reality (engineering, weather, epidemiology) on a quantum computer, we must hack the Schrödinger equation. We must turn non-unitary and non-linear problems into unitary linear ones. This chapter introduces the two mathematical tricks that make this possible: **Schrödingerization** and **Carleman Linearization**.\n",
    "\n",
    "### **32.1 Schrödingerization**\n",
    "\n",
    "Quantum computers are naturally good at solving $\\frac{d}{dt} |\\psi\\rangle = -i H |\\psi\\rangle$. This equation is unitary (energy conserving, reversible).\n",
    "Real-world equations like the Heat Equation or Black-Scholes are **Non-Unitary** (dissipative, irreversible).\n",
    "$$\\frac{d\\vec{u}}{dt} = -A \\vec{u}$$\n",
    "where $A$ is a Hermitian matrix (like the Laplacian $-\\nabla^2$). The solution is decay: $\\vec{u}(t) = e^{-At} \\vec{u}(0)$.\n",
    "\n",
    "We cannot simply set $H = -iA$ (an imaginary Hamiltonian) because Hamiltonians must be Hermitian.\n",
    "Instead, we use **Schrödingerization**. We embed the 1D dissipative dynamics into a 2D unitary rotation.\n",
    "\n",
    "#### **32.1.1 The Warped Phase Transformation**\n",
    "\n",
    "We introduce an auxiliary \"ancilla\" register (a new dimension $p$).\n",
    "We define a new state vector $\\vec{w}(t, p)$ that lives in a larger space.\n",
    "We construct a Hamiltonian that couples the original system to this momentum coordinate $p$:\n",
    "$$H = A \\otimes P_y$$\n",
    "where $P_y = -i \\frac{\\partial}{\\partial y}$.\n",
    "\n",
    "The unitary evolution is:\n",
    "$$U(t) = e^{-i (A \\otimes P_y) t}$$\n",
    "\n",
    "If we initialize the ancilla carefully (or project it later), we can select a slice of the evolution that looks exactly like $e^{-At}$.\n",
    "Essentially, we map \"decay in time\" to \"propagation in the auxiliary dimension.\" The information isn't lost; it just leaks into the ancilla [1].\n",
    "\n",
    "### **32.2 Boundary Conditions**\n",
    "\n",
    "In classical engineering (Finite Element Analysis), the boundary conditions (BCs) define the problem.\n",
    "\n",
    "  * **Dirichlet:** $\\psi(0) = 0$ (Fixed temperature).\n",
    "  * **Neumann:** $\\psi'(0) = 0$ (Insulated wall).\n",
    "\n",
    "In a quantum register, we don't have \"edges\"; we have qubit states like $|00\\dots0\\rangle$ and $|11\\dots1\\rangle$.\n",
    "To implement BCs, we modify the **Kinetic Matrix** (the Laplacian) at the specific rows corresponding to the boundary.\n",
    "\n",
    "#### **32.2.1 The Ghost Point Method**\n",
    "\n",
    "We extend the register by one virtual point $x_{-1}$.\n",
    "\n",
    "  * **Dirichlet:** We enforce $\\psi_{-1} = -\\psi_0$. The Laplacian at the boundary becomes $\\psi_1 - 2\\psi_0 + (-\\psi_0) = \\psi_1 - 3\\psi_0$.\n",
    "  * **Neumann:** We enforce $\\psi_{-1} = \\psi_0$. The Laplacian becomes $\\psi_1 - 2\\psi_0 + \\psi_0 = \\psi_1 - \\psi_0$.\n",
    "\n",
    "We implement this by adding a \"Boundary Hamiltonian\" $H_{bound}$ that applies a penalty or a specific hopping term only when the position register is in state $|0\\rangle$ or $|N-1\\rangle$.\n",
    "\n",
    "### **32.3 Taming Non-Linearity (Carleman Linearization)**\n",
    "\n",
    "The hardest problem in physics is **Non-Linearity**.\n",
    "$$\\frac{du}{dt} = -u + u^2$$\n",
    "There is no matrix $A$ such that $Au = u^2$. Linear algebra fails.\n",
    "\n",
    "However, we can trade **Complexity for Dimensions**.\n",
    "We can define a new variable $y = u^2$.\n",
    "Now we need an equation for $y$.\n",
    "$$\\frac{dy}{dt} = \\frac{d(u^2)}{dt} = 2u \\frac{du}{dt} = 2u(-u + u^2) = -2u^2 + 2u^3 = -2y + 2u^3$$\n",
    "Now we need a variable $z = u^3$. And so on.\n",
    "\n",
    "#### **32.3.1 The Infinite Linear Tower**\n",
    "\n",
    "We construct an infinite vector of **Tensor Powers**:\n",
    "$$\\vec{\\Psi} = \\begin{pmatrix} u \\\\ u^{\\otimes 2} \\\\ u^{\\otimes 3} \\\\ \\vdots \\end{pmatrix}$$\n",
    "\n",
    "The non-linear differential equation for $u$ becomes a **Linear** differential equation for the infinite vector $\\vec{\\Psi}$.\n",
    "$$\\frac{d\\vec{\\Psi}}{dt} = \\mathbf{A}_{Carleman} \\vec{\\Psi}$$\n",
    "\n",
    "The matrix $\\mathbf{A}_{Carleman}$ couples level $k$ to level $k+1$.\n",
    "For quantum simulation, we **truncate** this tower at some order $k=K$.\n",
    "If the solution is smooth (decaying), the error is small. We then use QSVT or Hamiltonian Simulation to solve this giant linear system.\n",
    "This approach enables the first quantum algorithms for **Computational Fluid Dynamics (CFD)** [2].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will implement **Carleman Linearization** for the simple non-linear logistic decay equation $du/dt = -u + u^2$. We will compare the exact non-linear solution to the solution obtained by solving the truncated linear system (Order $K=2, 3, 4$).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import expm\n",
    "\n",
    "def carleman_demo():\n",
    "    # 1. Problem: du/dt = -u + u^2 (Logistic decay)\n",
    "    # Exact solution: u(t) = u0 / (u0 + (1-u0)e^t)\n",
    "    u0 = 0.5\n",
    "    t_max = 5.0\n",
    "    times = np.linspace(0, t_max, 50)\n",
    "    \n",
    "    # Exact Solution\n",
    "    u_exact = u0 / (u0 + (1 - u0) * np.exp(times))\n",
    "    \n",
    "    # 2. Carleman Matrix Construction (Truncation Order K)\n",
    "    def build_carleman_matrix(K):\n",
    "        # State vector Y = [u, u^2, ..., u^K] (Size K)\n",
    "        # d(u^k)/dt = k * u^(k-1) * du/dt = k * u^(k-1) * (-u + u^2)\n",
    "        #           = -k * u^k + k * u^(k+1)\n",
    "        \n",
    "        # Matrix A has size K x K\n",
    "        A = np.zeros((K, K))\n",
    "        \n",
    "        for k in range(1, K + 1): # Index k corresponds to power u^k\n",
    "            idx = k - 1 # Matrix index (0 to K-1)\n",
    "            \n",
    "            # Term -k * u^k (Diagonal)\n",
    "            A[idx, idx] = -k\n",
    "            \n",
    "            # Term +k * u^(k+1) (Off-diagonal, coupling to higher power)\n",
    "            if idx + 1 < K:\n",
    "                A[idx, idx + 1] = k\n",
    "                \n",
    "        return A\n",
    "\n",
    "    # 3. Solve Linear Systems\n",
    "    orders = [2, 3, 5]\n",
    "    solutions = {}\n",
    "    \n",
    "    for K in orders:\n",
    "        A = build_carleman_matrix(K)\n",
    "        \n",
    "        # Initial state vector Y0 = [u0, u0^2, ..., u0^K]\n",
    "        Y0 = np.array([u0**k for k in range(1, K + 1)])\n",
    "        \n",
    "        # Evolve: Y(t) = exp(At) Y0\n",
    "        u_approx = []\n",
    "        for t in times:\n",
    "            U = expm(A * t)\n",
    "            Y_t = U @ Y0\n",
    "            u_approx.append(Y_t[0]) # We only care about u (first component)\n",
    "            \n",
    "        solutions[K] = u_approx\n",
    "\n",
    "    # 4. Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(times, u_exact, 'k-', linewidth=3, label='Exact Non-Linear')\n",
    "    \n",
    "    colors = ['r--', 'g--', 'b--']\n",
    "    for i, K in enumerate(orders):\n",
    "        plt.plot(times, solutions[K], colors[i], label=f'Carleman (K={K})')\n",
    "        \n",
    "    plt.title(\"Simulating Non-Linearity with Linear Algebra\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"u(t)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Notice: Higher K (more tensor powers) approximates the non-linearity longer.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    carleman_demo()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] S. Jin, N. Liu, and Y. Yu, \"Quantum simulation of partial differential equations: Applications and detailed analysis,\" *Physical Review A* **108**, 032603 (2023).  \n",
    "[2] J.-P. Liu, H. Ø. Kolden, H. K. Krovi, N. F. Loureiro, K. Trivisa, and A. M. Childs, \"Efficient quantum algorithm for dissipative nonlinear differential equations,\" *Proceedings of the National Academy of Sciences* **118**, e2026805118 (2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c27c912",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 33** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 11: Real-World Modeling**\n",
    "\n",
    "#### **Chapter 33: Quantum Finance (Econophysics)**\n",
    "\n",
    "In Section 10, we used quantum algorithms to solve abstract equations. Now we apply these tools to the chaotic, stochastic world of Global Finance.\n",
    "\n",
    "Traditional finance models markets using stochastic calculus (Brownian motion). In **Quantum Finance**, we model the market as a physical system—a **Financial Lattice**—where capital flows like a quantum fluid, seeking the lowest energy state (highest return). This chapter explores how to price derivatives and model arbitrage using the machinery of quantum mechanics.\n",
    "\n",
    "### **33.1 The Arbitrage Hamiltonian**\n",
    "\n",
    "We define the \"Universe\" as a graph where each node represents a tradable asset (Stock, Bond, Currency).\n",
    "The state vector is $|\\psi(t)\\rangle = \\sum_i \\alpha_i(t) |i\\rangle$.\n",
    "The amplitude $\\alpha_i$ represents the allocation of capital in asset $i$. The probability $P_i = |\\alpha_i|^2$ is the fraction of the portfolio invested in $i$.\n",
    "\n",
    "#### **33.1.1 Constructing the Hamiltonian**\n",
    "\n",
    "We postulate that the dynamics of capital flow are governed by a Hamiltonian $H$, representing the \"Energy\" of the portfolio. In physics, systems evolve to minimize energy. In finance, markets evolve to maximize utility. Therefore, we define **Financial Energy $\\sim$ -Return**.\n",
    "\n",
    "The Financial Hamiltonian has two main components:\n",
    "$$H_{fin} = \\mathbf{T} + \\mathbf{V}$$\n",
    "\n",
    "1.  **The Hopping Matrix ($\\mathbf{T}$): Liquidity & Transaction**\n",
    "    This term drives the movement of capital between assets.\n",
    "    $$H_{hop} = -\\sum_{\\langle i, j \\rangle} t_{ij} (c^\\dagger_i c_j + c^\\dagger_j c_i)$$\n",
    "\n",
    "      * **$t_{ij}$ (Hopping Amplitude):** Represents the **Liquidity** or ease of exchanging asset $i$ for $j$.\n",
    "      * **High $t$:** Liquid market. Capital flows instantly to correct imbalances.\n",
    "      * **Low $t$:** Illiquid market. High transaction costs prevent flow.\n",
    "\n",
    "2.  **The Potential Matrix ($\\mathbf{V}$): Fundamental Value**\n",
    "    This diagonal term represents the \"attractiveness\" of holding an asset.\n",
    "    $$H_{pot} = \\sum_i V_i(t) c^\\dagger_i c_i$$\n",
    "\n",
    "      * **$V_i$:** Can be modeled as $-(R_i - r_f)$, where $R_i$ is the expected return.\n",
    "      * **Potential Well:** A stock with high expected returns ($R_i \\gg 0$) creates a deep potential well ($V_i \\ll 0$). The capital wavefunction $|\\psi\\rangle$ naturally flows into and localizes in this well.\n",
    "\n",
    "### **33.2 Schrödingerizing Finance**\n",
    "\n",
    "The backbone of modern derivatives pricing is the **Black-Scholes-Merton (BSM) Equation**.\n",
    "$$\\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} + rS \\frac{\\partial V}{\\partial S} - rV = 0$$\n",
    "\n",
    "Physicists recognized long ago that this is mathematically identical to the **Schrödinger Equation** in imaginary time ($t \\to -i\\tau$), where the stock price $S$ plays the role of position $x$. However, BSM is non-unitary (it describes the dissipation of risk).\n",
    "\n",
    "To solve it on a quantum computer, we use the **Schrödingerization** technique (Chapter 32).\n",
    "We map the financial variables (Stock Price $S$, Volatility $\\sigma$) to a quantum register.\n",
    "We construct a unitary Hamiltonian $H_{BS}$ acting on a larger Hilbert space (System + Ancilla) such that a projection of the evolution yields the solution to the BSM equation.\n",
    "\n",
    "$$U(t) = e^{-i H_{BS} t}$$\n",
    "\n",
    "This allows us to simulate the full distribution of option prices, not just the mean, capturing the \"Quantum Jitter\" (Tail Risk) that classical Gaussian models often miss.\n",
    "\n",
    "### **33.3 Amplitude Estimation**\n",
    "\n",
    "The most practical application of quantum computing in finance today is **Monte Carlo Acceleration**.\n",
    "Banks run billions of Monte Carlo simulations to price complex derivatives (e.g., path-dependent Asian options) or calculate Value at Risk (VaR).\n",
    "The error of classical Monte Carlo scales as $\\epsilon \\sim 1/\\sqrt{N}$. To get $10\\times$ more precision, you need $100\\times$ more samples.\n",
    "\n",
    "#### **33.3.1 The Quantum Alternative**\n",
    "\n",
    "We encode the probability distribution of asset prices into the amplitudes of a quantum state:\n",
    "$$|\\psi\\rangle = \\sum_x \\sqrt{p(x)} |x\\rangle$$\n",
    "We define an operator $Q$ (the Grover diffusion operator) that rotates this state towards the \"target\" outcomes (e.g., scenarios where the bank loses money).\n",
    "\n",
    "By using **Quantum Amplitude Estimation (QAE)**, we can estimate the expectation value (the price) with an error scaling as:\n",
    "$$\\epsilon \\sim 1/N$$\n",
    "\n",
    "This is a **Quadratic Speedup**.\n",
    "\n",
    "  * Classical: $1,000,000$ samples for precision $10^{-3}$.\n",
    "  * Quantum: $1,000$ samples for precision $10^{-3}$.\n",
    "    This allows for real-time risk management in high-frequency trading environments.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will implement a simple Quantum Amplitude Estimation algorithm (using classical emulation) to price a European Call Option. We will compare the convergence rate to standard Monte Carlo.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def quantum_option_pricing():\n",
    "    # 1. Option Parameters\n",
    "    S0 = 100.0      # Initial Price\n",
    "    K = 105.0       # Strike Price\n",
    "    sigma = 0.2     # Volatility\n",
    "    r = 0.05        # Risk-free rate\n",
    "    T = 1.0         # Time to maturity\n",
    "    \n",
    "    # Exact Black-Scholes Price (for reference)\n",
    "    # (Omitted calculation for brevity, assume ~8.02)\n",
    "    bs_price = 8.021 \n",
    "    \n",
    "    # 2. Classical Monte Carlo\n",
    "    # Payoff = max(S_T - K, 0)\n",
    "    # S_T = S0 * exp((r - 0.5*sigma^2)T + sigma*sqrt(T)*Z)\n",
    "    \n",
    "    def classical_mc(n_samples):\n",
    "        Z = np.random.normal(0, 1, n_samples)\n",
    "        ST = S0 * np.exp((r - 0.5 * sigma**2) * T + sigma * np.sqrt(T) * Z)\n",
    "        payoffs = np.maximum(ST - K, 0)\n",
    "        price = np.exp(-r * T) * np.mean(payoffs)\n",
    "        return price\n",
    "\n",
    "    # 3. Quantum Amplitude Estimation (Emulated)\n",
    "    # QAE converges as O(1/M) where M is the number of Grover iterations (quantum samples).\n",
    "    # Classical MC converges as O(1/sqrt(N)).\n",
    "    # We simulate the *error scaling* rather than the full circuit (which requires a QPU).\n",
    "    \n",
    "    sample_counts = [10, 100, 1000, 10000, 100000]\n",
    "    errors_classical = []\n",
    "    errors_quantum = []\n",
    "    \n",
    "    print(f\"True BS Price: {bs_price:.4f}\")\n",
    "    \n",
    "    for n in sample_counts:\n",
    "        # Run Classical\n",
    "        # Average over a few runs to smooth the curve\n",
    "        c_prices = [classical_mc(n) for _ in range(20)]\n",
    "        err_c = np.mean(np.abs(np.array(c_prices) - bs_price))\n",
    "        errors_classical.append(err_c)\n",
    "        \n",
    "        # Emulate Quantum Error Scaling\n",
    "        # Error ~ C / N (Heisenberg Limit)\n",
    "        # We assume C is similar to the variance scale\n",
    "        # Ideally, Quantum counts queries, not samples. \n",
    "        # N quantum samples ~ N queries.\n",
    "        err_q = 10.0 / n # Theoretical scaling slope\n",
    "        errors_quantum.append(err_q)\n",
    "\n",
    "    # 4. Visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.loglog(sample_counts, errors_classical, 'bo-', label='Classical Monte Carlo ($1/\\sqrt{N}$)')\n",
    "    plt.loglog(sample_counts, errors_quantum, 'r--', label='Quantum Amplitude Est ($1/N$)')\n",
    "    \n",
    "    # Plot slopes for reference\n",
    "    plt.loglog(sample_counts, [10/np.sqrt(x) for x in sample_counts], 'k:', alpha=0.5, label='Slope -0.5')\n",
    "    plt.loglog(sample_counts, [100/x for x in sample_counts], 'k-.', alpha=0.5, label='Slope -1.0')\n",
    "    \n",
    "    plt.xlabel(\"Number of Samples / Queries\")\n",
    "    plt.ylabel(\"Pricing Error\")\n",
    "    plt.title(\"Convergence Speed: Quantum vs Classical\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"-\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    quantum_option_pricing()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] B. E. Baaquie, *Quantum Finance: Path Integrals and Hamiltonians for Options and Interest Rates* (Cambridge University Press, 2004).  \n",
    "[2] S. Woerner and D. J. Egger, \"Quantum risk analysis,\" *NPJ Quantum Information* **5**, 15 (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80f28f",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 34** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 11: Real-World Modeling**\n",
    "\n",
    "#### **Chapter 34: Quantum Scientific Machine Learning (Q-SciML)**\n",
    "\n",
    "In Chapter 25, we used Machine Learning to solve quantum problems (Neural Quantum States). Now we reverse the arrow. We ask: **Can quantum mechanics solve Machine Learning problems?**\n",
    "\n",
    "Scientific Machine Learning (SciML) focuses on learning physical laws from data, rather than just classifying images of cats. It deals with differential equations, symmetries, and chaos.\n",
    "It turns out that the mathematical structure of Quantum Mechanics—linear algebra in high-dimensional Hilbert spaces—is the native language of many SciML problems. This chapter explores how we can repurpose the Schrödinger equation as a learning algorithm.\n",
    "\n",
    "### **34.1 The Koopman Operator: Linearizing Chaos**\n",
    "\n",
    "Classical chaotic systems (like weather or fluid turbulence) are governed by non-linear differential equations:\n",
    "$$\\frac{d\\vec{x}}{dt} = \\vec{f}(\\vec{x})$$\n",
    "Non-linearity is hard. It breaks superposition. Predicting the future requires expensive step-by-step integration.\n",
    "\n",
    "In 1931, B.O. Koopman made a startling discovery: **Non-linear dynamics in state space is equivalent to Linear dynamics in observable space.**\n",
    "\n",
    "#### **34.1.1 Lifting to Hilbert Space**\n",
    "\n",
    "Instead of tracking the trajectory $\\vec{x}(t)$, let us track the evolution of measurement functions (observables) $g(\\vec{x})$.\n",
    "The evolution of an observable is governed by the **Koopman Operator** $\\mathcal{K}$:\n",
    "$$\\mathcal{K}^t g(\\vec{x}_0) = g(\\vec{x}_t)$$\n",
    "\n",
    "Remarkably, $\\mathcal{K}$ is a **Linear Operator**, even if the underlying dynamics $\\vec{f}(\\vec{x})$ are non-linear.\n",
    "The trade-off? The state space $\\vec{x}$ is finite-dimensional ($d$), but the space of observable functions is **infinite-dimensional**.\n",
    "\n",
    "#### **34.1.2 Quantum Simulation of Classical Physics**\n",
    "\n",
    "This looks exactly like Quantum Mechanics.\n",
    "\n",
    "  * The \"Observable Function\" $g(\\vec{x})$ is analogous to a Wavefunction $|\\psi\\rangle$.\n",
    "  * The Koopman Operator $\\mathcal{K}$ is analogous to the Propagator $U = e^{-iHt}$.\n",
    "\n",
    "We can use a quantum computer to simulate chaotic classical systems by encoding the observables into amplitudes. We decompose the dynamics into Koopman Modes (eigenfunctions of $\\mathcal{K}$).\n",
    "$$\\vec{x}(t) = \\sum_j c_j \\lambda_j^t \\vec{v}_j$$\n",
    "By using Quantum Phase Estimation, we can extract these eigenvalues $\\lambda_j$ and predict the long-term behavior of chaos without time-stepping [1].\n",
    "\n",
    "### **34.2 Geometric Deep Learning**\n",
    "\n",
    "In standard Deep Learning, we often ignore the laws of physics. We train a network to predict the energy of a molecule, but the network doesn't know that \"rotating the molecule shouldn't change the energy.\" It has to learn this invariance from scratch, wasting vast amounts of data.\n",
    "\n",
    "**Geometric Deep Learning** builds these symmetries directly into the architecture.\n",
    "\n",
    "#### **34.2.1 Group Equivariance**\n",
    "\n",
    "A function $f$ is **Equivariant** under a symmetry group $G$ (like rotation) if:\n",
    "$$f(R_g \\cdot x) = R_g \\cdot f(x)$$\n",
    "(Rotating the input rotates the output).\n",
    "\n",
    "A function is **Invariant** if:\n",
    "$$f(R_g \\cdot x) = f(x)$$\n",
    "(Rotating the input changes nothing).\n",
    "\n",
    "#### **34.2.2 Quantum Equivariant Neural Networks (QCNN)**\n",
    "\n",
    "On a quantum computer, symmetries are Unitary operators $U_g$.\n",
    "We construct a Quantum Neural Network (Parameterized Circuit) that commutes with the symmetry group.\n",
    "$$U(\\theta) U_g = U_g U(\\theta)$$\n",
    "\n",
    "For example, in a lattice model with translational symmetry, we use a **Quantum Convolutional Neural Network (QCNN)**. We apply the same gate parameters $\\theta$ to every pair of qubits.\n",
    "This has two massive advantages:\n",
    "\n",
    "1.  **Data Efficiency:** We don't need to show rotated versions of the dataset.\n",
    "2.  **No Barren Plateaus:** Constraining the circuit to a symmetry subspace prevents the gradients from vanishing in high-dimensional space [2].\n",
    "\n",
    "### **34.3 Quantum Kernels**\n",
    "\n",
    "The most powerful classical ML technique before Deep Learning was the **Support Vector Machine (SVM)**. It works by mapping data $\\vec{x}$ into a high-dimensional feature space where complex patterns become linearly separable.\n",
    "$$K(\\vec{x}, \\vec{y}) = \\langle \\phi(\\vec{x}) | \\phi(\\vec{y}) \\rangle$$\n",
    "\n",
    "#### **34.3.1 Hilbert Space as Feature Space**\n",
    "\n",
    "Quantum computers naturally map classical data into an exponentially large feature space: the Hilbert Space.\n",
    "We encode a data point $\\vec{x}$ into a quantum state:\n",
    "$$|\\phi(\\vec{x})\\rangle = U(\\vec{x}) |0\\rangle$$\n",
    "\n",
    "The inner product of these states is a **Quantum Kernel**:\n",
    "$$K_Q(\\vec{x}, \\vec{y}) = |\\langle \\phi(\\vec{x}) | \\phi(\\vec{y}) \\rangle|^2$$\n",
    "\n",
    "We can compute this efficiently on a quantum device (using the SWAP test or Inversion test), but computing it classically would require calculating $2^N$ amplitudes.\n",
    "\n",
    "#### **34.3.2 The Quantum Advantage**\n",
    "\n",
    "If the map $U(\\vec{x})$ is hard to simulate classically (e.g., it involves discrete log or deep entanglement), the Quantum Kernel can detect patterns that are invisible to classical kernels (like Gaussian RBF). This is promising for classifying data generated by quantum processes, such as phase transitions or chemical reaction outcomes [3].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will implement a **Quantum Kernel** classifier simulation. We will define a \"Quantum Feature Map\" (encoding data into rotation angles) and compute the Kernel Matrix. We then use a classical SVM to classify the data based on this quantum distance metric.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "def quantum_kernel_demo():\n",
    "    # 1. Generate Non-Linear Data\n",
    "    # Circles dataset: Class 0 is inner circle, Class 1 is outer circle.\n",
    "    # Not linearly separable in 2D Euclidean space.\n",
    "    X, y = make_circles(n_samples=100, factor=0.3, noise=0.05)\n",
    "    \n",
    "    # 2. Define Quantum Feature Map (Simulator)\n",
    "    # ZZFeatureMap logic: Encodes x[0], x[1] into rotation angles\n",
    "    # U(x) = exp(i * x[0] * Z0) * exp(i * x[1] * Z1) * exp(i * (pi-x[0])(pi-x[1]) * Z0Z1)\n",
    "    # This creates entanglement related to the data product.\n",
    "    \n",
    "    def get_state_vector(data_point):\n",
    "        # 2 Qubits\n",
    "        psi = np.array([1.0, 0.0, 0.0, 0.0], dtype=complex) # |00>\n",
    "        \n",
    "        # H gates (Superposition)\n",
    "        H = np.array([[1, 1], [1, -1]]) / np.sqrt(2)\n",
    "        H2 = np.kron(H, H)\n",
    "        psi = H2 @ psi\n",
    "        \n",
    "        # Phase encoding (Diagonal gates)\n",
    "        x0, x1 = data_point\n",
    "        phi0 = x0\n",
    "        phi1 = x1\n",
    "        phi01 = (np.pi - x0) * (np.pi - x1)\n",
    "        \n",
    "        # Diagonal phases for |00>, |01>, |10>, |11>\n",
    "        # Z0 contributes +1 for 0, -1 for 1\n",
    "        # Z1 contributes +1 for 0, -1 for 1\n",
    "        # Z0Z1 is product\n",
    "        \n",
    "        # Phase for state |ab>: exp(i * [ phi0*(-1)^a + phi1*(-1)^b + phi01*(-1)^(a+b) ])\n",
    "        # Let's verify standard ZZ map signs:\n",
    "        # Actually usually implemented as RZ gates.\n",
    "        # U = exp(-i phi Z). Diag terms: exp(-i phi), exp(i phi).\n",
    "        # Let's assume a simplified map:\n",
    "        # |00> -> 1\n",
    "        # |01> -> e^{i x0}\n",
    "        # |10> -> e^{i x1}\n",
    "        # |11> -> e^{i (x0+x1)^2}  (Non-linear term)\n",
    "        \n",
    "        phases = np.array([\n",
    "            1.0,\n",
    "            np.exp(1j * x0 * 2.0),\n",
    "            np.exp(1j * x1 * 2.0),\n",
    "            np.exp(1j * (x0 + x1)**2)\n",
    "        ])\n",
    "        \n",
    "        return psi * phases\n",
    "\n",
    "    # 3. Compute Kernel Matrix\n",
    "    # K_ij = |<psi(x_i) | psi(x_j)>|^2\n",
    "    \n",
    "    print(\"Computing Quantum Kernel Matrix...\")\n",
    "    n_samples = len(X)\n",
    "    K_matrix = np.zeros((n_samples, n_samples))\n",
    "    \n",
    "    # Pre-compute states\n",
    "    states = [get_state_vector(x) for x in X]\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        for j in range(i, n_samples): # Symmetric\n",
    "            overlap = np.vdot(states[i], states[j])\n",
    "            k_val = np.abs(overlap)**2\n",
    "            K_matrix[i, j] = k_val\n",
    "            K_matrix[j, i] = k_val\n",
    "            \n",
    "    # 4. Train Classical SVM with Quantum Kernel\n",
    "    # We pass 'precomputed' to tell SVM we are giving it the Kernel matrix K\n",
    "    clf = SVC(kernel='precomputed')\n",
    "    clf.fit(K_matrix, y)\n",
    "    \n",
    "    print(\"SVM Trained.\")\n",
    "    print(f\"Accuracy on training set: {clf.score(K_matrix, y) * 100:.1f}%\")\n",
    "    \n",
    "    # 5. Visualization\n",
    "    # Create a mesh to plot decision boundary\n",
    "    h = 0.1\n",
    "    x_min, x_max = X[:, 0].min() - 0.2, X[:, 0].max() + 0.2\n",
    "    y_min, y_max = X[:, 1].min() - 0.2, X[:, 1].max() + 0.2\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Compute Kernel between Mesh points and Training points\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    mesh_states = [get_state_vector(p) for p in mesh_points]\n",
    "    \n",
    "    K_mesh = np.zeros((len(mesh_points), n_samples))\n",
    "    for i in range(len(mesh_points)):\n",
    "        for j in range(n_samples):\n",
    "            overlap = np.vdot(mesh_states[i], states[j])\n",
    "            K_mesh[i, j] = np.abs(overlap)**2\n",
    "            \n",
    "    Z = clf.predict(K_mesh)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "    plt.title(\"Quantum Kernel Classification (Simulated)\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    quantum_kernel_demo()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] M. O. Williams, I. G. Kevrekidis, and C. W. Rowley, \"A Data-Driven Approximation of the Koopman Operator: Extending Dynamic Mode Decomposition,\" *Journal of Nonlinear Science* **25**, 1307 (2015).  \n",
    "[2] M. M. Bronstein, J. Bruna, T. Cohen, and P. Velickovic, \"Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges,\" *arXiv:2104.13478* (2021).  \n",
    "[3] V. Havlíček et al., \"Supervised learning with quantum-enhanced feature spaces,\" *Nature* **567**, 209 (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346106f",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 35** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 11: Real-World Modeling**\n",
    "\n",
    "#### **Chapter 35: Quantum Biology & Neuroscience**\n",
    "\n",
    "For decades, physicists assumed that quantum coherence could not survive in biological systems. Life is \"warm and wet.\" Thermal noise ($k_B T$) at room temperature should destroy any delicate superposition instantly.\n",
    "\n",
    "However, in 2007, experiments revealed that the **Fenna-Matthews-Olson (FMO) Complex**—a protein found in green sulfur bacteria—transports energy with nearly 100% efficiency using long-lived quantum coherence. This chapter explores how nature might be acting as a quantum engineer, and how the brain itself might be modeled using the mathematics of tensor networks.\n",
    "\n",
    "### **35.1 The Photosynthetic Graph**\n",
    "\n",
    "#### **35.1.1 The FMO Complex**\n",
    "\n",
    "The FMO complex acts as a wire connecting the \"antenna\" (which catches sunlight) to the \"reaction center\" (which stores energy). Structurally, it consists of 7 bacteriochlorophyll molecules (chromophores) embedded in a protein scaffold.\n",
    "\n",
    "To model this, we treat the protein as a **Quantum Graph** with $N=7$ nodes.\n",
    "\n",
    "  * **Nodes:** Each chromophore is a site $|n\\rangle$. The state represents the location of the \"Exciton\" (an electron-hole pair created by the photon).\n",
    "  * **Edges:** The chromophores are close enough that the exciton can tunnel between them via dipole-dipole interaction.\n",
    "\n",
    "#### **35.1.2 The Hamiltonian**\n",
    "\n",
    "The Hamiltonian describing the energy transfer is a standard tight-binding matrix, but with specific biological parameters:\n",
    "\n",
    "$$H_{FMO} = \\sum_{n=1}^7 E_n |n\\rangle\\langle n| + \\sum_{n \\neq m} V_{nm} |n\\rangle\\langle m|$$\n",
    "\n",
    "  * **Site Energies ($E_n$):** The protein scaffold twists each chlorophyll molecule slightly differently. This \"tuning\" creates a potential energy gradient, naturally guiding the exciton downhill toward the reaction center (Site 3 or 4).\n",
    "  * **Couplings ($V_{nm}$):** Determined by the distance and orientation of the molecules.\n",
    "\n",
    "If we diagonalize this $7 \\times 7$ matrix, we find that the eigenstates are **delocalized** across multiple sites. The exciton does not hop from A to B; it flows through the complex as a superposition wave [1].\n",
    "\n",
    "### **35.2 Environment-Assisted Transport (ENAQT)**\n",
    "\n",
    "Here lies the paradox. If the FMO complex were a perfect quantum crystal (Unitary evolution), the transport would actually be **inefficient**.\n",
    "Why?\n",
    "\n",
    "1.  **Anderson Localization:** The site energies $E_n$ are disordered. In a perfect 1D quantum system, disorder causes the wavefunction to get stuck (localize), unable to reach the exit.\n",
    "2.  **Destructive Interference:** Waves reflecting off the boundaries would create standing waves, trapping the energy inside the protein.\n",
    "\n",
    "Nature solves this by **adding noise**. This phenomenon is called **Environment-Assisted Quantum Transport (ENAQT)**.\n",
    "\n",
    "#### **35.2.1 The Goldilocks Zone**\n",
    "\n",
    "We model the interaction with the vibrating protein background using the **Lindblad Master Equation** (from Chapter 23).\n",
    "$$\\frac{d\\rho}{dt} = -i[H, \\rho] + \\mathcal{L}_{noise}[\\rho]$$\n",
    "\n",
    "The noise (dephasing) acts as a \"shaking\" mechanism.\n",
    "\n",
    "  * **Too Little Noise (Pure Quantum):** The exciton gets stuck in a localized eigenstate due to interference/disorder. Efficiency is low.\n",
    "  * **Too Much Noise (Pure Classical):** The Quantum Zeno effect freezes the motion. The hopping rate drops. Efficiency is low.\n",
    "  * **Just Right (The Biological Regime):** The noise breaks the destructive interference patterns and \"detunes\" the localization, allowing the exciton to slide continuously toward the sink.\n",
    "\n",
    "It appears that billions of years of evolution have tuned the coupling strengths $V_{nm}$ and the noise levels exactly to the peak of this efficiency curve [2].\n",
    "\n",
    "### **35.3 The Quantum Connectome**\n",
    "\n",
    "Moving from proteins to the brain, we encounter the most complex network known. While it is controversial whether the brain uses actual quantum coherence (Penrose-Hameroff), the **mathematics** of quantum mechanics—specifically Tensor Networks—has proven surprisingly effective at modeling neural information processing.\n",
    "\n",
    "#### **35.3.1 Neural Networks as Tensor Networks**\n",
    "\n",
    "In Chapter 24, we saw that a Matrix Product State (MPS) represents a 1D chain of entangled particles.\n",
    "A Deep Neural Network can be viewed mathematically as a specific type of Tensor Network.\n",
    "\n",
    "  * **Neurons $\\to$ Indices:** The firing state of neuron $i$ corresponds to a physical index.\n",
    "  * **Weights $\\to$ Tensors:** The weight matrices connecting layers are the internal tensors of the network.\n",
    "\n",
    "#### **35.3.2 The Entanglement of Thought**\n",
    "\n",
    "We can apply the concept of **Entanglement Entropy** to neural networks.\n",
    "If we partition the brain (or an artificial neural network) into Region A and Region B, the \"entanglement\" (mutual information) measures how much the activity in A predicts the activity in B.\n",
    "\n",
    "  * **Volume Law:** Random, chaotic activity (Seizure).\n",
    "  * **Area Law:** Structured, meaningful activity (Cognition).\n",
    "\n",
    "Recent work suggests that successfully trained neural networks naturally evolve towards an \"Area Law\" state, just like ground states of physical Hamiltonians. This implies that the brain might be optimizing itself to be efficiently compressible, living on the \"edge of chaos\" defined by tensor network geometry [3].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will simulate the ENAQT effect on a simple 1D chain. We will model the exciton transfer probability as a function of the dephasing rate to demonstrate that noise *increases* efficiency.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "def enaqt_simulation():\n",
    "    # 1. System Parameters\n",
    "    N = 5           # Number of sites\n",
    "    target = N - 1  # Sink is at the last site\n",
    "    t_hop = 1.0     # Hopping strength\n",
    "    \n",
    "    # 2. Hamiltonian (Tight Binding + Disorder)\n",
    "    # H = sum E_n |n><n| - t sum (|n><n+1| + h.c.)\n",
    "    # We add disorder to site energies to cause localization\n",
    "    disorder_strength = 2.0\n",
    "    energies = (np.random.rand(N) - 0.5) * disorder_strength\n",
    "    \n",
    "    H = np.diag(energies)\n",
    "    off_diag = -t_hop * np.ones(N-1)\n",
    "    H += np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n",
    "    \n",
    "    # 3. Lindblad Dissipators\n",
    "    # We have two types of noise:\n",
    "    # A. Dephasing (L_n = sqrt(gamma) * |n><n|) -> Scrambles phase, preserves energy\n",
    "    # B. Sink (L_sink = sqrt(rate) * |vac><target|) -> Removes particle (Success!)\n",
    "    # To model the sink in a density matrix of size N, we usually add an anti-Hermitian term \n",
    "    # to H_eff or track the \"population lost\". \n",
    "    # Here, let's track population at the target site as \"success\".\n",
    "    \n",
    "    # We construct the Liouvillian Superoperator logic directly for the ODE solver\n",
    "    # State is rho (NxN), flattened to N^2 vector\n",
    "    \n",
    "    def dynamics(t, rho_vec, gamma_dephasing):\n",
    "        rho = rho_vec.reshape((N, N))\n",
    "        \n",
    "        # Unitary: -i[H, rho]\n",
    "        comm = H @ rho - rho @ H\n",
    "        d_rho = -1j * comm\n",
    "        \n",
    "        # Dephasing: sum_n gamma * (2 n rho n - {n, rho})\n",
    "        # For projectors P_n = |n><n|, P_n rho P_n removes off-diagonals.\n",
    "        # The sum of L_n rho L_n term effectively kills off-diagonals.\n",
    "        # Simple Dephasing term: D_ij = -gamma if i!=j\n",
    "        \n",
    "        # Element-wise decay of off-diagonals\n",
    "        dephasing_matrix = -gamma_dephasing * np.ones((N, N))\n",
    "        np.fill_diagonal(dephasing_matrix, 0.0) # Diagonals don't decay\n",
    "        d_rho += dephasing_matrix * rho\n",
    "        \n",
    "        # Sink: We remove population from the target site \"irreversibly\"\n",
    "        # d_rho_NN = -2 * kappa * rho_NN\n",
    "        # In a real FMO, this goes to a reaction center.\n",
    "        sink_rate = 1.0\n",
    "        # We add an imaginary potential to the target site in H is simpler\n",
    "        # H_eff = H - i * kappa * |target><target|\n",
    "        # This causes norm decay. The decay amount is the yield.\n",
    "        # Let's adjust the unitary part to be non-Hermitian for the sink.\n",
    "        \n",
    "        return d_rho.flatten()\n",
    "\n",
    "    # We use a simplified Non-Hermitian Hamiltonian approach for the Sink\n",
    "    # H_eff = H - i * k * |Target><Target|\n",
    "    # Master Eq: d_rho/dt = -i(H_eff rho - rho H_eff^dag) + Dephasing\n",
    "    \n",
    "    def run_transport(gamma):\n",
    "        H_eff = H - 1j * 1.0 * np.zeros((N, N))\n",
    "        H_eff[target, target] -= 1j * 1.0 # Sink rate 1.0\n",
    "        \n",
    "        def derivative(t, y):\n",
    "            rho = y.reshape((N, N))\n",
    "            # Non-Hermitian evolution\n",
    "            term1 = -1j * (H_eff @ rho - rho @ H_eff.conj().T)\n",
    "            \n",
    "            # Dephasing (Purely destroys coherence)\n",
    "            # L[rho] = -gamma * (rho - diag(rho))\n",
    "            # Only off-diagonals decay\n",
    "            off_diag = rho.copy()\n",
    "            np.fill_diagonal(off_diag, 0.0)\n",
    "            term2 = -gamma * off_diag\n",
    "            \n",
    "            return (term1 + term2).flatten()\n",
    "            \n",
    "        rho0 = np.zeros((N, N), dtype=complex)\n",
    "        rho0[0, 0] = 1.0 # Start at site 0\n",
    "        \n",
    "        sol = solve_ivp(derivative, (0, 20), rho0.flatten(), t_eval=[20])\n",
    "        final_rho = sol.y[:, -1].reshape((N, N))\n",
    "        \n",
    "        # The population remaining in the system\n",
    "        p_remaining = np.real(np.trace(final_rho))\n",
    "        # Efficiency = 1 - remaining (Assuming everything lost went to sink)\n",
    "        return 1.0 - p_remaining\n",
    "\n",
    "    # 4. Scan Noise Levels\n",
    "    gammas = np.linspace(0, 5, 20)\n",
    "    efficiencies = []\n",
    "    \n",
    "    print(\"Simulating Quantum Transport with Noise...\")\n",
    "    for g in gammas:\n",
    "        eff = run_transport(g)\n",
    "        efficiencies.append(eff)\n",
    "        \n",
    "    # 5. Visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(gammas, efficiencies, 'o-', color='green')\n",
    "    plt.title(\"ENAQT: Noise-Assisted Transport\")\n",
    "    plt.xlabel(\"Dephasing Rate $\\gamma$ (Noise)\")\n",
    "    plt.ylabel(\"Transport Efficiency\")\n",
    "    plt.grid(True)\n",
    "    plt.axvline(x=gammas[np.argmax(efficiencies)], color='r', linestyle='--', label='Optimal Noise')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    enaqt_simulation()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] G. S. Engel et al., \"Evidence for wavelike energy transfer through quantum coherence in photosynthetic systems,\" *Nature* **446**, 782 (2007).  \n",
    "[2] M. B. Plenio and S. F. Huelga, \"Dephasing-assisted transport: quantum networks and biomolecules,\" *New Journal of Physics* **10**, 113019 (2008).  \n",
    "[3] Y. Levine et al., \"Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design,\" *Neural Computation* **31**, 1012 (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11da152e",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 36** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "# **Part III: Real-World Applications & Engineering**\n",
    "\n",
    "## **Subtitle:** *From Hamiltonians to Hardware*\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 12: Execution (The Hardware Layer)**\n",
    "\n",
    "#### **Chapter 36: Variational Quantum Algorithms (VQE & QAOA)**\n",
    "\n",
    "In Volumes I-III, we assumed we had perfect \"Fault-Tolerant\" quantum computers that could run algorithms like Phase Estimation (Chapter 4) forever without error.\n",
    "The reality of the **NISQ (Noisy Intermediate-Scale Quantum)** era is different. Our qubits are fragile. They decohere in microseconds. We cannot run deep circuits with thousands of gates.\n",
    "\n",
    "To solve this, we don't try to solve the problem entirely on the quantum chip. Instead, we use a **Hybrid Loop**, combining the robustness of classical CPUs with the sampling power of Quantum QPUs. This class of algorithms, known as **Variational Quantum Algorithms (VQAs)**, is the workhorse of modern quantum engineering.\n",
    "\n",
    "### **36.1 The Hybrid Loop: Making Noise Useful**\n",
    "\n",
    "#### **36.1.1 The Variational Principle**\n",
    "\n",
    "The core physics behind this approach is the **Variational Principle**. For any Hamiltonian $H$ and any parameterized wavefunction $|\\psi(\\vec{\\theta})\\rangle$, the expectation value of the energy satisfies:\n",
    "\n",
    "$$\\langle \\psi(\\vec{\\theta}) | H | \\psi(\\vec{\\theta}) \\rangle \\ge E_{ground}$$\n",
    "\n",
    "No matter what wavefunction we guess, its energy will *always* be higher than or equal to the true ground state energy.\n",
    "Therefore, finding the ground state becomes a classical minimization game: **Find the parameters $\\vec{\\theta}$ that minimize the energy.**\n",
    "\n",
    "#### **36.1.2 The Division of Labor**\n",
    "\n",
    "We split the computer into two parts:\n",
    "\n",
    "1.  **The QPU (Quantum Processing Unit):** Its only job is to prepare the state $|\\psi(\\vec{\\theta})\\rangle$ and measure it. It acts as a specialized \"function evaluator\" that computes $E(\\vec{\\theta})$.\n",
    "2.  **The CPU (Classical Processing Unit):** It takes the measured energy $E$ and uses a classical optimizer (like Gradient Descent, SPSA, or Nelder-Mead) to suggest a new set of parameters $\\vec{\\theta}_{new}$.\n",
    "\n",
    "This loop repeats until convergence. Because the quantum circuit is kept shallow (short depth), it can run within the decoherence time of the qubits [1].\n",
    "\n",
    "### **36.2 Hamiltonian Averaging: Measuring $\\langle H \\rangle$**\n",
    "\n",
    "How do we actually measure energy on a quantum computer?\n",
    "We cannot just ask the QPU \"What is the Energy?\" because the Hamiltonian $H$ is generally not diagonal in the computational basis. We can only measure $Z$ (0 or 1).\n",
    "\n",
    "However, we know from Volume II that any Hamiltonian can be decomposed into a sum of **Pauli Strings** (tensor products of $I, X, Y, Z$).\n",
    "\n",
    "$$H = \\sum_k c_k P_k$$\n",
    "\n",
    "Example: For a molecule, $H$ might look like $0.5 Z_1 Z_2 - 0.2 X_1 X_2 + \\dots$\n",
    "\n",
    "#### **36.2.1 The Sampling Procedure**\n",
    "\n",
    "To measure the expectation value $\\langle H \\rangle$:\n",
    "\n",
    "1.  **Decompose:** Break $H$ into $K$ terms (e.g., $P_1 = Z_1 Z_2$, $P_2 = X_1 X_2$).\n",
    "2.  **Measure Basis by Basis:**\n",
    "      * To measure $Z_1 Z_2$: Run the circuit and measure qubits 1 and 2 in the standard basis. Calculate the parity.\n",
    "      * To measure $X_1 X_2$: Run the circuit again, but append Hadamard gates to rotate the X-basis to the Z-basis before measuring.\n",
    "3.  **Average:** Sum the results weighted by the coefficients $c_k$.\n",
    "    $$\\langle H \\rangle = \\sum_k c_k \\langle P_k \\rangle_{measured}$$\n",
    "\n",
    "This transforms the impossible task of \"measuring energy\" into the manageable task of \"counting statistics.\" However, it introduces **Shot Noise**. The error scales as $1/\\sqrt{M}$, where $M$ is the number of measurement shots [2].\n",
    "\n",
    "### **36.3 QAOA: The Quantum Approximate Optimization Algorithm**\n",
    "\n",
    "The most famous application of this variational approach is **QAOA**, designed to solve combinatorial optimization problems (like the Ising models from Chapter 30).\n",
    "\n",
    "#### **36.3.1 The Adiabatic Inspiration**\n",
    "\n",
    "QAOA is inspired by **Adiabatic Quantum Computing**.\n",
    "If we start in the ground state of a simple Hamiltonian $H_{mix}$ (usually $\\sum X_i$, the \"superposition\" state) and slowly evolve to the problem Hamiltonian $H_{cost}$ (the Ising model), the system stays in the ground state.\n",
    "$$H(t) = (1-t/T) H_{mix} + (t/T) H_{cost}$$\n",
    "\n",
    "#### **36.3.2 Trotterizing the Evolution**\n",
    "\n",
    "We cannot run a continuous long evolution on NISQ chips. So, we chop the time evolution into $p$ discrete steps (layers).\n",
    "At each layer, we apply two unitary operators:\n",
    "\n",
    "1.  **The Cost Unitary ($U_C$):** $e^{-i \\gamma H_{cost}}$. This applies phase shifts to good solutions. Since $H_{cost}$ is diagonal in the Z-basis, this is just a sequence of $R_z$ rotations.\n",
    "2.  **The Mixer Unitary ($U_M$):** $e^{-i \\beta H_{mix}}$. This mixes the population to explore new states. Since $H_{mix} = \\sum X_i$, this is a sequence of $R_x$ rotations.\n",
    "\n",
    "The total ansatz is a \"sandwich\" of these layers:\n",
    "$$|\\psi(\\vec{\\gamma}, \\vec{\\beta})\\rangle = U_M(\\beta_p) U_C(\\gamma_p) \\dots U_M(\\beta_1) U_C(\\gamma_1) |+\\rangle^{\\otimes N}$$\n",
    "\n",
    "#### **36.3.3 The Optimization Landscape**\n",
    "\n",
    "The classical optimizer tunes the angles $\\vec{\\gamma}$ and $\\vec{\\beta}$ to minimize the cost function $\\langle H_{cost} \\rangle$.\n",
    "\n",
    "  * **$p=1$:** Shallow circuit. Fast, but approximate solution.\n",
    "  * **$p \\to \\infty$:** Approaches perfect Adiabatic evolution (exact solution).\n",
    "\n",
    "QAOA turns discrete optimization into continuous optimization, allowing us to use gradients to find the best solution [3].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will implement a full **QAOA simulation** for the MaxCut problem. We will define the graph, construct the Cost and Mixer unitaries, and use a classical optimizer (`scipy.optimize`) to find the optimal angles $\\gamma$ and $\\beta$.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def qaoa_maxcut_simulation():\n",
    "    # 1. Define the Problem (MaxCut on a Graph)\n",
    "    # A simple triangle graph (Frustrated system)\n",
    "    # 0 -- 1\n",
    "    #  \\  /\n",
    "    #   2\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from([(0, 1), (1, 2), (2, 0)])\n",
    "    N = G.number_of_nodes()\n",
    "    \n",
    "    print(f\"Solving MaxCut for {N}-node Triangle Graph.\")\n",
    "    \n",
    "    # MaxCut Cost Hamiltonian H_C\n",
    "    # For edge (i,j), we want spins to be opposite.\n",
    "    # Cost function C = sum_{<ij>} 0.5 * (1 - Z_i Z_j)\n",
    "    # To MINIMIZE energy, we assume H = sum_{<ij>} Z_i Z_j\n",
    "    # (Opposite spins -> -1, Aligned -> +1)\n",
    "    \n",
    "    # 2. Quantum Operators (Simulation)\n",
    "    # We simulate the state vector directly (size 2^N)\n",
    "    \n",
    "    # Pauli Z and X matrices\n",
    "    I = np.array([[1, 0], [0, 1]])\n",
    "    Z = np.array([[1, 0], [0, -1]])\n",
    "    X = np.array([[0, 1], [1, 0]])\n",
    "    \n",
    "    # Helper to create N-qubit operator\n",
    "    def get_op(op, target):\n",
    "        ops = [I] * N\n",
    "        ops[target] = op\n",
    "        full_op = ops[0]\n",
    "        for i in range(1, N):\n",
    "            full_op = np.kron(full_op, ops[i])\n",
    "        return full_op\n",
    "\n",
    "    # Build H_cost (diagonal)\n",
    "    H_cost = np.zeros(2**N)\n",
    "    for u, v in G.edges():\n",
    "        ZZ = get_op(Z, u) @ get_op(Z, v)\n",
    "        # We use diagonal of ZZ for efficiency\n",
    "        H_cost += np.diag(ZZ)\n",
    "        \n",
    "    # Build H_mixer = sum X_i\n",
    "    H_mixer = np.zeros((2**N, 2**N))\n",
    "    for i in range(N):\n",
    "        H_mixer += get_op(X, i)\n",
    "        \n",
    "    # 3. QAOA Circuit Function\n",
    "    # Returns the expected energy <psi|H_c|psi>\n",
    "    def qaoa_energy(params):\n",
    "        # Unpack params: [gamma1, ..., gamma_p, beta1, ..., beta_p]\n",
    "        p = len(params) // 2\n",
    "        gammas = params[:p]\n",
    "        betas = params[p:]\n",
    "        \n",
    "        # Initial state |+> = H|0>\n",
    "        psi = np.ones(2**N) / np.sqrt(2**N)\n",
    "        \n",
    "        for k in range(p):\n",
    "            # Cost Layer: exp(-i * gamma * H_cost)\n",
    "            # Since H_cost is diagonal, this is element-wise phase mult\n",
    "            U_c = np.exp(-1j * gammas[k] * H_cost)\n",
    "            psi = U_c * psi\n",
    "            \n",
    "            # Mixer Layer: exp(-i * beta * H_mixer)\n",
    "            # H_mixer is not diagonal, requires matrix exp\n",
    "            # In simulation we cheat with linalg.expm\n",
    "            # In real hardware, this is RX gates\n",
    "            U_m = np.linalg.eigh(H_mixer) # Pre-computing eigenbasis would be faster\n",
    "            # But for N=3, expm is instant\n",
    "            from scipy.linalg import expm\n",
    "            U_m_mat = expm(-1j * betas[k] * H_mixer)\n",
    "            psi = U_m_mat @ psi\n",
    "            \n",
    "        # Measure Energy <psi|H_c|psi>\n",
    "        # H_cost is real diagonal array\n",
    "        expected_energy = np.sum(np.abs(psi)**2 * H_cost)\n",
    "        return expected_energy\n",
    "\n",
    "    # 4. Classical Optimization\n",
    "    p_depth = 2\n",
    "    # Random initial guess\n",
    "    init_params = np.random.uniform(0, np.pi, 2 * p_depth)\n",
    "    \n",
    "    print(f\"Optimizing QAOA (Depth p={p_depth})...\")\n",
    "    result = opt.minimize(qaoa_energy, init_params, method='COBYLA')\n",
    "    \n",
    "    print(\"\\nOptimization Complete.\")\n",
    "    print(f\"Optimal Energy: {result.fun:.4f}\")\n",
    "    \n",
    "    # 5. Analyze Result State\n",
    "    opt_params = result.x\n",
    "    \n",
    "    # Re-run circuit to get final state\n",
    "    gammas = opt_params[:p_depth]\n",
    "    betas = opt_params[p_depth:]\n",
    "    psi = np.ones(2**N) / np.sqrt(2**N)\n",
    "    \n",
    "    # Construct final unitary\n",
    "    # (Repeat logic from qaoa_energy but return state)\n",
    "    for k in range(p_depth):\n",
    "        psi = np.exp(-1j * gammas[k] * H_cost) * psi\n",
    "        from scipy.linalg import expm\n",
    "        psi = expm(-1j * betas[k] * H_mixer) @ psi\n",
    "        \n",
    "    probs = np.abs(psi)**2\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    states = [f\"{i:0{N}b}\" for i in range(2**N)]\n",
    "    plt.bar(states, probs, color='purple')\n",
    "    plt.xlabel(\"Bitstring (Cut Configuration)\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.title(f\"QAOA Result (p={p_depth}) for Triangle Graph\")\n",
    "    \n",
    "    # Highlight MaxCut solutions\n",
    "    # For triangle, MaxCut is 2 edges cut (e.g. 011, 100, 110...)\n",
    "    # 000 and 111 have 0 cuts.\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Interpretation:\")\n",
    "    print(\"Highest peaks correspond to optimal MaxCut partitions.\")\n",
    "    print(\"Notice the symmetry: |011> and |100> (bit-flip) are equivalent cuts.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    qaoa_maxcut_simulation()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] A. Peruzzo et al., \"A variational eigenvalue solver on a photonic quantum processor,\" *Nature Communications* **5**, 4213 (2014).  \n",
    "[2] J. R. McClean et al., \"The theory of variational hybrid quantum-classical algorithms,\" *New Journal of Physics* **18**, 023023 (2016).  \n",
    "[3] E. Farhi, J. Goldstone, and S. Gutmann, \"A Quantum Approximate Optimization Algorithm,\" *arXiv:1411.4028* (2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c442d",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 37** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 12: Execution (The Hardware Layer)**\n",
    "\n",
    "#### **Chapter 37: Dynamic Circuits & Error Mitigation**\n",
    "\n",
    "We have entered the era of **Noisy Intermediate-Scale Quantum (NISQ)** devices. These machines are powerful but imperfect. Gates have error rates of $0.1\\% - 1\\%$, and qubits decohere before we can run long algorithms.\n",
    "\n",
    "Until we reach full Fault Tolerance (Chapter 38), we must live with errors. This chapter introduces the \"survival kit\" for the NISQ era: **Error Mitigation**. We learn how to cancel out noise mathematically without correcting it physically, and how to use dynamic feedback (measuring and reacting in real-time) to perform magic tricks like Teleportation.\n",
    "\n",
    "### **37.1 Zero-Noise Extrapolation (ZNE)**\n",
    "\n",
    "If you have a noisy radio signal, you can't just turn up the volume to fix it. But if you knew exactly how the static increased as you turned a knob, you could mathematically predict what the signal *would* sound like if the static were zero.\n",
    "\n",
    "This is the principle of **Zero-Noise Extrapolation (ZNE)**. It is currently the most effective method for getting accurate results from noisy quantum processors.\n",
    "\n",
    "#### **37.1.1 Amplifying the Noise**\n",
    "\n",
    "It sounds counter-intuitive: to fix the noise, we first **make it worse**.\n",
    "We intentionally stretch the duration of our quantum gates.\n",
    "\n",
    "  * **Scale Factor $\\lambda = 1$:** Normal execution. Noise level $\\epsilon$.\n",
    "  * **Scale Factor $\\lambda = 3$:** We replace every gate $U$ with the identity sandwich $U U^\\dagger U$. Mathematically, this is still $U$. Physically, it takes 3 times longer, so it accumulates 3 times as much decoherence noise ($3\\epsilon$).\n",
    "  * **Scale Factor $\\lambda = 5$:** $U U^\\dagger U U^\\dagger U$. Noise level $5\\epsilon$.\n",
    "\n",
    "#### **37.1.2 The Richardson Extrapolation**\n",
    "\n",
    "We measure the expectation value $E(\\lambda)$ at different noise scales ($\\lambda = 1, 3, 5$).\n",
    "We then fit a curve (usually linear or exponential) to these points and extrapolate back to the unphysical point **$\\lambda = 0$**.\n",
    "$$E_{clean} \\approx \\lim_{\\lambda \\to 0} E(\\lambda)$$\n",
    "\n",
    "This technique allowed IBM to simulate the Hydrogen molecule with chemical accuracy on a noisy chip, proving that we don't need perfect hardware to do perfect science [1].\n",
    "\n",
    "### **37.2 Mid-Circuit Measurement**\n",
    "\n",
    "For decades, quantum algorithms were static: you initialize, run a unitary sequence, and measure at the very end.\n",
    "Modern hardware (like Honeywell/Quantinuum and IBM Quantum) now supports **Dynamic Circuits**. You can measure a qubit *in the middle* of a computation, and use the result (0 or 1) to change the future gates applied to other qubits.\n",
    "\n",
    "#### **37.2.1 Feed-Forward Logic**\n",
    "\n",
    "This enables `if/else` logic in the quantum domain.\n",
    "**Teleportation** is the prime example. Alice wants to send a state $|\\psi\\rangle$ to Bob.\n",
    "\n",
    "1.  Alice entangles $|\\psi\\rangle$ with her half of a Bell pair.\n",
    "2.  Alice measures her two qubits. She gets two classical bits $m_1, m_2$.\n",
    "3.  **The Feed-Forward:** Bob must apply a correction operator $X^{m_2} Z^{m_1}$ to his qubit.\n",
    "      * If $m_1=1$, apply $Z$. If $0$, do nothing.\n",
    "      * If $m_2=1$, apply $X$. If $0$, do nothing.\n",
    "\n",
    "Without real-time feed-forward, Bob would have to wait for a classical phone call, breaking the coherence of the algorithm. With dynamic circuits, the correction happens within the coherence time (microseconds).\n",
    "\n",
    "#### **37.2.2 Adaptive State Preparation**\n",
    "\n",
    "We can also use this to \"fix\" states. Suppose we want to prepare the ground state $|000\\rangle$.\n",
    "\n",
    "1.  Measure all qubits.\n",
    "2.  If Qubit 1 is $|1\\rangle$, apply $X$ (flip it back to $|0\\rangle$).\n",
    "3.  Repeat.\n",
    "    This **Reset** operation is effectively a refrigerator, pumping entropy out of the system mid-circuit.\n",
    "\n",
    "### **37.3 Randomized Benchmarking**\n",
    "\n",
    "How do we know our gates are good? If we run a circuit and get the wrong answer, was it the gate error, the measurement error, or the state preparation error?\n",
    "**Randomized Benchmarking (RB)** is the industry standard protocol for disentangling these errors and measuring the average **Gate Fidelity**.\n",
    "\n",
    "#### **37.3.1 The Protocol**\n",
    "\n",
    "1.  **Scramble:** Apply a random sequence of $m$ Clifford gates (gates that map Pauli operators to Pauli operators, like $H, S, CNOT$). This randomizes the state across the Hilbert space.\n",
    "2.  **Invert:** Calculate the *exact inverse* of the entire random sequence (which is also a Clifford gate). Apply it.\n",
    "3.  **Measure:** Ideally, we should return exactly to the starting state $|0\\rangle$.\n",
    "    $$U_{inv} \\cdot (C_m \\dots C_2 C_1) |0\\rangle = |0\\rangle$$\n",
    "\n",
    "#### **37.3.2 The Decay Curve**\n",
    "\n",
    "In reality, errors accumulate. The probability of successfully returning to $|0\\rangle$ decays exponentially with the sequence length $m$.\n",
    "$$P_{survival}(m) = A p^m + B$$\n",
    "\n",
    "  * **$p$:** The depolarizing parameter.\n",
    "  * **Gate Error:** $r = \\frac{d-1}{d}(1-p)$.\n",
    "\n",
    "By fitting this decay curve, we extract a single number (e.g., \"99.9% Fidelity\") that characterizes the quality of the hardware, independent of the specific circuit being run [2].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will simulate **Zero-Noise Extrapolation (ZNE)**. We will run a noisy circuit at noise levels $\\lambda=1, 3, 5$, introducing artificial depolarizing noise. We will then perform a linear extrapolation to estimate the zero-noise expectation value.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def zne_demo():\n",
    "    # 1. Define the True Physics\n",
    "    # We want to measure <Z> of a state rotated by theta.\n",
    "    # U = Ry(theta). True <Z> = cos(theta).\n",
    "    theta = np.pi / 3\n",
    "    true_val = np.cos(theta) # 0.5\n",
    "    \n",
    "    # 2. Define Noise Model (Depolarizing Channel)\n",
    "    # The state rho becomes: p * rho + (1-p)/2 * I\n",
    "    # p = exp(-epsilon * depth)\n",
    "    # epsilon is error rate per unit depth\n",
    "    error_rate = 0.05 # 5% error per unit depth\n",
    "    \n",
    "    def run_noisy_experiment(scale_factor):\n",
    "        # Scale factor lambda means effective depth is lambda * base_depth\n",
    "        # Let's assume base depth = 1 gate.\n",
    "        effective_depth = scale_factor\n",
    "        \n",
    "        # Depolarizing parameter p (survival probability)\n",
    "        p = np.exp(-error_rate * effective_depth)\n",
    "        \n",
    "        # Measured Expectation Value:\n",
    "        # <Z>_noisy = p * <Z>_true + (1-p) * <Z>_mixed\n",
    "        # <Z>_mixed for maximally mixed state is 0.\n",
    "        measured_val = p * true_val\n",
    "        \n",
    "        # Add some statistical shot noise (finite sampling)\n",
    "        shots = 1000\n",
    "        std_dev = np.sqrt((1 - measured_val**2) / shots)\n",
    "        noise = np.random.normal(0, std_dev)\n",
    "        \n",
    "        return measured_val + noise\n",
    "\n",
    "    # 3. Perform ZNE Experiments\n",
    "    scales = [1, 3, 5] # Richardson rescaling factors\n",
    "    results = []\n",
    "    \n",
    "    print(f\"True Value: {true_val:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for s in scales:\n",
    "        val = run_noisy_experiment(s)\n",
    "        results.append(val)\n",
    "        print(f\"Scale {s} (Noise ~{s*error_rate:.2f}): Measured {val:.4f}\")\n",
    "        \n",
    "    # 4. Extrapolate to Zero\n",
    "    # Fit model E(lambda) = A * exp(-B * lambda) + C -> usually linear for small noise\n",
    "    # Richardson extrapolation for scales [1,3,5] using linear fit E = a*lambda + b\n",
    "    \n",
    "    # Linear Fit\n",
    "    coeffs = np.polyfit(scales, results, 1)\n",
    "    # coeffs[0] is slope, coeffs[1] is intercept (lambda=0)\n",
    "    zne_val = coeffs[1]\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Extrapolated (Zero Noise): {zne_val:.4f}\")\n",
    "    print(f\"Error (Unmitigated): {abs(results[0] - true_val):.4f}\")\n",
    "    print(f\"Error (ZNE):         {abs(zne_val - true_val):.4f}\")\n",
    "    \n",
    "    # 5. Visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.errorbar(scales, results, yerr=0.02, fmt='ko', label='Noisy Measurements')\n",
    "    \n",
    "    # Plot fit line\n",
    "    x_range = np.linspace(0, 6, 100)\n",
    "    y_fit = coeffs[0] * x_range + coeffs[1]\n",
    "    plt.plot(x_range, y_fit, 'b--', label='Linear Extrapolation')\n",
    "    \n",
    "    # Plot Truth\n",
    "    plt.axhline(true_val, color='g', linestyle='-', label='True Value')\n",
    "    plt.scatter([0], [zne_val], color='r', s=100, zorder=5, label='ZNE Prediction')\n",
    "    \n",
    "    plt.title(\"Error Mitigation: Zero-Noise Extrapolation\")\n",
    "    plt.xlabel(\"Noise Scale Factor ($\\lambda$)\")\n",
    "    plt.ylabel(\"Expectation Value $\\langle Z \\rangle$\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(-0.5, 6)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    zne_demo()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] K. Temme, S. Bravyi, and J. M. Gambetta, \"Error Mitigation for Short-Depth Quantum Circuits,\" *Physical Review Letters* **119**, 180509 (2017).  \n",
    "[2] E. Magesan, J. M. Gambetta, and J. Emerson, \"Scalable and Robust Randomized Benchmarking of Quantum Processes,\" *Physical Review Letters* **106**, 180504 (2011)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f8a06",
   "metadata": {},
   "source": [
    "Here is the complete draft for **Chapter 38** of *Physics of the Discrete World*.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Section 12: Execution (The Hardware Layer)**\n",
    "\n",
    "#### **Chapter 38: The Road to Fault Tolerance (LDPC)**\n",
    "\n",
    "We have reached the end of our journey. In Volumes I-IV, we built the universe from the ground up: from single particles on a grid to complex many-body systems and quantum neural networks.\n",
    "But there is one final hurdle. The physical qubits we build (superconducting loops, trapped ions) are noisy. To build a computer that can solve the mysteries of the universe (like the mass gap of QCD or the folding of proteins), we need **Logical Qubits** that last forever.\n",
    "\n",
    "This chapter outlines the endgame of quantum engineering: **Fault Tolerance**. We move beyond the simple Surface Code to the next generation of error correction (qLDPC) and discuss the expensive \"fuel\" required to run quantum algorithms: **Magic States**. We conclude with a reflection on the fundamental nature of our discrete reality.\n",
    "\n",
    "### **38.1 Beyond the Surface Code: Quantum LDPC**\n",
    "\n",
    "For 20 years, the **Surface Code** (Toric Code with boundaries, Chapter 27) has been the leading candidate for fault tolerance. It is local (check operators only involve 4 neighbors), making it easy to build on a 2D chip.\n",
    "However, it is wildly inefficient.\n",
    "\n",
    "  * **Encoding Rate:** To encode $k=1$ logical qubit with distance $d$ (protection), we need $n \\approx 2d^2$ physical qubits.\n",
    "  * **Scaling:** As $d \\to \\infty$, the rate $k/n \\to 0$. We need a football field of physical qubits just to store a few logical bytes.\n",
    "\n",
    "#### **38.1.1 The Connectivity Breakout**\n",
    "\n",
    "To beat this limit, we must break the 2D locality constraint. We need **Quantum Low-Density Parity-Check (qLDPC)** codes.\n",
    "These codes are defined on **Expander Graphs** (Chapter 21). The qubits are connected in a complex, non-local network where information is delocalized rapidly.\n",
    "\n",
    "#### **38.1.2 Constant Rate Encoding**\n",
    "\n",
    "In 2021, a breakthrough proved that \"Good\" qLDPC codes exist.\n",
    "\n",
    "  * **Rate:** $k/n \\to \\text{const}$ (e.g., we can encode 100 logical qubits in 1000 physical qubits, rather than 100,000).\n",
    "  * **Distance:** $d \\sim n$.\n",
    "    This efficiency gain changes the economics of quantum computing. Instead of millions of qubits, we might only need thousands to achieve fault tolerance, provided we can build the complex wiring required by the expander graph [1].\n",
    "\n",
    "### **38.2 Magic States**\n",
    "\n",
    "Error correction codes (like Surface or qLDPC) are great at storing memory. They can perform **Clifford Gates** (Hadamard, CNOT, S) fault-tolerantly.\n",
    "But there is a catch: **Clifford Gates are not Universal.** You cannot build a computer using only Cliffords (this is the Gottesman-Knill theorem). To run a useful algorithm (like Shor's or Grover's), you need at least one **Non-Clifford Gate** (usually the $T$ gate, a $\\pi/4$ rotation).\n",
    "\n",
    "#### **38.2.1 The Distillation Factory**\n",
    "\n",
    "We cannot perform a $T$-gate directly on encoded data without spreading errors.\n",
    "Instead, we use a trick: **Gate Teleportation**.\n",
    "\n",
    "1.  We prepare a special raw resource state called a **Magic State**: $|A\\rangle = T|+\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle + e^{i\\pi/4}|1\\rangle)$.\n",
    "2.  We prepare many noisy copies of $|A\\rangle$.\n",
    "3.  We run a **Distillation Circuit** (using only perfect Clifford gates) that checks parity.\n",
    "      * If the check fails: Discard the states.\n",
    "      * If the check passes: The output state is cleaner (lower error).\n",
    "\n",
    "#### **38.2.2 The Cost of Magic**\n",
    "\n",
    "This process is expensive. To produce one clean Magic State (error $10^{-12}$), we might need to burn 100 noisy input states.\n",
    "A future Fault-Tolerant Quantum Computer will likely be 90% a \"Magic State Factory\" and only 10% a \"Processor.\" The physics of the discrete world dictates that non-linear logic (non-Clifford rotation) is the most expensive resource in the universe [2].\n",
    "\n",
    "### **38.3 It From Bit**\n",
    "\n",
    "We conclude our four-volume series with a return to the philosophy of John Archibald Wheeler: **\"It From Bit.\"**\n",
    "\n",
    "In Volume I, we discretized space into a lattice to make the math easier.\n",
    "In Volume II, we saw that particles are just excitations of fields on this lattice.\n",
    "In Volume III, we saw that forces are just connection matrices on the links.\n",
    "In Volume IV, we saw that even gravity and space-time might emerge from the entanglement of qubits (AdS/CFT).\n",
    "\n",
    "Is the universe *actually* discrete?\n",
    "\n",
    "  * If space is continuous, the information density is infinite. A single cubic nanometer could store the entire library of congress in the digits of its coordinates. This leads to paradoxes (Black Hole Information limit).\n",
    "  * If space is discrete (a Planck-scale lattice), the information is finite. The universe computes its next state just like our simulations.\n",
    "\n",
    "Whether reality is truly a Tensor Network or whether that is just the best lens we have to see it, the lesson of physics is clear: **Information is Physical.** By mastering the physics of the discrete world, we do not just simulate nature; we learn to speak her native language [3].\n",
    "\n",
    "-----\n",
    "\n",
    "### **Computable Physics: Python Implementation**\n",
    "\n",
    "We will simulate the **Magic State Distillation** process. The standard \"15-to-1\" protocol takes 15 raw states (with error $\\epsilon$) and produces 1 output state with error $35\\epsilon^3$. We will plot this distillation flow to visualize how we purify quantum fuel.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def magic_state_distillation():\n",
    "    # 1. Define the Error Map\n",
    "    # For the standard 15-to-1 protocol (Bravyi-Kitaev):\n",
    "    # p_out = 35 * p_in^3 + O(p_in^4)\n",
    "    # This suppresses error cubically.\n",
    "    \n",
    "    def distill_step(p_in):\n",
    "        # Exact formula for 15-to-1 protocol (assuming perfect Clifford gates)\n",
    "        # Probability of accepting the block\n",
    "        p_accept = (1 - p_in)**15 + 15 * p_in * (1 - p_in)**14  # etc... simplified\n",
    "        # The leading order approximation is robust for small p\n",
    "        return 35 * p_in**3\n",
    "\n",
    "    # 2. Iterate Distillation Layers\n",
    "    # We start with raw physical error rates (e.g. 1%)\n",
    "    # We want to reach \"logical\" error rates (e.g. 1e-12)\n",
    "    \n",
    "    initial_errors = np.linspace(0.001, 0.18, 100) # Threshold is around 14%\n",
    "    \n",
    "    # Track evolution\n",
    "    rounds = 3\n",
    "    results = [initial_errors]\n",
    "    \n",
    "    for _ in range(rounds):\n",
    "        current_p = results[-1]\n",
    "        next_p = distill_step(current_p)\n",
    "        # Cap at 0.5 (random noise) for visualization stability\n",
    "        next_p = np.minimum(next_p, 0.5)\n",
    "        results.append(next_p)\n",
    "        \n",
    "    # 3. Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(initial_errors, initial_errors, 'k--', label='Break-even (Input=Output)')\n",
    "    \n",
    "    colors = ['r', 'orange', 'g']\n",
    "    for i in range(rounds):\n",
    "        plt.plot(initial_errors, results[i+1], label=f'Round {i+1} Output', color=colors[i], linewidth=2)\n",
    "        \n",
    "    plt.title(\"Magic State Distillation: Purifying Quantum Fuel\")\n",
    "    plt.xlabel(\"Input Error Rate $\\epsilon_{in}$\")\n",
    "    plt.ylabel(\"Output Error Rate $\\epsilon_{out}$\")\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.axvline(0.14, color='k', linestyle=':', label='Threshold (~14%)')\n",
    "    \n",
    "    plt.text(0.01, 1e-5, \"Success Region\\n(Error -> 0)\", fontsize=12, color='green')\n",
    "    plt.text(0.2, 1e-1, \"Failure Region\\n(Error -> 0.5)\", fontsize=12, color='red')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate rounds needed for target\n",
    "    target = 1e-12\n",
    "    start_p = 0.01 # 1% error\n",
    "    curr = start_p\n",
    "    count = 0\n",
    "    print(f\"Starting with error {start_p}:\")\n",
    "    while curr > target and count < 10:\n",
    "        count += 1\n",
    "        curr = distill_step(curr)\n",
    "        print(f\"Round {count}: Error ~ {curr:.2e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    magic_state_distillation()\n",
    "```\n",
    "\n",
    "### **References**\n",
    "\n",
    "[1] N. P. Breuckmann and J. N. Eberhardt, \"Quantum Low-Density Parity-Check Codes,\" *PRX Quantum* **2**, 040101 (2021).  \n",
    "[2] S. Bravyi and A. Kitaev, \"Universal quantum computation with ideal Clifford gates and noisy ancillas,\" *Physical Review A* **71**, 022316 (2005).  \n",
    "[3] J. A. Wheeler, \"Information, physics, quantum: The search for links,\" *Complexity, Entropy, and the Physics of Information* (Addison-Wesley, 1990)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
